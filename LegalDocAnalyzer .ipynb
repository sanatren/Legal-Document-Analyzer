{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d8a2310cfd24026a3218ce1ba578ec5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f35911f2b07d496fa4188916e6fab3ba",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151,537/150,000 \u001b[0m [ \u001b[33m0:15:59\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m162 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">151,537/150,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:15:59</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">162 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f35911f2b07d496fa4188916e6fab3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRRGe0raJA-Q",
        "outputId": "2235fd20-31fb-48d3-d5b6-03587a4ed251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ First extraction complete: /content/dataset\n",
            "✅ Second extraction complete: /content/dataset_extracted\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "outer_zip_path = \"/content/7152317.zip\"  # Your main zip file\n",
        "extract_outer_path = \"/content/dataset\"  # First extraction\n",
        "\n",
        "# First extraction (7152317.zip)\n",
        "with zipfile.ZipFile(outer_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_outer_path)\n",
        "\n",
        "print(f\"✅ First extraction complete: {extract_outer_path}\")\n",
        "\n",
        "# Now extract the inner dataset.zip\n",
        "inner_zip_path = os.path.join(extract_outer_path, \"dataset.zip\")\n",
        "final_extract_path = \"/content/dataset_extracted\"\n",
        "\n",
        "if os.path.exists(inner_zip_path):\n",
        "    with zipfile.ZipFile(inner_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(final_extract_path)\n",
        "    print(f\"✅ Second extraction complete: {final_extract_path}\")\n",
        "else:\n",
        "    print(\"⚠️ Inner dataset.zip not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List extracted folder structure\n",
        "for root, dirs, files in os.walk(final_extract_path):\n",
        "    print(f\"📂 {root}\")\n",
        "    for dir_name in dirs:\n",
        "        print(f\"📁 {dir_name}\")\n",
        "    for file_name in files:\n",
        "        print(f\"📄 {file_name}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KsLnWavCJCEL",
        "outputId": "8d0d4e83-b71c-481f-fe23-025eb2b2d21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "📄 1477.txt\n",
            "📄 6302.txt\n",
            "📄 5421.txt\n",
            "📄 2664.txt\n",
            "📄 2191.txt\n",
            "📄 6510.txt\n",
            "📄 4604.txt\n",
            "📄 624.txt\n",
            "📄 6293.txt\n",
            "📄 3058.txt\n",
            "📄 6264.txt\n",
            "📄 1629.txt\n",
            "📄 4705.txt\n",
            "📄 5775.txt\n",
            "📄 370.txt\n",
            "📄 2975.txt\n",
            "📄 6377.txt\n",
            "📄 1103.txt\n",
            "📄 3814.txt\n",
            "📄 897.txt\n",
            "📄 2441.txt\n",
            "📄 4247.txt\n",
            "📄 4758.txt\n",
            "📄 6721.txt\n",
            "📄 1185.txt\n",
            "📄 748.txt\n",
            "📄 2383.txt\n",
            "📄 1473.txt\n",
            "📄 5200.txt\n",
            "📄 7039.txt\n",
            "📄 3005.txt\n",
            "📄 1492.txt\n",
            "📄 2963.txt\n",
            "📄 1846.txt\n",
            "📄 1249.txt\n",
            "📄 4906.txt\n",
            "📄 899.txt\n",
            "📄 3395.txt\n",
            "📄 5752.txt\n",
            "📄 830.txt\n",
            "📄 5608.txt\n",
            "📄 3558.txt\n",
            "📄 2753.txt\n",
            "📄 1485.txt\n",
            "📄 722.txt\n",
            "📄 3291.txt\n",
            "📄 832.txt\n",
            "📄 6127.txt\n",
            "📄 5035.txt\n",
            "📄 4250.txt\n",
            "📄 5009.txt\n",
            "📄 4631.txt\n",
            "📄 6714.txt\n",
            "📄 309.txt\n",
            "📄 2406.txt\n",
            "📄 2870.txt\n",
            "📄 2578.txt\n",
            "📄 2333.txt\n",
            "📄 2272.txt\n",
            "📄 2588.txt\n",
            "📄 228.txt\n",
            "📄 4275.txt\n",
            "📄 6791.txt\n",
            "📄 2959.txt\n",
            "📄 3945.txt\n",
            "📄 3687.txt\n",
            "📄 3209.txt\n",
            "📄 5211.txt\n",
            "📄 3811.txt\n",
            "📄 251.txt\n",
            "📄 3025.txt\n",
            "📄 1524.txt\n",
            "📄 2172.txt\n",
            "📄 4646.txt\n",
            "📄 2020.txt\n",
            "📄 6773.txt\n",
            "📄 6490.txt\n",
            "📄 872.txt\n",
            "📄 6569.txt\n",
            "📄 932.txt\n",
            "📄 6384.txt\n",
            "📄 3096.txt\n",
            "📄 6446.txt\n",
            "📄 5252.txt\n",
            "📄 520.txt\n",
            "📄 6679.txt\n",
            "📄 3069.txt\n",
            "📄 2232.txt\n",
            "📄 5594.txt\n",
            "📄 3860.txt\n",
            "📄 5293.txt\n",
            "📄 1268.txt\n",
            "📄 4494.txt\n",
            "📄 3776.txt\n",
            "📄 4230.txt\n",
            "📄 3850.txt\n",
            "📄 3461.txt\n",
            "📄 3524.txt\n",
            "📄 6639.txt\n",
            "📄 2187.txt\n",
            "📄 858.txt\n",
            "📄 6030.txt\n",
            "📄 1338.txt\n",
            "📄 5154.txt\n",
            "📄 26.txt\n",
            "📄 4474.txt\n",
            "📄 6085.txt\n",
            "📄 2968.txt\n",
            "📄 3106.txt\n",
            "📄 4468.txt\n",
            "📄 781.txt\n",
            "📄 2154.txt\n",
            "📄 4539.txt\n",
            "📄 5005.txt\n",
            "📄 4389.txt\n",
            "📄 3964.txt\n",
            "📄 1488.txt\n",
            "📄 3513.txt\n",
            "📄 3883.txt\n",
            "📄 5264.txt\n",
            "📄 4241.txt\n",
            "📄 2492.txt\n",
            "📄 5336.txt\n",
            "📄 4493.txt\n",
            "📄 6855.txt\n",
            "📄 297.txt\n",
            "📄 5495.txt\n",
            "📄 2780.txt\n",
            "📄 5509.txt\n",
            "📄 6168.txt\n",
            "📄 3154.txt\n",
            "📄 1467.txt\n",
            "📄 4954.txt\n",
            "📄 6625.txt\n",
            "📄 6464.txt\n",
            "📄 2518.txt\n",
            "📄 1061.txt\n",
            "📄 5473.txt\n",
            "📄 5515.txt\n",
            "📄 6090.txt\n",
            "📄 2716.txt\n",
            "📄 4739.txt\n",
            "📄 6938.txt\n",
            "📄 740.txt\n",
            "📄 5503.txt\n",
            "📄 7132.txt\n",
            "📄 6834.txt\n",
            "📄 2330.txt\n",
            "📄 1842.txt\n",
            "📄 58.txt\n",
            "📄 6196.txt\n",
            "📄 3336.txt\n",
            "📄 324.txt\n",
            "📄 3991.txt\n",
            "📄 494.txt\n",
            "📄 600.txt\n",
            "📄 1910.txt\n",
            "📄 2577.txt\n",
            "📄 3093.txt\n",
            "📄 6988.txt\n",
            "📄 5516.txt\n",
            "📄 720.txt\n",
            "📄 2812.txt\n",
            "📄 3155.txt\n",
            "📄 6885.txt\n",
            "📄 4166.txt\n",
            "📄 4206.txt\n",
            "📄 1841.txt\n",
            "📄 1867.txt\n",
            "📄 1942.txt\n",
            "📄 4425.txt\n",
            "📄 4843.txt\n",
            "📄 5747.txt\n",
            "📄 4975.txt\n",
            "📄 348.txt\n",
            "📄 3657.txt\n",
            "📄 2399.txt\n",
            "📄 1588.txt\n",
            "📄 6111.txt\n",
            "📄 356.txt\n",
            "📄 5836.txt\n",
            "📄 2950.txt\n",
            "📄 2834.txt\n",
            "📄 4154.txt\n",
            "📄 1671.txt\n",
            "📄 664.txt\n",
            "📄 2831.txt\n",
            "📄 6806.txt\n",
            "📄 73.txt\n",
            "📄 6546.txt\n",
            "📄 2206.txt\n",
            "📄 3668.txt\n",
            "📄 3767.txt\n",
            "📄 958.txt\n",
            "📄 7002.txt\n",
            "📄 3426.txt\n",
            "📄 1260.txt\n",
            "📄 3884.txt\n",
            "📄 691.txt\n",
            "📄 7064.txt\n",
            "📄 3401.txt\n",
            "📄 2452.txt\n",
            "📄 5130.txt\n",
            "📄 2626.txt\n",
            "📄 6315.txt\n",
            "📄 1368.txt\n",
            "📄 5458.txt\n",
            "📄 4406.txt\n",
            "📄 2847.txt\n",
            "📄 7016.txt\n",
            "📄 5093.txt\n",
            "📄 1198.txt\n",
            "📄 7077.txt\n",
            "📄 6275.txt\n",
            "📄 6141.txt\n",
            "📄 3734.txt\n",
            "📄 2466.txt\n",
            "📄 6290.txt\n",
            "📄 5883.txt\n",
            "📄 1753.txt\n",
            "📄 1427.txt\n",
            "📄 479.txt\n",
            "📄 295.txt\n",
            "📄 1403.txt\n",
            "📄 663.txt\n",
            "📄 6756.txt\n",
            "📄 186.txt\n",
            "📄 5769.txt\n",
            "📄 5269.txt\n",
            "📄 6183.txt\n",
            "📄 285.txt\n",
            "📄 2479.txt\n",
            "📄 4803.txt\n",
            "📄 5096.txt\n",
            "📄 5537.txt\n",
            "📄 763.txt\n",
            "📄 3094.txt\n",
            "📄 2526.txt\n",
            "📄 4253.txt\n",
            "📄 2788.txt\n",
            "📄 1739.txt\n",
            "📄 5426.txt\n",
            "📄 6964.txt\n",
            "📄 2702.txt\n",
            "📄 1586.txt\n",
            "📄 4960.txt\n",
            "📄 2167.txt\n",
            "📄 1886.txt\n",
            "📄 7026.txt\n",
            "📄 4015.txt\n",
            "📄 330.txt\n",
            "📄 3017.txt\n",
            "📄 5949.txt\n",
            "📄 3080.txt\n",
            "📄 5008.txt\n",
            "📄 3947.txt\n",
            "📄 1927.txt\n",
            "📄 5042.txt\n",
            "📄 2086.txt\n",
            "📄 5642.txt\n",
            "📄 2108.txt\n",
            "📄 6310.txt\n",
            "📄 4676.txt\n",
            "📄 3373.txt\n",
            "📄 5198.txt\n",
            "📄 845.txt\n",
            "📄 7057.txt\n",
            "📄 224.txt\n",
            "📄 6255.txt\n",
            "📄 6427.txt\n",
            "📄 2004.txt\n",
            "📄 3192.txt\n",
            "📄 3417.txt\n",
            "📄 2849.txt\n",
            "📄 3825.txt\n",
            "📄 2697.txt\n",
            "📄 4757.txt\n",
            "📄 4656.txt\n",
            "📄 5797.txt\n",
            "📄 2297.txt\n",
            "📄 5714.txt\n",
            "📄 6356.txt\n",
            "📄 1516.txt\n",
            "📄 5624.txt\n",
            "📄 6149.txt\n",
            "📄 6960.txt\n",
            "📄 839.txt\n",
            "📄 988.txt\n",
            "📄 1265.txt\n",
            "📄 3200.txt\n",
            "📄 4908.txt\n",
            "📄 2405.txt\n",
            "📄 6965.txt\n",
            "📄 6214.txt\n",
            "📄 1446.txt\n",
            "📄 848.txt\n",
            "📄 3056.txt\n",
            "📄 612.txt\n",
            "📄 915.txt\n",
            "📄 2556.txt\n",
            "📄 1602.txt\n",
            "📄 935.txt\n",
            "📄 4100.txt\n",
            "📄 7025.txt\n",
            "📄 5321.txt\n",
            "📄 5599.txt\n",
            "📄 4075.txt\n",
            "📄 4638.txt\n",
            "📄 2458.txt\n",
            "📄 4380.txt\n",
            "📄 6250.txt\n",
            "📄 1412.txt\n",
            "📄 4156.txt\n",
            "📄 2527.txt\n",
            "📄 2966.txt\n",
            "📄 1290.txt\n",
            "📄 5523.txt\n",
            "📄 4904.txt\n",
            "📄 2929.txt\n",
            "📄 2598.txt\n",
            "📄 1803.txt\n",
            "📄 7066.txt\n",
            "📄 3237.txt\n",
            "📄 1584.txt\n",
            "📄 6421.txt\n",
            "📄 677.txt\n",
            "📄 1072.txt\n",
            "📄 4399.txt\n",
            "📄 7027.txt\n",
            "📄 4492.txt\n",
            "📄 465.txt\n",
            "📄 3197.txt\n",
            "📄 5468.txt\n",
            "📄 3102.txt\n",
            "📄 6719.txt\n",
            "📄 3899.txt\n",
            "📄 4509.txt\n",
            "📄 3314.txt\n",
            "📄 6224.txt\n",
            "📄 3231.txt\n",
            "📄 3566.txt\n",
            "📄 158.txt\n",
            "📄 4728.txt\n",
            "📄 5085.txt\n",
            "📄 2507.txt\n",
            "📄 3839.txt\n",
            "📄 3228.txt\n",
            "📄 5111.txt\n",
            "📄 6549.txt\n",
            "📄 1131.txt\n",
            "📄 3027.txt\n",
            "📄 1081.txt\n",
            "📄 3313.txt\n",
            "📄 4337.txt\n",
            "📄 4939.txt\n",
            "📄 708.txt\n",
            "📄 5896.txt\n",
            "📄 4556.txt\n",
            "📄 6400.txt\n",
            "📄 2629.txt\n",
            "📄 1459.txt\n",
            "📄 3662.txt\n",
            "📄 4752.txt\n",
            "📄 152.txt\n",
            "📄 7020.txt\n",
            "📄 553.txt\n",
            "📄 1561.txt\n",
            "📄 1646.txt\n",
            "📄 6666.txt\n",
            "📄 6617.txt\n",
            "📄 1968.txt\n",
            "📄 4928.txt\n",
            "📄 5017.txt\n",
            "📄 2927.txt\n",
            "📄 3629.txt\n",
            "📄 5615.txt\n",
            "📄 2988.txt\n",
            "📄 35.txt\n",
            "📄 2832.txt\n",
            "📄 4240.txt\n",
            "📄 3780.txt\n",
            "📄 2321.txt\n",
            "📄 5644.txt\n",
            "📄 1649.txt\n",
            "📄 3518.txt\n",
            "📄 6511.txt\n",
            "📄 4993.txt\n",
            "📄 6813.txt\n",
            "📄 6044.txt\n",
            "📄 1047.txt\n",
            "📄 704.txt\n",
            "📄 4701.txt\n",
            "📄 3144.txt\n",
            "📄 4270.txt\n",
            "📄 4863.txt\n",
            "📄 5284.txt\n",
            "📄 5614.txt\n",
            "📄 3846.txt\n",
            "📄 3645.txt\n",
            "📄 3344.txt\n",
            "📄 1243.txt\n",
            "📄 4078.txt\n",
            "📄 2853.txt\n",
            "📄 2904.txt\n",
            "📄 5014.txt\n",
            "📄 3276.txt\n",
            "📄 5153.txt\n",
            "📄 6081.txt\n",
            "📄 4371.txt\n",
            "📄 6977.txt\n",
            "📄 7001.txt\n",
            "📄 2286.txt\n",
            "📄 93.txt\n",
            "📄 1089.txt\n",
            "📄 6845.txt\n",
            "📄 5340.txt\n",
            "📄 6934.txt\n",
            "📄 2249.txt\n",
            "📄 871.txt\n",
            "📄 4407.txt\n",
            "📄 1104.txt\n",
            "📄 7115.txt\n",
            "📄 5777.txt\n",
            "📄 1564.txt\n",
            "📄 2887.txt\n",
            "📄 2623.txt\n",
            "📄 5734.txt\n",
            "📄 4744.txt\n",
            "📄 6417.txt\n",
            "📄 6649.txt\n",
            "📄 6897.txt\n",
            "📄 6760.txt\n",
            "📄 393.txt\n",
            "📄 5403.txt\n",
            "📄 7078.txt\n",
            "📄 1211.txt\n",
            "📄 3440.txt\n",
            "📄 4946.txt\n",
            "📄 6972.txt\n",
            "📄 3873.txt\n",
            "📄 6450.txt\n",
            "📄 1059.txt\n",
            "📄 5168.txt\n",
            "📄 700.txt\n",
            "📄 3556.txt\n",
            "📄 5482.txt\n",
            "📄 3147.txt\n",
            "📄 3216.txt\n",
            "📄 4426.txt\n",
            "📄 217.txt\n",
            "📄 5237.txt\n",
            "📄 447.txt\n",
            "📄 1074.txt\n",
            "📄 1869.txt\n",
            "📄 3835.txt\n",
            "📄 1286.txt\n",
            "📄 2893.txt\n",
            "📄 2688.txt\n",
            "📄 1527.txt\n",
            "📄 3297.txt\n",
            "📄 1042.txt\n",
            "📄 1011.txt\n",
            "📄 5484.txt\n",
            "📄 4746.txt\n",
            "📄 3803.txt\n",
            "📄 692.txt\n",
            "📄 1819.txt\n",
            "📄 2711.txt\n",
            "📄 4369.txt\n",
            "📄 5703.txt\n",
            "📄 3312.txt\n",
            "📄 1754.txt\n",
            "📄 3034.txt\n",
            "📄 4857.txt\n",
            "📄 2534.txt\n",
            "📄 5268.txt\n",
            "📄 2081.txt\n",
            "📄 1390.txt\n",
            "📄 1735.txt\n",
            "📄 1888.txt\n",
            "📄 6432.txt\n",
            "📄 5281.txt\n",
            "📄 1205.txt\n",
            "📄 2705.txt\n",
            "📄 750.txt\n",
            "📄 3625.txt\n",
            "📄 2160.txt\n",
            "📄 1607.txt\n",
            "📄 4338.txt\n",
            "📄 4619.txt\n",
            "📄 5672.txt\n",
            "📄 5681.txt\n",
            "📄 5394.txt\n",
            "📄 1601.txt\n",
            "📄 3252.txt\n",
            "📄 6178.txt\n",
            "📄 2442.txt\n",
            "📄 1165.txt\n",
            "📄 1670.txt\n",
            "📄 3534.txt\n",
            "📄 5231.txt\n",
            "📄 4842.txt\n",
            "📄 7063.txt\n",
            "📄 2532.txt\n",
            "📄 4549.txt\n",
            "📄 5676.txt\n",
            "📄 2385.txt\n",
            "📄 1644.txt\n",
            "📄 5601.txt\n",
            "📄 5435.txt\n",
            "📄 6656.txt\n",
            "📄 1801.txt\n",
            "📄 1623.txt\n",
            "📄 3712.txt\n",
            "📄 3327.txt\n",
            "📄 4089.txt\n",
            "📄 1423.txt\n",
            "📄 3128.txt\n",
            "📄 4672.txt\n",
            "📄 5012.txt\n",
            "📄 1779.txt\n",
            "📄 3528.txt\n",
            "📄 3217.txt\n",
            "📄 150.txt\n",
            "📄 5593.txt\n",
            "📄 464.txt\n",
            "📄 5702.txt\n",
            "📄 5151.txt\n",
            "📄 4415.txt\n",
            "📄 1302.txt\n",
            "📄 2395.txt\n",
            "📄 4134.txt\n",
            "📄 1188.txt\n",
            "📄 4302.txt\n",
            "📄 2665.txt\n",
            "📄 1830.txt\n",
            "📄 6035.txt\n",
            "📄 3746.txt\n",
            "📄 5555.txt\n",
            "📄 2173.txt\n",
            "📄 707.txt\n",
            "📄 5376.txt\n",
            "📄 2292.txt\n",
            "📄 4969.txt\n",
            "📄 2444.txt\n",
            "📄 3894.txt\n",
            "📄 4228.txt\n",
            "📄 1858.txt\n",
            "📄 5236.txt\n",
            "📄 5610.txt\n",
            "📄 6112.txt\n",
            "📄 201.txt\n",
            "📄 1107.txt\n",
            "📄 113.txt\n",
            "📄 7036.txt\n",
            "📄 3428.txt\n",
            "📄 2695.txt\n",
            "📄 6327.txt\n",
            "📄 2260.txt\n",
            "📄 4290.txt\n",
            "📄 516.txt\n",
            "📄 5402.txt\n",
            "📄 1116.txt\n",
            "📄 1575.txt\n",
            "📄 3498.txt\n",
            "📄 3730.txt\n",
            "📄 5529.txt\n",
            "📄 5765.txt\n",
            "📄 3878.txt\n",
            "📄 2684.txt\n",
            "📄 2058.txt\n",
            "📄 1291.txt\n",
            "📄 443.txt\n",
            "📄 3568.txt\n",
            "📄 2941.txt\n",
            "📄 5453.txt\n",
            "📄 837.txt\n",
            "📄 2313.txt\n",
            "📄 944.txt\n",
            "📄 2305.txt\n",
            "📄 2146.txt\n",
            "📄 6201.txt\n",
            "📄 6775.txt\n",
            "📄 3794.txt\n",
            "📄 4540.txt\n",
            "📄 5497.txt\n",
            "📄 469.txt\n",
            "📄 5744.txt\n",
            "📄 502.txt\n",
            "📄 423.txt\n",
            "📄 5423.txt\n",
            "📄 5846.txt\n",
            "📄 4185.txt\n",
            "📄 523.txt\n",
            "📄 76.txt\n",
            "📄 1397.txt\n",
            "📄 5660.txt\n",
            "📄 907.txt\n",
            "📄 5562.txt\n",
            "📄 1967.txt\n",
            "📄 2040.txt\n",
            "📄 3997.txt\n",
            "📄 2876.txt\n",
            "📄 4314.txt\n",
            "📄 4876.txt\n",
            "📄 4104.txt\n",
            "📄 6239.txt\n",
            "📄 5413.txt\n",
            "📄 4368.txt\n",
            "📄 1414.txt\n",
            "📄 3798.txt\n",
            "📄 2049.txt\n",
            "📄 1881.txt\n",
            "📄 3887.txt\n",
            "📄 353.txt\n",
            "📄 6248.txt\n",
            "📄 751.txt\n",
            "📄 3176.txt\n",
            "📄 3405.txt\n",
            "📄 776.txt\n",
            "📄 6301.txt\n",
            "📄 4783.txt\n",
            "📄 5930.txt\n",
            "📄 683.txt\n",
            "📄 1377.txt\n",
            "📄 2168.txt\n",
            "📄 363.txt\n",
            "📄 5242.txt\n",
            "📄 3910.txt\n",
            "📄 4170.txt\n",
            "📄 62.txt\n",
            "📄 5098.txt\n",
            "📄 1861.txt\n",
            "📄 5487.txt\n",
            "📄 2520.txt\n",
            "📄 2777.txt\n",
            "📄 4147.txt\n",
            "📄 5118.txt\n",
            "📄 4094.txt\n",
            "📄 6751.txt\n",
            "📄 3396.txt\n",
            "📄 6298.txt\n",
            "📄 4292.txt\n",
            "📄 2858.txt\n",
            "📄 3247.txt\n",
            "📄 4585.txt\n",
            "📄 5446.txt\n",
            "📄 4113.txt\n",
            "📄 1439.txt\n",
            "📄 1289.txt\n",
            "📄 4925.txt\n",
            "📄 4643.txt\n",
            "📄 2439.txt\n",
            "📄 3070.txt\n",
            "📄 2132.txt\n",
            "📄 4652.txt\n",
            "📄 304.txt\n",
            "📄 6453.txt\n",
            "📄 2471.txt\n",
            "📄 4683.txt\n",
            "📄 2671.txt\n",
            "📄 1010.txt\n",
            "📄 1449.txt\n",
            "📄 1741.txt\n",
            "📄 6508.txt\n",
            "📄 4936.txt\n",
            "📄 3293.txt\n",
            "📄 2566.txt\n",
            "📄 2814.txt\n",
            "📄 1269.txt\n",
            "📄 4776.txt\n",
            "📄 2691.txt\n",
            "📄 1512.txt\n",
            "📄 6931.txt\n",
            "📄 6496.txt\n",
            "📄 4788.txt\n",
            "📄 6590.txt\n",
            "📄 874.txt\n",
            "📄 5055.txt\n",
            "📄 3992.txt\n",
            "📄 5718.txt\n",
            "📄 1009.txt\n",
            "📄 4148.txt\n",
            "📄 2992.txt\n",
            "📄 2642.txt\n",
            "📄 4.txt\n",
            "📄 818.txt\n",
            "📄 4459.txt\n",
            "📄 5575.txt\n",
            "📄 5187.txt\n",
            "📄 1792.txt\n",
            "📄 1238.txt\n",
            "📄 2854.txt\n",
            "📄 3601.txt\n",
            "📄 1013.txt\n",
            "📄 2984.txt\n",
            "📄 2645.txt\n",
            "📄 6642.txt\n",
            "📄 6839.txt\n",
            "📄 1820.txt\n",
            "📄 3793.txt\n",
            "📄 5955.txt\n",
            "📄 3018.txt\n",
            "📄 3047.txt\n",
            "📄 109.txt\n",
            "📄 4992.txt\n",
            "📄 289.txt\n",
            "📄 1182.txt\n",
            "📄 1532.txt\n",
            "📄 3738.txt\n",
            "📄 489.txt\n",
            "📄 4905.txt\n",
            "📄 4196.txt\n",
            "📄 4657.txt\n",
            "📄 3218.txt\n",
            "📄 462.txt\n",
            "📄 930.txt\n",
            "📄 4512.txt\n",
            "📄 2804.txt\n",
            "📄 5221.txt\n",
            "📄 3158.txt\n",
            "📄 3322.txt\n",
            "📄 2508.txt\n",
            "📄 1994.txt\n",
            "📄 4910.txt\n",
            "📄 1717.txt\n",
            "📄 4354.txt\n",
            "📄 1618.txt\n",
            "📄 6594.txt\n",
            "📄 4591.txt\n",
            "📄 1026.txt\n",
            "📄 8.txt\n",
            "📄 1221.txt\n",
            "📄 4022.txt\n",
            "📄 4231.txt\n",
            "📄 5636.txt\n",
            "📄 3977.txt\n",
            "📄 1749.txt\n",
            "📄 4039.txt\n",
            "📄 3048.txt\n",
            "📄 2512.txt\n",
            "📄 163.txt\n",
            "📄 5623.txt\n",
            "📄 6067.txt\n",
            "📄 5303.txt\n",
            "📄 5708.txt\n",
            "📄 2087.txt\n",
            "📄 5120.txt\n",
            "📄 1722.txt\n",
            "📄 910.txt\n",
            "📄 4036.txt\n",
            "📄 2693.txt\n",
            "📄 1595.txt\n",
            "📄 2728.txt\n",
            "📄 425.txt\n",
            "📄 2987.txt\n",
            "📄 2169.txt\n",
            "📄 4724.txt\n",
            "📄 5430.txt\n",
            "📄 1880.txt\n",
            "📄 6745.txt\n",
            "📄 1641.txt\n",
            "📄 6534.txt\n",
            "📄 2175.txt\n",
            "📄 1935.txt\n",
            "📄 2850.txt\n",
            "📄 607.txt\n",
            "📄 1738.txt\n",
            "📄 4011.txt\n",
            "📄 1712.txt\n",
            "📄 2274.txt\n",
            "📄 1444.txt\n",
            "📄 2062.txt\n",
            "📄 2380.txt\n",
            "📄 3039.txt\n",
            "📄 6627.txt\n",
            "📄 6346.txt\n",
            "📄 30.txt\n",
            "📄 6548.txt\n",
            "📄 3330.txt\n",
            "📄 976.txt\n",
            "📄 148.txt\n",
            "📄 6314.txt\n",
            "📄 6207.txt\n",
            "📄 89.txt\n",
            "📄 6540.txt\n",
            "📄 5657.txt\n",
            "📄 2377.txt\n",
            "📄 249.txt\n",
            "📄 3488.txt\n",
            "📄 4391.txt\n",
            "📄 466.txt\n",
            "📄 3505.txt\n",
            "📄 6878.txt\n",
            "📄 2506.txt\n",
            "📄 3135.txt\n",
            "📄 4277.txt\n",
            "📄 6896.txt\n",
            "📄 6429.txt\n",
            "📄 2749.txt\n",
            "📄 6012.txt\n",
            "📄 1192.txt\n",
            "📄 1447.txt\n",
            "📄 5972.txt\n",
            "📄 1189.txt\n",
            "📄 2863.txt\n",
            "📄 1864.txt\n",
            "📄 5203.txt\n",
            "📄 114.txt\n",
            "📄 4386.txt\n",
            "📄 581.txt\n",
            "📄 3258.txt\n",
            "📄 2648.txt\n",
            "📄 3057.txt\n",
            "📄 4372.txt\n",
            "📄 6586.txt\n",
            "📄 1706.txt\n",
            "📄 4442.txt\n",
            "📄 6981.txt\n",
            "📄 862.txt\n",
            "📄 4460.txt\n",
            "📄 3521.txt\n",
            "📄 1525.txt\n",
            "📄 1196.txt\n",
            "📄 923.txt\n",
            "📄 6339.txt\n",
            "📄 4770.txt\n",
            "📄 2871.txt\n",
            "📄 4986.txt\n",
            "📄 4981.txt\n",
            "📄 3665.txt\n",
            "📄 2290.txt\n",
            "📄 2359.txt\n",
            "📄 4003.txt\n",
            "📄 4032.txt\n",
            "📄 6385.txt\n",
            "📄 2265.txt\n",
            "📄 1036.txt\n",
            "📄 486.txt\n",
            "📄 2611.txt\n",
            "📄 5016.txt\n",
            "📄 6727.txt\n",
            "📄 6718.txt\n",
            "📄 5282.txt\n",
            "📄 4537.txt\n",
            "📄 358.txt\n",
            "📄 6135.txt\n",
            "📄 3561.txt\n",
            "📄 1225.txt\n",
            "📄 1839.txt\n",
            "📄 3929.txt\n",
            "📄 4610.txt\n",
            "📄 236.txt\n",
            "📄 5652.txt\n",
            "📄 996.txt\n",
            "📄 1718.txt\n",
            "📄 1111.txt\n",
            "📄 216.txt\n",
            "📄 4299.txt\n",
            "📄 2174.txt\n",
            "📄 5206.txt\n",
            "📄 6028.txt\n",
            "📄 4199.txt\n",
            "📄 3448.txt\n",
            "📄 6306.txt\n",
            "📄 4830.txt\n",
            "📄 4789.txt\n",
            "📄 3083.txt\n",
            "📄 2449.txt\n",
            "📄 2469.txt\n",
            "📄 2990.txt\n",
            "📄 3706.txt\n",
            "📄 5647.txt\n",
            "📄 3331.txt\n",
            "📄 3586.txt\n",
            "📄 4505.txt\n",
            "📄 4606.txt\n",
            "📄 328.txt\n",
            "📄 343.txt\n",
            "📄 5815.txt\n",
            "📄 349.txt\n",
            "📄 702.txt\n",
            "📄 2237.txt\n",
            "📄 6999.txt\n",
            "📄 4397.txt\n",
            "📄 6167.txt\n",
            "📄 4962.txt\n",
            "📄 6783.txt\n",
            "📄 303.txt\n",
            "📄 5564.txt\n",
            "📄 1363.txt\n",
            "📄 620.txt\n",
            "📄 4909.txt\n",
            "📄 1639.txt\n",
            "📄 5174.txt\n",
            "📄 5378.txt\n",
            "📄 4987.txt\n",
            "📄 638.txt\n",
            "📄 1587.txt\n",
            "📄 273.txt\n",
            "📄 4085.txt\n",
            "📄 387.txt\n",
            "📄 4347.txt\n",
            "📄 3698.txt\n",
            "📄 376.txt\n",
            "📄 5076.txt\n",
            "📄 5139.txt\n",
            "📄 4763.txt\n",
            "📄 4072.txt\n",
            "📄 1821.txt\n",
            "📄 7024.txt\n",
            "📄 5760.txt\n",
            "📄 3493.txt\n",
            "📄 959.txt\n",
            "📄 5952.txt\n",
            "📄 3854.txt\n",
            "📄 4756.txt\n",
            "📄 2745.txt\n",
            "📄 6478.txt\n",
            "📄 6846.txt\n",
            "📄 5158.txt\n",
            "📄 3351.txt\n",
            "📄 5961.txt\n",
            "📄 1502.txt\n",
            "📄 250.txt\n",
            "📄 1887.txt\n",
            "📄 617.txt\n",
            "📄 1040.txt\n",
            "📄 1580.txt\n",
            "📄 2892.txt\n",
            "📄 5386.txt\n",
            "📄 4065.txt\n",
            "📄 5560.txt\n",
            "📄 4201.txt\n",
            "📄 6288.txt\n",
            "📄 3076.txt\n",
            "📄 1621.txt\n",
            "📄 244.txt\n",
            "📄 74.txt\n",
            "📄 7011.txt\n",
            "📄 6262.txt\n",
            "📄 5984.txt\n",
            "📄 588.txt\n",
            "📄 4219.txt\n",
            "📄 1354.txt\n",
            "📄 5092.txt\n",
            "📄 6513.txt\n",
            "📄 5003.txt\n",
            "📄 3279.txt\n",
            "📄 6939.txt\n",
            "📄 5554.txt\n",
            "📄 4421.txt\n",
            "📄 2129.txt\n",
            "📄 6844.txt\n",
            "📄 4846.txt\n",
            "📄 4367.txt\n",
            "📄 6073.txt\n",
            "📄 1878.txt\n",
            "📄 5294.txt\n",
            "📄 5149.txt\n",
            "📄 4727.txt\n",
            "📄 693.txt\n",
            "📄 6180.txt\n",
            "📄 877.txt\n",
            "📄 6484.txt\n",
            "📄 2481.txt\n",
            "📄 878.txt\n",
            "📄 2989.txt\n",
            "📄 6108.txt\n",
            "📄 6645.txt\n",
            "📄 6120.txt\n",
            "📄 789.txt\n",
            "📄 5405.txt\n",
            "📄 5872.txt\n",
            "📄 4151.txt\n",
            "📄 6563.txt\n",
            "📄 3934.txt\n",
            "📄 6795.txt\n",
            "📄 6460.txt\n",
            "📄 6393.txt\n",
            "📄 3159.txt\n",
            "📄 1345.txt\n",
            "📄 728.txt\n",
            "📄 1518.txt\n",
            "📄 40.txt\n",
            "📄 5155.txt\n",
            "📄 5910.txt\n",
            "📄 4662.txt\n",
            "📄 4775.txt\n",
            "📄 5108.txt\n",
            "📄 5313.txt\n",
            "📄 4766.txt\n",
            "📄 4934.txt\n",
            "📄 6365.txt\n",
            "📄 6733.txt\n",
            "📄 3599.txt\n",
            "📄 3089.txt\n",
            "📄 6873.txt\n",
            "📄 6554.txt\n",
            "📄 3805.txt\n",
            "📄 1080.txt\n",
            "📄 6147.txt\n",
            "📄 5184.txt\n",
            "📄 2270.txt\n",
            "📄 4845.txt\n",
            "📄 3596.txt\n",
            "📄 2879.txt\n",
            "📄 5322.txt\n",
            "📄 888.txt\n",
            "📄 2138.txt\n",
            "📄 990.txt\n",
            "📄 2589.txt\n",
            "📄 3914.txt\n",
            "📄 4723.txt\n",
            "📄 3999.txt\n",
            "📄 6283.txt\n",
            "📄 6193.txt\n",
            "📄 373.txt\n",
            "📄 6762.txt\n",
            "📄 5748.txt\n",
            "📄 646.txt\n",
            "📄 449.txt\n",
            "📄 3675.txt\n",
            "📄 2939.txt\n",
            "📄 5150.txt\n",
            "📄 2562.txt\n",
            "📄 5175.txt\n",
            "📄 346.txt\n",
            "📄 4564.txt\n",
            "📄 2468.txt\n",
            "📄 1418.txt\n",
            "📄 3824.txt\n",
            "📄 6380.txt\n",
            "📄 1513.txt\n",
            "📄 4437.txt\n",
            "📄 1947.txt\n",
            "📄 6461.txt\n",
            "📄 1883.txt\n",
            "📄 1504.txt\n",
            "📄 6814.txt\n",
            "📄 5798.txt\n",
            "📄 2411.txt\n",
            "📄 4732.txt\n",
            "📄 2636.txt\n",
            "📄 2320.txt\n",
            "📄 1627.txt\n",
            "📄 3563.txt\n",
            "📄 6748.txt\n",
            "📄 3420.txt\n",
            "📄 654.txt\n",
            "📄 2353.txt\n",
            "📄 6633.txt\n",
            "📄 2778.txt\n",
            "📄 1033.txt\n",
            "📄 1393.txt\n",
            "📄 6323.txt\n",
            "📄 4632.txt\n",
            "📄 1435.txt\n",
            "📄 1578.txt\n",
            "📄 2252.txt\n",
            "📄 507.txt\n",
            "📄 3140.txt\n",
            "📄 6692.txt\n",
            "📄 5238.txt\n",
            "📄 5244.txt\n",
            "📄 6826.txt\n",
            "📄 3728.txt\n",
            "📄 1082.txt\n",
            "📄 1136.txt\n",
            "📄 6595.txt\n",
            "📄 5319.txt\n",
            "📄 2877.txt\n",
            "📄 6482.txt\n",
            "📄 5230.txt\n",
            "📄 3226.txt\n",
            "📄 6022.txt\n",
            "📄 4947.txt\n",
            "📄 4348.txt\n",
            "📄 6410.txt\n",
            "📄 3244.txt\n",
            "📄 1457.txt\n",
            "📄 5483.txt\n",
            "📄 3770.txt\n",
            "📄 5392.txt\n",
            "📄 1159.txt\n",
            "📄 2977.txt\n",
            "📄 5510.txt\n",
            "📄 3050.txt\n",
            "📄 3806.txt\n",
            "📄 6908.txt\n",
            "📄 312.txt\n",
            "📄 6634.txt\n",
            "📄 5031.txt\n",
            "📄 3079.txt\n",
            "📄 2230.txt\n",
            "📄 5567.txt\n",
            "📄 3555.txt\n",
            "📄 3952.txt\n",
            "📄 20.txt\n",
            "📄 3661.txt\n",
            "📄 1775.txt\n",
            "📄 867.txt\n",
            "📄 1387.txt\n",
            "📄 2803.txt\n",
            "📄 2038.txt\n",
            "📄 476.txt\n",
            "📄 850.txt\n",
            "📄 631.txt\n",
            "📄 288.txt\n",
            "📄 6736.txt\n",
            "📄 3598.txt\n",
            "📄 6985.txt\n",
            "📄 5997.txt\n",
            "📄 2178.txt\n",
            "📄 6599.txt\n",
            "📄 6055.txt\n",
            "📄 1204.txt\n",
            "📄 2355.txt\n",
            "📄 5368.txt\n",
            "📄 57.txt\n",
            "📄 4272.txt\n",
            "📄 658.txt\n",
            "📄 4833.txt\n",
            "📄 535.txt\n",
            "📄 5502.txt\n",
            "📄 1782.txt\n",
            "📄 3001.txt\n",
            "📄 1859.txt\n",
            "📄 4642.txt\n",
            "📄 2198.txt\n",
            "📄 6467.txt\n",
            "📄 5028.txt\n",
            "📄 6812.txt\n",
            "📄 5388.txt\n",
            "📄 4251.txt\n",
            "📄 4175.txt\n",
            "📄 4287.txt\n",
            "📄 1827.txt\n",
            "📄 5856.txt\n",
            "📄 1853.txt\n",
            "📄 4599.txt\n",
            "📄 6819.txt\n",
            "📄 4268.txt\n",
            "📄 3136.txt\n",
            "📄 1609.txt\n",
            "📄 23.txt\n",
            "📄 6731.txt\n",
            "📄 426.txt\n",
            "📄 5412.txt\n",
            "📄 1924.txt\n",
            "📄 3424.txt\n",
            "📄 806.txt\n",
            "📄 2323.txt\n",
            "📄 6757.txt\n",
            "📄 2539.txt\n",
            "📄 2766.txt\n",
            "📄 3490.txt\n",
            "📄 1384.txt\n",
            "📄 5395.txt\n",
            "📄 739.txt\n",
            "📄 3435.txt\n",
            "📄 4476.txt\n",
            "📄 2011.txt\n",
            "📄 6716.txt\n",
            "📄 5265.txt\n",
            "📄 4617.txt\n",
            "📄 998.txt\n",
            "📄 392.txt\n",
            "📄 4220.txt\n",
            "📄 622.txt\n",
            "📄 3554.txt\n",
            "📄 2536.txt\n",
            "📄 1237.txt\n",
            "📄 4821.txt\n",
            "📄 3412.txt\n",
            "📄 3320.txt\n",
            "📄 3.txt\n",
            "📄 4897.txt\n",
            "📄 4659.txt\n",
            "📄 5982.txt\n",
            "📄 777.txt\n",
            "📄 2409.txt\n",
            "📄 1087.txt\n",
            "📄 4722.txt\n",
            "📄 6680.txt\n",
            "📄 1362.txt\n",
            "📄 3962.txt\n",
            "📄 3148.txt\n",
            "📄 1636.txt\n",
            "📄 3819.txt\n",
            "📄 1866.txt\n",
            "📄 6659.txt\n",
            "📄 2083.txt\n",
            "📄 807.txt\n",
            "📄 4870.txt\n",
            "📄 2817.txt\n",
            "📄 791.txt\n",
            "📄 6480.txt\n",
            "📄 820.txt\n",
            "📄 5924.txt\n",
            "📄 4834.txt\n",
            "📄 6337.txt\n",
            "📄 5010.txt\n",
            "📄 4237.txt\n",
            "📄 1750.txt\n",
            "📄 6559.txt\n",
            "📄 6418.txt\n",
            "📄 5770.txt\n",
            "📄 2907.txt\n",
            "📄 5374.txt\n",
            "📄 5474.txt\n",
            "📄 3103.txt\n",
            "📄 4868.txt\n",
            "📄 6864.txt\n",
            "📄 5025.txt\n",
            "📄 3324.txt\n",
            "📄 1426.txt\n",
            "📄 4098.txt\n",
            "📄 3073.txt\n",
            "📄 6329.txt\n",
            "📄 5135.txt\n",
            "📄 2792.txt\n",
            "📄 281.txt\n",
            "📄 1611.txt\n",
            "📄 5622.txt\n",
            "📄 4488.txt\n",
            "📄 6369.txt\n",
            "📄 4692.txt\n",
            "📄 583.txt\n",
            "📄 1015.txt\n",
            "📄 3890.txt\n",
            "📄 3280.txt\n",
            "📄 4323.txt\n",
            "📄 2810.txt\n",
            "📄 3950.txt\n",
            "📄 1215.txt\n",
            "📄 7056.txt\n",
            "📄 780.txt\n",
            "📄 4835.txt\n",
            "📄 321.txt\n",
            "📄 4816.txt\n",
            "📄 1928.txt\n",
            "📄 4667.txt\n",
            "📄 5109.txt\n",
            "📄 5256.txt\n",
            "📄 2280.txt\n",
            "📄 5792.txt\n",
            "📄 3430.txt\n",
            "📄 7068.txt\n",
            "📄 219.txt\n",
            "📄 4753.txt\n",
            "📄 3046.txt\n",
            "📄 7047.txt\n",
            "📄 4891.txt\n",
            "📄 5353.txt\n",
            "📄 3466.txt\n",
            "📄 3345.txt\n",
            "📄 5774.txt\n",
            "📄 1799.txt\n",
            "📄 5021.txt\n",
            "📄 455.txt\n",
            "📄 3207.txt\n",
            "📄 3350.txt\n",
            "📄 5308.txt\n",
            "📄 2659.txt\n",
            "📄 126.txt\n",
            "📄 2798.txt\n",
            "📄 3982.txt\n",
            "📄 1500.txt\n",
            "📄 1737.txt\n",
            "📄 6932.txt\n",
            "📄 2372.txt\n",
            "📄 4711.txt\n",
            "📄 5489.txt\n",
            "📄 5079.txt\n",
            "📄 2210.txt\n",
            "📄 6520.txt\n",
            "📄 338.txt\n",
            "📄 719.txt\n",
            "📄 1960.txt\n",
            "📄 4536.txt\n",
            "📄 811.txt\n",
            "📄 3686.txt\n",
            "📄 875.txt\n",
            "📄 2488.txt\n",
            "📄 2678.txt\n",
            "📄 5914.txt\n",
            "📄 4787.txt\n",
            "📄 2477.txt\n",
            "📄 4303.txt\n",
            "📄 5697.txt\n",
            "📄 1655.txt\n",
            "📄 3429.txt\n",
            "📄 32.txt\n",
            "📄 814.txt\n",
            "📄 5022.txt\n",
            "📄 684.txt\n",
            "📄 5521.txt\n",
            "📄 3752.txt\n",
            "📄 5585.txt\n",
            "📄 2276.txt\n",
            "📄 5843.txt\n",
            "📄 4741.txt\n",
            "📄 5210.txt\n",
            "📄 47.txt\n",
            "📄 2933.txt\n",
            "📄 4099.txt\n",
            "📄 4700.txt\n",
            "📄 4704.txt\n",
            "📄 2204.txt\n",
            "📄 5251.txt\n",
            "📄 3110.txt\n",
            "📄 1554.txt\n",
            "📄 4742.txt\n",
            "📄 3054.txt\n",
            "📄 3421.txt\n",
            "📄 5457.txt\n",
            "📄 2064.txt\n",
            "📄 4401.txt\n",
            "📄 506.txt\n",
            "📄 5802.txt\n",
            "📄 4008.txt\n",
            "📄 3666.txt\n",
            "📄 6772.txt\n",
            "📄 1593.txt\n",
            "📄 3227.txt\n",
            "📄 6605.txt\n",
            "📄 7054.txt\n",
            "📄 2605.txt\n",
            "📄 6940.txt\n",
            "📄 1149.txt\n",
            "📄 801.txt\n",
            "📄 2784.txt\n",
            "📄 144.txt\n",
            "📄 4200.txt\n",
            "📄 5100.txt\n",
            "📄 6830.txt\n",
            "📄 1381.txt\n",
            "📄 4454.txt\n",
            "📄 2909.txt\n",
            "📄 2682.txt\n",
            "📄 1544.txt\n",
            "📄 207.txt\n",
            "📄 6330.txt\n",
            "📄 2683.txt\n",
            "📄 5683.txt\n",
            "📄 5605.txt\n",
            "📄 2446.txt\n",
            "📄 1325.txt\n",
            "📄 3587.txt\n",
            "📄 5406.txt\n",
            "📄 4708.txt\n",
            "📄 1470.txt\n",
            "📄 6096.txt\n",
            "📄 398.txt\n",
            "📄 3374.txt\n",
            "📄 3085.txt\n",
            "📄 7112.txt\n",
            "📄 3167.txt\n",
            "📄 6654.txt\n",
            "📄 3026.txt\n",
            "📄 307.txt\n",
            "📄 2384.txt\n",
            "📄 6292.txt\n",
            "📄 5609.txt\n",
            "📄 1060.txt\n",
            "📄 2493.txt\n",
            "📄 2714.txt\n",
            "📄 3765.txt\n",
            "📄 6517.txt\n",
            "📄 3441.txt\n",
            "📄 2554.txt\n",
            "📄 4772.txt\n",
            "📄 2622.txt\n",
            "📄 6675.txt\n",
            "📄 374.txt\n",
            "📄 2855.txt\n",
            "📄 4689.txt\n",
            "📄 4439.txt\n",
            "📄 3879.txt\n",
            "📄 5606.txt\n",
            "📄 2126.txt\n",
            "📄 2706.txt\n",
            "📄 6788.txt\n",
            "📄 1573.txt\n",
            "📄 3993.txt\n",
            "📄 6289.txt\n",
            "📄 5547.txt\n",
            "📄 4654.txt\n",
            "📄 514.txt\n",
            "📄 4330.txt\n",
            "📄 5698.txt\n",
            "📄 942.txt\n",
            "📄 6431.txt\n",
            "📄 3198.txt\n",
            "📄 1517.txt\n",
            "📄 2540.txt\n",
            "📄 3464.txt\n",
            "📄 3242.txt\n",
            "📄 3455.txt\n",
            "📄 4703.txt\n",
            "📄 2587.txt\n",
            "📄 296.txt\n",
            "📄 245.txt\n",
            "📄 6588.txt\n",
            "📄 3570.txt\n",
            "📄 278.txt\n",
            "📄 416.txt\n",
            "📄 987.txt\n",
            "📄 2209.txt\n",
            "📄 5077.txt\n",
            "📄 2322.txt\n",
            "📄 3605.txt\n",
            "📄 2113.txt\n",
            "📄 2014.txt\n",
            "📄 6613.txt\n",
            "📄 1171.txt\n",
            "📄 3656.txt\n",
            "📄 1654.txt\n",
            "📄 4332.txt\n",
            "📄 3264.txt\n",
            "📄 3741.txt\n",
            "📄 3565.txt\n",
            "📄 4949.txt\n",
            "📄 3479.txt\n",
            "📄 5728.txt\n",
            "📄 1892.txt\n",
            "📄 4719.txt\n",
            "📄 6437.txt\n",
            "📄 6020.txt\n",
            "📄 5464.txt\n",
            "📄 815.txt\n",
            "📄 2874.txt\n",
            "📄 5939.txt\n",
            "📄 4970.txt\n",
            "📄 6809.txt\n",
            "📄 6286.txt\n",
            "📄 4943.txt\n",
            "📄 6935.txt\n",
            "📄 6742.txt\n",
            "📄 721.txt\n",
            "📄 4169.txt\n",
            "📄 6743.txt\n",
            "📄 4853.txt\n",
            "📄 4446.txt\n",
            "📄 6039.txt\n",
            "📄 2846.txt\n",
            "📄 1663.txt\n",
            "📄 927.txt\n",
            "📄 3319.txt\n",
            "📄 2915.txt\n",
            "📄 831.txt\n",
            "📄 2091.txt\n",
            "📄 5243.txt\n",
            "📄 1232.txt\n",
            "📄 1099.txt\n",
            "📄 2394.txt\n",
            "📄 6404.txt\n",
            "📄 5638.txt\n",
            "📄 7100.txt\n",
            "📄 4791.txt\n",
            "📄 2077.txt\n",
            "📄 4961.txt\n",
            "📄 4761.txt\n",
            "📄 5167.txt\n",
            "📄 3219.txt\n",
            "📄 632.txt\n",
            "📄 6507.txt\n",
            "📄 1969.txt\n",
            "📄 5948.txt\n",
            "📄 2112.txt\n",
            "📄 3381.txt\n",
            "📄 1805.txt\n",
            "📄 6616.txt\n",
            "📄 1176.txt\n",
            "📄 2455.txt\n",
            "📄 1468.txt\n",
            "📄 1606.txt\n",
            "📄 1703.txt\n",
            "📄 50.txt\n",
            "📄 2729.txt\n",
            "📄 6414.txt\n",
            "📄 2925.txt\n",
            "📄 1138.txt\n",
            "📄 4462.txt\n",
            "📄 2590.txt\n",
            "📄 5534.txt\n",
            "📄 2208.txt\n",
            "📄 1809.txt\n",
            "📄 3753.txt\n",
            "📄 2013.txt\n",
            "📄 294.txt\n",
            "📄 1770.txt\n",
            "📄 6032.txt\n",
            "📄 3755.txt\n",
            "📄 531.txt\n",
            "📄 1383.txt\n",
            "📄 7061.txt\n",
            "📄 6340.txt\n",
            "📄 5274.txt\n",
            "📄 1234.txt\n",
            "📄 3152.txt\n",
            "📄 4720.txt\n",
            "📄 2514.txt\n",
            "📄 3326.txt\n",
            "📄 3368.txt\n",
            "📄 6774.txt\n",
            "📄 6790.txt\n",
            "📄 1795.txt\n",
            "📄 3837.txt\n",
            "📄 4809.txt\n",
            "📄 3773.txt\n",
            "📄 4456.txt\n",
            "📄 1551.txt\n",
            "📄 1434.txt\n",
            "📄 6784.txt\n",
            "📄 734.txt\n",
            "📄 2979.txt\n",
            "📄 4670.txt\n",
            "📄 3469.txt\n",
            "📄 4333.txt\n",
            "📄 6335.txt\n",
            "📄 369.txt\n",
            "📄 5569.txt\n",
            "📄 6600.txt\n",
            "📄 5816.txt\n",
            "📄 1604.txt\n",
            "📄 2306.txt\n",
            "📄 6836.txt\n",
            "📄 1282.txt\n",
            "📄 2089.txt\n",
            "📄 4950.txt\n",
            "📄 3557.txt\n",
            "📄 4298.txt\n",
            "📄 6333.txt\n",
            "📄 1194.txt\n",
            "📄 193.txt\n",
            "📄 3582.txt\n",
            "📄 6646.txt\n",
            "📄 5312.txt\n",
            "📄 4381.txt\n",
            "📄 2568.txt\n",
            "📄 2217.txt\n",
            "📄 3808.txt\n",
            "📄 3832.txt\n",
            "📄 554.txt\n",
            "📄 726.txt\n",
            "📄 2456.txt\n",
            "📄 4485.txt\n",
            "📄 247.txt\n",
            "📄 1836.txt\n",
            "📄 3759.txt\n",
            "📄 2381.txt\n",
            "📄 1996.txt\n",
            "📄 3957.txt\n",
            "📄 2824.txt\n",
            "📄 5689.txt\n",
            "📄 105.txt\n",
            "📄 4628.txt\n",
            "📄 2371.txt\n",
            "📄 6708.txt\n",
            "📄 6527.txt\n",
            "📄 3337.txt\n",
            "📄 274.txt\n",
            "📄 5389.txt\n",
            "📄 187.txt\n",
            "📄 4582.txt\n",
            "📄 4431.txt\n",
            "📄 4855.txt\n",
            "📄 2726.txt\n",
            "📄 135.txt\n",
            "📄 6915.txt\n",
            "📄 5901.txt\n",
            "📄 3402.txt\n",
            "📄 5923.txt\n",
            "📄 31.txt\n",
            "📄 5830.txt\n",
            "📄 3916.txt\n",
            "📄 4383.txt\n",
            "📄 6361.txt\n",
            "📄 5729.txt\n",
            "📄 5911.txt\n",
            "📄 1173.txt\n",
            "📄 2845.txt\n",
            "📄 666.txt\n",
            "📄 1625.txt\n",
            "📄 3795.txt\n",
            "📄 1690.txt\n",
            "📄 5903.txt\n",
            "📄 292.txt\n",
            "📄 2596.txt\n",
            "📄 4534.txt\n",
            "📄 2624.txt\n",
            "📄 5469.txt\n",
            "📄 5725.txt\n",
            "📄 1235.txt\n",
            "📄 2349.txt\n",
            "📄 6792.txt\n",
            "📄 1277.txt\n",
            "📄 3390.txt\n",
            "📄 7059.txt\n",
            "📄 4965.txt\n",
            "📄 6901.txt\n",
            "📄 86.txt\n",
            "📄 768.txt\n",
            "📄 5842.txt\n",
            "📄 2680.txt\n",
            "📄 595.txt\n",
            "📄 6296.txt\n",
            "📄 4037.txt\n",
            "📄 5583.txt\n",
            "📄 6181.txt\n",
            "📄 3711.txt\n",
            "📄 4453.txt\n",
            "📄 6849.txt\n",
            "📄 4128.txt\n",
            "📄 6638.txt\n",
            "📄 5272.txt\n",
            "📄 1849.txt\n",
            "📄 6117.txt\n",
            "📄 4686.txt\n",
            "📄 3796.txt\n",
            "📄 1299.txt\n",
            "📄 3816.txt\n",
            "📄 1950.txt\n",
            "📄 226.txt\n",
            "📄 4239.txt\n",
            "📄 5653.txt\n",
            "📄 6119.txt\n",
            "📄 3672.txt\n",
            "📄 4684.txt\n",
            "📄 1585.txt\n",
            "📄 3786.txt\n",
            "📄 5044.txt\n",
            "📄 435.txt\n",
            "📄 4966.txt\n",
            "📄 246.txt\n",
            "📄 3256.txt\n",
            "📄 2118.txt\n",
            "📄 7104.txt\n",
            "📄 1258.txt\n",
            "📄 4392.txt\n",
            "📄 3059.txt\n",
            "📄 1326.txt\n",
            "📄 3913.txt\n",
            "📄 2362.txt\n",
            "📄 5148.txt\n",
            "📄 7126.txt\n",
            "📄 2340.txt\n",
            "📄 1806.txt\n",
            "📄 3400.txt\n",
            "📄 6132.txt\n",
            "📄 6628.txt\n",
            "📄 653.txt\n",
            "📄 5946.txt\n",
            "📄 1790.txt\n",
            "📄 5020.txt\n",
            "📄 271.txt\n",
            "📄 59.txt\n",
            "📄 3370.txt\n",
            "📄 4601.txt\n",
            "📄 1566.txt\n",
            "📄 4059.txt\n",
            "📄 1017.txt\n",
            "📄 1984.txt\n",
            "📄 323.txt\n",
            "📄 6684.txt\n",
            "📄 5678.txt\n",
            "📄 3585.txt\n",
            "📄 4608.txt\n",
            "📄 5229.txt\n",
            "📄 3099.txt\n",
            "📄 2980.txt\n",
            "📄 1419.txt\n",
            "📄 4428.txt\n",
            "📄 3184.txt\n",
            "📄 668.txt\n",
            "📄 1454.txt\n",
            "📄 1574.txt\n",
            "📄 1058.txt\n",
            "📄 1716.txt\n",
            "📄 6390.txt\n",
            "📄 6392.txt\n",
            "📄 4434.txt\n",
            "📄 2193.txt\n",
            "📄 1857.txt\n",
            "📄 396.txt\n",
            "📄 3714.txt\n",
            "📄 1448.txt\n",
            "📄 2140.txt\n",
            "📄 4713.txt\n",
            "📄 1921.txt\n",
            "📄 2275.txt\n",
            "📄 6623.txt\n",
            "📄 3779.txt\n",
            "📄 2189.txt\n",
            "📄 6100.txt\n",
            "📄 4033.txt\n",
            "📄 4600.txt\n",
            "📄 970.txt\n",
            "📄 4076.txt\n",
            "📄 3936.txt\n",
            "📄 5629.txt\n",
            "📄 1125.txt\n",
            "📄 1083.txt\n",
            "📄 5899.txt\n",
            "📄 604.txt\n",
            "📄 391.txt\n",
            "📄 51.txt\n",
            "📄 3683.txt\n",
            "📄 3863.txt\n",
            "📄 5619.txt\n",
            "📄 3841.txt\n",
            "📄 372.txt\n",
            "📄 6993.txt\n",
            "📄 570.txt\n",
            "📄 4131.txt\n",
            "📄 4820.txt\n",
            "📄 797.txt\n",
            "📄 5595.txt\n",
            "📄 5063.txt\n",
            "📄 5271.txt\n",
            "📄 5860.txt\n",
            "📄 2300.txt\n",
            "📄 6725.txt\n",
            "📄 4183.txt\n",
            "📄 2759.txt\n",
            "📄 2571.txt\n",
            "📄 852.txt\n",
            "📄 6472.txt\n",
            "📄 1109.txt\n",
            "📄 132.txt\n",
            "📄 856.txt\n",
            "📄 5631.txt\n",
            "📄 5214.txt\n",
            "📄 1495.txt\n",
            "📄 2703.txt\n",
            "📄 6489.txt\n",
            "📄 3804.txt\n",
            "📄 5612.txt\n",
            "📄 2608.txt\n",
            "📄 1294.txt\n",
            "📄 3996.txt\n",
            "📄 5115.txt\n",
            "📄 639.txt\n",
            "📄 3867.txt\n",
            "📄 1230.txt\n",
            "📄 819.txt\n",
            "📄 4886.txt\n",
            "📄 4004.txt\n",
            "📄 5372.txt\n",
            "📄 418.txt\n",
            "📄 821.txt\n",
            "📄 2418.txt\n",
            "📄 5857.txt\n",
            "📄 1038.txt\n",
            "📄 3445.txt\n",
            "📄 2672.txt\n",
            "📄 3968.txt\n",
            "📄 6344.txt\n",
            "📄 6698.txt\n",
            "📄 2072.txt\n",
            "📄 5361.txt\n",
            "📄 1787.txt\n",
            "📄 5591.txt\n",
            "📄 2263.txt\n",
            "📄 5304.txt\n",
            "📄 1154.txt\n",
            "📄 6891.txt\n",
            "📄 4875.txt\n",
            "📄 4849.txt\n",
            "📄 1157.txt\n",
            "📄 218.txt\n",
            "📄 4867.txt\n",
            "📄 195.txt\n",
            "📄 2163.txt\n",
            "📄 6143.txt\n",
            "📄 1800.txt\n",
            "📄 606.txt\n",
            "📄 235.txt\n",
            "📄 45.txt\n",
            "📄 4668.txt\n",
            "📄 882.txt\n",
            "📄 3905.txt\n",
            "📄 2954.txt\n",
            "📄 41.txt\n",
            "📄 6781.txt\n",
            "📄 367.txt\n",
            "📄 3560.txt\n",
            "📄 4945.txt\n",
            "📄 1686.txt\n",
            "📄 2934.txt\n",
            "📄 3439.txt\n",
            "📄 127.txt\n",
            "📄 5429.txt\n",
            "📄 5778.txt\n",
            "📄 1930.txt\n",
            "📄 4216.txt\n",
            "📄 2654.txt\n",
            "📄 3236.txt\n",
            "📄 5302.txt\n",
            "📄 384.txt\n",
            "📄 4254.txt\n",
            "📄 605.txt\n",
            "📄 4471.txt\n",
            "📄 2414.txt\n",
            "📄 2420.txt\n",
            "📄 4052.txt\n",
            "📄 2165.txt\n",
            "📄 4123.txt\n",
            "📄 3695.txt\n",
            "📄 1191.txt\n",
            "📄 1596.txt\n",
            "📄 4095.txt\n",
            "📄 326.txt\n",
            "📄 499.txt\n",
            "📄 1308.txt\n",
            "📄 565.txt\n",
            "📄 2200.txt\n",
            "📄 7032.txt\n",
            "📄 3631.txt\n",
            "📄 6583.txt\n",
            "📄 5720.txt\n",
            "📄 1217.txt\n",
            "📄 3281.txt\n",
            "📄 4747.txt\n",
            "📄 4709.txt\n",
            "📄 4440.txt\n",
            "📄 3535.txt\n",
            "📄 1582.txt\n",
            "📄 2515.txt\n",
            "📄 5563.txt\n",
            "📄 1023.txt\n",
            "📄 590.txt\n",
            "📄 1465.txt\n",
            "📄 4060.txt\n",
            "📄 4125.txt\n",
            "📄 1624.txt\n",
            "📄 4179.txt\n",
            "📄 3380.txt\n",
            "📄 6887.txt\n",
            "📄 209.txt\n",
            "📄 6726.txt\n",
            "📄 3882.txt\n",
            "📄 7097.txt\n",
            "📄 3651.txt\n",
            "📄 5220.txt\n",
            "📄 3098.txt\n",
            "📄 3911.txt\n",
            "📄 5694.txt\n",
            "📄 6381.txt\n",
            "📄 1698.txt\n",
            "📄 918.txt\n",
            "📄 4120.txt\n",
            "📄 1451.txt\n",
            "📄 2152.txt\n",
            "📄 4525.txt\n",
            "📄 6179.txt\n",
            "📄 6991.txt\n",
            "📄 3571.txt\n",
            "📄 3958.txt\n",
            "📄 6483.txt\n",
            "📄 1938.txt\n",
            "📄 2919.txt\n",
            "📄 4826.txt\n",
            "📄 4271.txt\n",
            "📄 6946.txt\n",
            "📄 1293.txt\n",
            "📄 6913.txt\n",
            "📄 644.txt\n",
            "📄 3349.txt\n",
            "📄 4523.txt\n",
            "📄 3942.txt\n",
            "📄 1838.txt\n",
            "📄 762.txt\n",
            "📄 6898.txt\n",
            "📄 6006.txt\n",
            "📄 594.txt\n",
            "📄 6026.txt\n",
            "📄 5277.txt\n",
            "📄 1380.txt\n",
            "📄 6184.txt\n",
            "📄 5249.txt\n",
            "📄 865.txt\n",
            "📄 3932.txt\n",
            "📄 1484.txt\n",
            "📄 3371.txt\n",
            "📄 5574.txt\n",
            "📄 1203.txt\n",
            "📄 237.txt\n",
            "📄 515.txt\n",
            "📄 754.txt\n",
            "📄 4024.txt\n",
            "📄 6457.txt\n",
            "📄 1975.txt\n",
            "📄 6438.txt\n",
            "📄 6456.txt\n",
            "📄 5208.txt\n",
            "📄 6331.txt\n",
            "📄 6399.txt\n",
            "📄 33.txt\n",
            "📄 6550.txt\n",
            "📄 70.txt\n",
            "📄 4336.txt\n",
            "📄 6422.txt\n",
            "📄 4940.txt\n",
            "📄 53.txt\n",
            "📄 6982.txt\n",
            "📄 1199.txt\n",
            "📄 5202.txt\n",
            "📄 197.txt\n",
            "📄 6263.txt\n",
            "📄 6944.txt\n",
            "📄 6441.txt\n",
            "📄 3348.txt\n",
            "📄 4087.txt\n",
            "📄 156.txt\n",
            "📄 3377.txt\n",
            "📄 1255.txt\n",
            "📄 1724.txt\n",
            "📄 3092.txt\n",
            "📄 4387.txt\n",
            "📄 2100.txt\n",
            "📄 1483.txt\n",
            "📄 3530.txt\n",
            "📄 6735.txt\n",
            "📄 6395.txt\n",
            "📄 5362.txt\n",
            "📄 4519.txt\n",
            "📄 174.txt\n",
            "📄 703.txt\n",
            "📄 1093.txt\n",
            "📄 5169.txt\n",
            "📄 66.txt\n",
            "📄 2192.txt\n",
            "📄 3459.txt\n",
            "📄 5219.txt\n",
            "📄 1926.txt\n",
            "📄 65.txt\n",
            "📄 3022.txt\n",
            "📄 1965.txt\n",
            "📄 3989.txt\n",
            "📄 2613.txt\n",
            "📄 389.txt\n",
            "📄 5582.txt\n",
            "📄 5795.txt\n",
            "📄 5568.txt\n",
            "📄 1990.txt\n",
            "📄 1509.txt\n",
            "📄 1202.txt\n",
            "📄 4133.txt\n",
            "📄 2435.txt\n",
            "📄 598.txt\n",
            "📄 1656.txt\n",
            "📄 4806.txt\n",
            "📄 825.txt\n",
            "📄 809.txt\n",
            "📄 4012.txt\n",
            "📄 1077.txt\n",
            "📄 6801.txt\n",
            "📄 6378.txt\n",
            "📄 6396.txt\n",
            "📄 2369.txt\n",
            "📄 3522.txt\n",
            "📄 6102.txt\n",
            "📄 3273.txt\n",
            "📄 2996.txt\n",
            "📄 5782.txt\n",
            "📄 69.txt\n",
            "📄 1068.txt\n",
            "📄 1966.txt\n",
            "📄 6242.txt\n",
            "📄 1963.txt\n",
            "📄 2971.txt\n",
            "📄 5329.txt\n",
            "📄 6699.txt\n",
            "📄 3869.txt\n",
            "📄 2993.txt\n",
            "📄 849.txt\n",
            "📄 4417.txt\n",
            "📄 5351.txt\n",
            "📄 434.txt\n",
            "📄 3853.txt\n",
            "📄 799.txt\n",
            "📄 2815.txt\n",
            "📄 2617.txt\n",
            "📄 2050.txt\n",
            "📄 6688.txt\n",
            "📄 1466.txt\n",
            "📄 98.txt\n",
            "📄 2597.txt\n",
            "📄 1756.txt\n",
            "📄 3514.txt\n",
            "📄 1431.txt\n",
            "📄 729.txt\n",
            "📄 1665.txt\n",
            "📄 4177.txt\n",
            "📄 3856.txt\n",
            "📄 4243.txt\n",
            "📄 191.txt\n",
            "📄 146.txt\n",
            "📄 2054.txt\n",
            "📄 950.txt\n",
            "📄 3923.txt\n",
            "📄 5387.txt\n",
            "📄 164.txt\n",
            "📄 360.txt\n",
            "📄 610.txt\n",
            "📄 2115.txt\n",
            "📄 1896.txt\n",
            "📄 3717.txt\n",
            "📄 2183.txt\n",
            "📄 5002.txt\n",
            "📄 4450.txt\n",
            "📄 6171.txt\n",
            "📄 3161.txt\n",
            "📄 2553.txt\n",
            "📄 4885.txt\n",
            "📄 6200.txt\n",
            "📄 6309.txt\n",
            "📄 4045.txt\n",
            "📄 2873.txt\n",
            "📄 855.txt\n",
            "📄 1679.txt\n",
            "📄 3768.txt\n",
            "📄 6992.txt\n",
            "📄 5133.txt\n",
            "📄 1659.txt\n",
            "📄 6116.txt\n",
            "📄 5082.txt\n",
            "📄 5145.txt\n",
            "📄 143.txt\n",
            "📄 2136.txt\n",
            "📄 3623.txt\n",
            "📄 6831.txt\n",
            "📄 1155.txt\n",
            "📄 4893.txt\n",
            "📄 6189.txt\n",
            "📄 6182.txt\n",
            "📄 6416.txt\n",
            "📄 3375.txt\n",
            "📄 4715.txt\n",
            "📄 6941.txt\n",
            "📄 1479.txt\n",
            "📄 4114.txt\n",
            "📄 3763.txt\n",
            "📄 2008.txt\n",
            "📄 5292.txt\n",
            "📄 3532.txt\n",
            "📄 1271.txt\n",
            "📄 4645.txt\n",
            "📄 1428.txt\n",
            "📄 5204.txt\n",
            "📄 5973.txt\n",
            "📄 4027.txt\n",
            "📄 2483.txt\n",
            "📄 298.txt\n",
            "📄 851.txt\n",
            "📄 4084.txt\n",
            "📄 6279.txt\n",
            "📄 1666.txt\n",
            "📄 2453.txt\n",
            "📄 7055.txt\n",
            "📄 893.txt\n",
            "📄 1535.txt\n",
            "📄 4038.txt\n",
            "📄 1501.txt\n",
            "📄 1548.txt\n",
            "📄 1184.txt\n",
            "📄 4390.txt\n",
            "📄 999.txt\n",
            "📄 948.txt\n",
            "📄 1758.txt\n",
            "📄 796.txt\n",
            "📄 4209.txt\n",
            "📄 2224.txt\n",
            "📄 2264.txt\n",
            "📄 238.txt\n",
            "📄 7038.txt\n",
            "📄 4839.txt\n",
            "📄 2367.txt\n",
            "📄 4901.txt\n",
            "📄 3117.txt\n",
            "📄 6798.txt\n",
            "📄 3077.txt\n",
            "📄 4716.txt\n",
            "📄 7053.txt\n",
            "📄 6724.txt\n",
            "📄 7033.txt\n",
            "📄 2127.txt\n",
            "📄 5976.txt\n",
            "📄 6243.txt\n",
            "📄 2742.txt\n",
            "📄 6357.txt\n",
            "📄 747.txt\n",
            "📄 803.txt\n",
            "📄 1962.txt\n",
            "📄 7015.txt\n",
            "📄 2730.txt\n",
            "📄 2227.txt\n",
            "📄 1469.txt\n",
            "📄 3868.txt\n",
            "📄 361.txt\n",
            "📄 202.txt\n",
            "📄 5659.txt\n",
            "📄 1682.txt\n",
            "📄 3081.txt\n",
            "📄 2516.txt\n",
            "📄 419.txt\n",
            "📄 138.txt\n",
            "📄 1220.txt\n",
            "📄 2908.txt\n",
            "📄 7014.txt\n",
            "📄 5492.txt\n",
            "📄 5891.txt\n",
            "📄 3055.txt\n",
            "📄 4223.txt\n",
            "📄 2521.txt\n",
            "📄 184.txt\n",
            "📄 3970.txt\n",
            "📄 5081.txt\n",
            "📄 2840.txt\n",
            "📄 108.txt\n",
            "📄 5897.txt\n",
            "📄 6802.txt\n",
            "📄 3062.txt\n",
            "📄 5006.txt\n",
            "📄 5182.txt\n",
            "📄 7050.txt\n",
            "📄 3304.txt\n",
            "📄 1142.txt\n",
            "📄 280.txt\n",
            "📄 3597.txt\n",
            "📄 6126.txt\n",
            "📄 980.txt\n",
            "📄 864.txt\n",
            "📄 1247.txt\n",
            "📄 714.txt\n",
            "📄 4418.txt\n",
            "📄 5912.txt\n",
            "📄 4416.txt\n",
            "📄 1908.txt\n",
            "📄 1024.txt\n",
            "📄 5094.txt\n",
            "📄 2592.txt\n",
            "📄 6683.txt\n",
            "📄 3476.txt\n",
            "📄 6208.txt\n",
            "📄 4665.txt\n",
            "📄 3389.txt\n",
            "📄 3354.txt\n",
            "📄 4016.txt\n",
            "📄 5440.txt\n",
            "📄 36.txt\n",
            "📄 5826.txt\n",
            "📄 4811.txt\n",
            "📄 133.txt\n",
            "📄 6581.txt\n",
            "📄 3517.txt\n",
            "📄 6173.txt\n",
            "📄 2273.txt\n",
            "📄 4952.txt\n",
            "📄 5433.txt\n",
            "📄 5367.txt\n",
            "📄 5073.txt\n",
            "📄 6678.txt\n",
            "📄 7120.txt\n",
            "📄 4186.txt\n",
            "📄 4404.txt\n",
            "📄 4634.txt\n",
            "📄 2233.txt\n",
            "📄 4650.txt\n",
            "📄 4409.txt\n",
            "📄 6093.txt\n",
            "📄 3589.txt\n",
            "📄 1704.txt\n",
            "📄 5781.txt\n",
            "📄 6029.txt\n",
            "📄 794.txt\n",
            "📄 5101.txt\n",
            "📄 4396.txt\n",
            "📄 6744.txt\n",
            "📄 537.txt\n",
            "📄 3253.txt\n",
            "📄 5841.txt\n",
            "📄 473.txt\n",
            "📄 4055.txt\n",
            "📄 3170.txt\n",
            "📄 6353.txt\n",
            "📄 4882.txt\n",
            "📄 3834.txt\n",
            "📄 5817.txt\n",
            "📄 5161.txt\n",
            "📄 3724.txt\n",
            "📄 2981.txt\n",
            "📄 4249.txt\n",
            "📄 4831.txt\n",
            "📄 6019.txt\n",
            "📄 4264.txt\n",
            "📄 4918.txt\n",
            "📄 1555.txt\n",
            "📄 2767.txt\n",
            "📄 1837.txt\n",
            "📄 3617.txt\n",
            "📄 1150.txt\n",
            "📄 4202.txt\n",
            "📄 3703.txt\n",
            "📄 5233.txt\n",
            "📄 5262.txt\n",
            "📄 2246.txt\n",
            "📄 2662.txt\n",
            "📄 4877.txt\n",
            "📄 4210.txt\n",
            "📄 6515.txt\n",
            "📄 5985.txt\n",
            "📄 928.txt\n",
            "📄 6593.txt\n",
            "📄 546.txt\n",
            "📄 6086.txt\n",
            "📄 3859.txt\n",
            "📄 6013.txt\n",
            "📄 635.txt\n",
            "📄 1610.txt\n",
            "📄 1699.txt\n",
            "📄 6050.txt\n",
            "📄 1408.txt\n",
            "📄 4762.txt\n",
            "📄 6651.txt\n",
            "📄 4921.txt\n",
            "📄 3898.txt\n",
            "📄 1208.txt\n",
            "📄 1030.txt\n",
            "📄 1642.txt\n",
            "📄 1430.txt\n",
            "📄 5673.txt\n",
            "📄 4550.txt\n",
            "📄 5205.txt\n",
            "📄 5239.txt\n",
            "📄 478.txt\n",
            "📄 4136.txt\n",
            "📄 5996.txt\n",
            "📄 3012.txt\n",
            "📄 5980.txt\n",
            "📄 6700.txt\n",
            "📄 3592.txt\n",
            "📄 198.txt\n",
            "📄 2782.txt\n",
            "📄 901.txt\n",
            "📄 5089.txt\n",
            "📄 846.txt\n",
            "📄 501.txt\n",
            "📄 3352.txt\n",
            "📄 1252.txt\n",
            "📄 2326.txt\n",
            "📄 7.txt\n",
            "📄 4376.txt\n",
            "📄 5331.txt\n",
            "📄 2457.txt\n",
            "📄 956.txt\n",
            "📄 3383.txt\n",
            "📄 4074.txt\n",
            "📄 485.txt\n",
            "📄 5932.txt\n",
            "📄 2722.txt\n",
            "📄 6959.txt\n",
            "📄 2557.txt\n",
            "📄 3615.txt\n",
            "📄 350.txt\n",
            "📄 6854.txt\n",
            "📄 253.txt\n",
            "📄 6974.txt\n",
            "📄 408.txt\n",
            "📄 2661.txt\n",
            "📄 1870.txt\n",
            "📄 2739.txt\n",
            "📄 6951.txt\n",
            "📄 4107.txt\n",
            "📄 2465.txt\n",
            "📄 997.txt\n",
            "📄 5934.txt\n",
            "📄 2407.txt\n",
            "📄 1956.txt\n",
            "📄 5690.txt\n",
            "📄 558.txt\n",
            "📄 1873.txt\n",
            "📄 4141.txt\n",
            "📄 3642.txt\n",
            "📄 2763.txt\n",
            "📄 1436.txt\n",
            "📄 4207.txt\n",
            "📄 2109.txt\n",
            "📄 4844.txt\n",
            "📄 5066.txt\n",
            "📄 1317.txt\n",
            "📄 1634.txt\n",
            "📄 6764.txt\n",
            "📄 5766.txt\n",
            "📄 5557.txt\n",
            "📄 6241.txt\n",
            "📄 1037.txt\n",
            "📄 5370.txt\n",
            "📄 5186.txt\n",
            "📄 6786.txt\n",
            "📄 972.txt\n",
            "📄 2955.txt\n",
            "📄 1745.txt\n",
            "📄 4503.txt\n",
            "📄 5850.txt\n",
            "📄 931.txt\n",
            "📄 6174.txt\n",
            "📄 1851.txt\n",
            "📄 7119.txt\n",
            "📄 364.txt\n",
            "📄 3288.txt\n",
            "📄 2628.txt\n",
            "📄 3053.txt\n",
            "📄 5607.txt\n",
            "📄 3013.txt\n",
            "📄 3129.txt\n",
            "📄 208.txt\n",
            "📄 6918.txt\n",
            "📄 3725.txt\n",
            "📄 3127.txt\n",
            "📄 817.txt\n",
            "📄 5530.txt\n",
            "📄 5962.txt\n",
            "📄 1382.txt\n",
            "📄 3021.txt\n",
            "📄 4245.txt\n",
            "📄 3477.txt\n",
            "📄 3245.txt\n",
            "📄 1223.txt\n",
            "📄 5936.txt\n",
            "📄 7008.txt\n",
            "📄 1946.txt\n",
            "📄 461.txt\n",
            "📄 5414.txt\n",
            "📄 4160.txt\n",
            "📄 7131.txt\n",
            "📄 4255.txt\n",
            "📄 5316.txt\n",
            "📄 4217.txt\n",
            "📄 6194.txt\n",
            "📄 960.txt\n",
            "📄 5876.txt\n",
            "📄 1871.txt\n",
            "📄 1581.txt\n",
            "📄 6899.txt\n",
            "📄 6532.txt\n",
            "📄 5620.txt\n",
            "📄 204.txt\n",
            "📄 6037.txt\n",
            "📄 1944.txt\n",
            "📄 2433.txt\n",
            "📄 2618.txt\n",
            "📄 6240.txt\n",
            "📄 6671.txt\n",
            "📄 4246.txt\n",
            "📄 3658.txt\n",
            "📄 4261.txt\n",
            "📄 1391.txt\n",
            "📄 2063.txt\n",
            "📄 2188.txt\n",
            "📄 5348.txt\n",
            "📄 3444.txt\n",
            "📄 413.txt\n",
            "📄 1971.txt\n",
            "📄 6544.txt\n",
            "📄 24.txt\n",
            "📄 698.txt\n",
            "📄 2150.txt\n",
            "📄 5532.txt\n",
            "📄 3432.txt\n",
            "📄 6955.txt\n",
            "📄 4887.txt\n",
            "📄 771.txt\n",
            "📄 4489.txt\n",
            "📄 4017.txt\n",
            "📄 3301.txt\n",
            "📄 2155.txt\n",
            "📄 5819.txt\n",
            "📄 1556.txt\n",
            "📄 3342.txt\n",
            "📄 551.txt\n",
            "📄 2866.txt\n",
            "📄 4583.txt\n",
            "📄 6204.txt\n",
            "📄 213.txt\n",
            "📄 1653.txt\n",
            "📄 2948.txt\n",
            "📄 3801.txt\n",
            "📄 732.txt\n",
            "📄 1759.txt\n",
            "📄 2026.txt\n",
            "📄 2084.txt\n",
            "📄 6912.txt\n",
            "📄 6567.txt\n",
            "📄 6643.txt\n",
            "📄 6440.txt\n",
            "📄 1894.txt\n",
            "📄 4423.txt\n",
            "📄 6805.txt\n",
            "📄 1130.txt\n",
            "📄 2800.txt\n",
            "📄 1352.txt\n",
            "📄 2176.txt\n",
            "📄 6312.txt\n",
            "📄 261.txt\n",
            "📄 1233.txt\n",
            "📄 6172.txt\n",
            "📄 1710.txt\n",
            "📄 6324.txt\n",
            "📄 3917.txt\n",
            "📄 3315.txt\n",
            "📄 5550.txt\n",
            "📄 879.txt\n",
            "📄 4464.txt\n",
            "📄 1834.txt\n",
            "📄 3851.txt\n",
            "📄 5811.txt\n",
            "📄 3011.txt\n",
            "📄 2839.txt\n",
            "📄 4062.txt\n",
            "📄 3885.txt\n",
            "📄 4907.txt\n",
            "📄 6444.txt\n",
            "📄 4944.txt\n",
            "📄 3487.txt\n",
            "📄 2986.txt\n",
            "📄 1823.txt\n",
            "📄 1696.txt\n",
            "📄 2186.txt\n",
            "📄 1590.txt\n",
            "📄 7117.txt\n",
            "📄 7102.txt\n",
            "📄 4315.txt\n",
            "📄 3865.txt\n",
            "📄 786.txt\n",
            "📄 267.txt\n",
            "📄 6265.txt\n",
            "📄 7018.txt\n",
            "📄 5898.txt\n",
            "📄 5806.txt\n",
            "📄 6281.txt\n",
            "📄 4146.txt\n",
            "📄 5922.txt\n",
            "📄 1148.txt\n",
            "📄 1002.txt\n",
            "📄 5129.txt\n",
            "📄 5975.txt\n",
            "📄 5290.txt\n",
            "📄 6920.txt\n",
            "📄 2074.txt\n",
            "📄 6937.txt\n",
            "📄 5824.txt\n",
            "📄 5855.txt\n",
            "📄 3971.txt\n",
            "📄 7045.txt\n",
            "📄 3028.txt\n",
            "📄 3739.txt\n",
            "📄 1376.txt\n",
            "📄 6562.txt\n",
            "📄 5460.txt\n",
            "📄 4998.txt\n",
            "📄 694.txt\n",
            "📄 3986.txt\n",
            "📄 585.txt\n",
            "📄 6674.txt\n",
            "📄 2018.txt\n",
            "📄 3480.txt\n",
            "📄 474.txt\n",
            "📄 6099.txt\n",
            "📄 562.txt\n",
            "📄 6052.txt\n",
            "📄 2836.txt\n",
            "📄 6479.txt\n",
            "📄 774.txt\n",
            "📄 5716.txt\n",
            "📄 3787.txt\n",
            "📄 5247.txt\n",
            "📄 1139.txt\n",
            "📄 593.txt\n",
            "📄 4895.txt\n",
            "📄 2531.txt\n",
            "📄 475.txt\n",
            "📄 5420.txt\n",
            "📄 2805.txt\n",
            "📄 2006.txt\n",
            "📄 6388.txt\n",
            "📄 6134.txt\n",
            "📄 1044.txt\n",
            "📄 5588.txt\n",
            "📄 5628.txt\n",
            "📄 5511.txt\n",
            "📄 6487.txt\n",
            "📄 6057.txt\n",
            "📄 1615.txt\n",
            "📄 6689.txt\n",
            "📄 1295.txt\n",
            "📄 3150.txt\n",
            "📄 14.txt\n",
            "📄 6185.txt\n",
            "📄 2958.txt\n",
            "📄 3332.txt\n",
            "📄 812.txt\n",
            "📄 5978.txt\n",
            "📄 4629.txt\n",
            "📄 529.txt\n",
            "📄 5478.txt\n",
            "📄 4865.txt\n",
            "📄 5124.txt\n",
            "📄 6877.txt\n",
            "📄 5862.txt\n",
            "📄 6477.txt\n",
            "📄 5375.txt\n",
            "📄 2668.txt\n",
            "📄 2764.txt\n",
            "📄 1163.txt\n",
            "📄 2012.txt\n",
            "📄 5140.txt\n",
            "📄 2647.txt\n",
            "📄 5969.txt\n",
            "📄 6451.txt\n",
            "📄 841.txt\n",
            "📄 5540.txt\n",
            "📄 3196.txt\n",
            "📄 6114.txt\n",
            "📄 1292.txt\n",
            "📄 4086.txt\n",
            "📄 4447.txt\n",
            "📄 1400.txt\n",
            "📄 3438.txt\n",
            "📄 7127.txt\n",
            "📄 3000.txt\n",
            "📄 2797.txt\n",
            "📄 870.txt\n",
            "📄 895.txt\n",
            "📄 2375.txt\n",
            "📄 701.txt\n",
            "📄 4913.txt\n",
            "📄 3677.txt\n",
            "📄 3671.txt\n",
            "📄 6074.txt\n",
            "📄 3082.txt\n",
            "📄 1156.txt\n",
            "📄 611.txt\n",
            "📄 3015.txt\n",
            "📄 5543.txt\n",
            "📄 351.txt\n",
            "📄 4777.txt\n",
            "📄 6247.txt\n",
            "📄 5380.txt\n",
            "📄 3799.txt\n",
            "📄 5731.txt\n",
            "📄 4194.txt\n",
            "📄 3995.txt\n",
            "📄 3132.txt\n",
            "📄 1285.txt\n",
            "📄 5366.txt\n",
            "📄 3369.txt\n",
            "📄 2825.txt\n",
            "📄 1228.txt\n",
            "📄 3131.txt\n",
            "📄 1210.txt\n",
            "📄 5342.txt\n",
            "📄 5052.txt\n",
            "📄 177.txt\n",
            "📄 4609.txt\n",
            "📄 5864.txt\n",
            "📄 3538.txt\n",
            "📄 3460.txt\n",
            "📄 1090.txt\n",
            "📄 4233.txt\n",
            "📄 4790.txt\n",
            "📄 2470.txt\n",
            "📄 578.txt\n",
            "📄 1050.txt\n",
            "📄 4581.txt\n",
            "📄 1146.txt\n",
            "📄 3249.txt\n",
            "📄 1301.txt\n",
            "📄 3468.txt\n",
            "📄 4793.txt\n",
            "📄 1562.txt\n",
            "📄 6705.txt\n",
            "📄 4484.txt\n",
            "📄 5527.txt\n",
            "📄 3499.txt\n",
            "📄 4658.txt\n",
            "📄 6191.txt\n",
            "📄 3583.txt\n",
            "📄 2001.txt\n",
            "📄 4590.txt\n",
            "📄 1523.txt\n",
            "📄 7118.txt\n",
            "📄 5307.txt\n",
            "📄 4973.txt\n",
            "📄 5834.txt\n",
            "📄 43.txt\n",
            "📄 2602.txt\n",
            "📄 5194.txt\n",
            "📄 1995.txt\n",
            "📄 3065.txt\n",
            "📄 6469.txt\n",
            "📄 4545.txt\n",
            "📄 4213.txt\n",
            "📄 325.txt\n",
            "📄 4313.txt\n",
            "📄 1069.txt\n",
            "📄 3139.txt\n",
            "📄 3086.txt\n",
            "📄 1053.txt\n",
            "📄 2522.txt\n",
            "📄 5357.txt\n",
            "📄 2715.txt\n",
            "📄 3512.txt\n",
            "📄 2930.txt\n",
            "📄 975.txt\n",
            "📄 6470.txt\n",
            "📄 6636.txt\n",
            "📄 1757.txt\n",
            "📄 1152.txt\n",
            "📄 6706.txt\n",
            "📄 6817.txt\n",
            "📄 6229.txt\n",
            "📄 3423.txt\n",
            "📄 4702.txt\n",
            "📄 582.txt\n",
            "📄 6137.txt\n",
            "📄 2184.txt\n",
            "📄 6083.txt\n",
            "📄 6447.txt\n",
            "📄 4132.txt\n",
            "📄 2491.txt\n",
            "📄 2912.txt\n",
            "📄 6349.txt\n",
            "📄 6954.txt\n",
            "📄 2482.txt\n",
            "📄 2283.txt\n",
            "📄 3815.txt\n",
            "📄 7129.txt\n",
            "📄 538.txt\n",
            "📄 1747.txt\n",
            "📄 679.txt\n",
            "📄 6564.txt\n",
            "📄 1816.txt\n",
            "📄 4515.txt\n",
            "📄 2291.txt\n",
            "📄 5602.txt\n",
            "📄 3166.txt\n",
            "📄 4379.txt\n",
            "📄 4034.txt\n",
            "📄 4924.txt\n",
            "📄 4101.txt\n",
            "📄 4414.txt\n",
            "📄 579.txt\n",
            "📄 4637.txt\n",
            "📄 6761.txt\n",
            "📄 2242.txt\n",
            "📄 6027.txt\n",
            "📄 6942.txt\n",
            "📄 2391.txt\n",
            "📄 3791.txt\n",
            "📄 5648.txt\n",
            "📄 1664.txt\n",
            "📄 3848.txt\n",
            "📄 3290.txt\n",
            "📄 417.txt\n",
            "📄 7124.txt\n",
            "📄 3248.txt\n",
            "📄 4466.txt\n",
            "📄 6770.txt\n",
            "📄 1648.txt\n",
            "📄 6129.txt\n",
            "📄 4295.txt\n",
            "📄 3123.txt\n",
            "📄 5539.txt\n",
            "📄 5627.txt\n",
            "📄 2625.txt\n",
            "📄 4221.txt\n",
            "📄 5034.txt\n",
            "📄 513.txt\n",
            "📄 5041.txt\n",
            "📄 3580.txt\n",
            "📄 3453.txt\n",
            "📄 1668.txt\n",
            "📄 5065.txt\n",
            "📄 3014.txt\n",
            "📄 5751.txt\n",
            "📄 2185.txt\n",
            "📄 6492.txt\n",
            "📄 4090.txt\n",
            "📄 530.txt\n",
            "📄 5418.txt\n",
            "📄 1066.txt\n",
            "📄 2039.txt\n",
            "📄 2098.txt\n",
            "📄 3616.txt\n",
            "📄 886.txt\n",
            "📄 3491.txt\n",
            "📄 6002.txt\n",
            "📄 6347.txt\n",
            "📄 5821.txt\n",
            "📄 2917.txt\n",
            "📄 2911.txt\n",
            "📄 2285.txt\n",
            "📄 6493.txt\n",
            "📄 1117.txt\n",
            "📄 1903.txt\n",
            "📄 1248.txt\n",
            "📄 1421.txt\n",
            "📄 6409.txt\n",
            "📄 2982.txt\n",
            "📄 4430.txt\n",
            "📄 3239.txt\n",
            "📄 6620.txt\n",
            "📄 4355.txt\n",
            "📄 3222.txt\n",
            "📄 6905.txt\n",
            "📄 561.txt\n",
            "📄 2324.txt\n",
            "📄 4433.txt\n",
            "📄 974.txt\n",
            "📄 909.txt\n",
            "📄 6152.txt\n",
            "📄 3463.txt\n",
            "📄 6755.txt\n",
            "📄 3203.txt\n",
            "📄 3296.txt\n",
            "📄 7044.txt\n",
            "📄 6088.txt\n",
            "📄 6068.txt\n",
            "📄 1899.txt\n",
            "📄 6644.txt\n",
            "📄 1597.txt\n",
            "📄 4544.txt\n",
            "📄 5107.txt\n",
            "📄 1296.txt\n",
            "📄 2674.txt\n",
            "📄 357.txt\n",
            "📄 4171.txt\n",
            "📄 3384.txt\n",
            "📄 2121.txt\n",
            "📄 2278.txt\n",
            "📄 6711.txt\n",
            "📄 805.txt\n",
            "📄 3646.txt\n",
            "📄 5358.txt\n",
            "📄 6128.txt\n",
            "📄 5216.txt\n",
            "📄 223.txt\n",
            "📄 5163.txt\n",
            "📄 5544.txt\n",
            "📄 5796.txt\n",
            "📄 564.txt\n",
            "📄 1452.txt\n",
            "📄 3394.txt\n",
            "📄 5921.txt\n",
            "📄 5112.txt\n",
            "📄 4467.txt\n",
            "📄 1250.txt\n",
            "📄 3457.txt\n",
            "📄 5695.txt\n",
            "📄 6368.txt\n",
            "📄 6676.txt\n",
            "📄 3006.txt\n",
            "📄 6904.txt\n",
            "📄 1793.txt\n",
            "📄 2402.txt\n",
            "📄 2890.txt\n",
            "📄 1064.txt\n",
            "📄 129.txt\n",
            "📄 4889.txt\n",
            "📄 3325.txt\n",
            "📄 5311.txt\n",
            "📄 995.txt\n",
            "📄 788.txt\n",
            "📄 3614.txt\n",
            "📄 2997.txt\n",
            "📄 3802.txt\n",
            "📄 5723.txt\n",
            "📄 3031.txt\n",
            "📄 5885.txt\n",
            "📄 7076.txt\n",
            "📄 3643.txt\n",
            "📄 2424.txt\n",
            "📄 5737.txt\n",
            "📄 2222.txt\n",
            "📄 772.txt\n",
            "📄 4621.txt\n",
            "📄 5222.txt\n",
            "📄 3221.txt\n",
            "📄 5125.txt\n",
            "📄 5809.txt\n",
            "📄 1764.txt\n",
            "📄 4856.txt\n",
            "📄 5908.txt\n",
            "📄 5134.txt\n",
            "📄 5808.txt\n",
            "📄 4931.txt\n",
            "📄 477.txt\n",
            "📄 1158.txt\n",
            "📄 4764.txt\n",
            "📄 2171.txt\n",
            "📄 3756.txt\n",
            "📄 4750.txt\n",
            "📄 4130.txt\n",
            "📄 5257.txt\n",
            "📄 5670.txt\n",
            "📄 259.txt\n",
            "📄 6260.txt\n",
            "📄 3922.txt\n",
            "📄 1688.txt\n",
            "📄 3576.txt\n",
            "📄 4013.txt\n",
            "📄 4188.txt\n",
            "📄 4785.txt\n",
            "📄 2615.txt\n",
            "📄 2400.txt\n",
            "📄 2404.txt\n",
            "📄 6876.txt\n",
            "📄 6094.txt\n",
            "📄 6070.txt\n",
            "📄 2786.txt\n",
            "📄 6212.txt\n",
            "📄 1559.txt\n",
            "📄 2451.txt\n",
            "📄 1071.txt\n",
            "📄 3709.txt\n",
            "📄 3339.txt\n",
            "📄 3818.txt\n",
            "📄 1481.txt\n",
            "📄 5988.txt\n",
            "📄 2770.txt\n",
            "📄 894.txt\n",
            "📄 5577.txt\n",
            "📄 6948.txt\n",
            "📄 2.txt\n",
            "📄 3690.txt\n",
            "📄 3433.txt\n",
            "📄 6893.txt\n",
            "📄 5054.txt\n",
            "📄 2936.txt\n",
            "📄 2737.txt\n",
            "📄 3931.txt\n",
            "📄 7034.txt\n",
            "📄 937.txt\n",
            "📄 3437.txt\n",
            "📄 3194.txt\n",
            "📄 3190.txt\n",
            "📄 2464.txt\n",
            "📄 2476.txt\n",
            "📄 1216.txt\n",
            "📄 2053.txt\n",
            "📄 4234.txt\n",
            "📄 276.txt\n",
            "📄 2214.txt\n",
            "📄 6017.txt\n",
            "📄 2036.txt\n",
            "📄 3749.txt\n",
            "📄 938.txt\n",
            "📄 5724.txt\n",
            "📄 5036.txt\n",
            "📄 269.txt\n",
            "📄 3413.txt\n",
            "📄 4054.txt\n",
            "📄 4779.txt\n",
            "📄 3886.txt\n",
            "📄 6160.txt\n",
            "📄 1429.txt\n",
            "📄 6570.txt\n",
            "📄 6031.txt\n",
            "📄 7108.txt\n",
            "📄 252.txt\n",
            "📄 706.txt\n",
            "📄 1520.txt\n",
            "📄 2650.txt\n",
            "📄 4155.txt\n",
            "📄 446.txt\n",
            "📄 2331.txt\n",
            "📄 4498.txt\n",
            "📄 6650.txt\n",
            "📄 3347.txt\n",
            "📄 6278.txt\n",
            "📄 6958.txt\n",
            "📄 1579.txt\n",
            "📄 2298.txt\n",
            "📄 2601.txt\n",
            "📄 2888.txt\n",
            "📄 1063.txt\n",
            "📄 3393.txt\n",
            "📄 840.txt\n",
            "📄 3700.txt\n",
            "📄 1812.txt\n",
            "📄 964.txt\n",
            "📄 1045.txt\n",
            "📄 1569.txt\n",
            "📄 2069.txt\n",
            "📄 5049.txt\n",
            "📄 2523.txt\n",
            "📄 2361.txt\n",
            "📄 2022.txt\n",
            "📄 636.txt\n",
            "📄 16.txt\n",
            "📄 386.txt\n",
            "📄 4066.txt\n",
            "📄 1876.txt\n",
            "📄 1458.txt\n",
            "📄 6228.txt\n",
            "📄 902.txt\n",
            "📄 2426.txt\n",
            "📄 2123.txt\n",
            "📄 4457.txt\n",
            "📄 4639.txt\n",
            "📄 5450.txt\n",
            "📄 6154.txt\n",
            "📄 5103.txt\n",
            "📄 5894.txt\n",
            "📄 5884.txt\n",
            "📄 4814.txt\n",
            "📄 4326.txt\n",
            "📄 2808.txt\n",
            "📄 5633.txt\n",
            "📄 3849.txt\n",
            "📄 3751.txt\n",
            "📄 1781.txt\n",
            "📄 5680.txt\n",
            "📄 2699.txt\n",
            "📄 27.txt\n",
            "📄 4859.txt\n",
            "📄 6328.txt\n",
            "📄 1847.txt\n",
            "📄 4784.txt\n",
            "📄 1371.txt\n",
            "📄 1200.txt\n",
            "📄 2828.txt\n",
            "📄 1489.txt\n",
            "📄 1494.txt\n",
            "📄 6983.txt\n",
            "📄 6364.txt\n",
            "📄 6203.txt\n",
            "📄 4691.txt\n",
            "📄 453.txt\n",
            "📄 118.txt\n",
            "📄 957.txt\n",
            "📄 6165.txt\n",
            "📄 4093.txt\n",
            "📄 2350.txt\n",
            "📄 2490.txt\n",
            "📄 6886.txt\n",
            "📄 5431.txt\n",
            "📄 4382.txt\n",
            "📄 6641.txt\n",
            "📄 4384.txt\n",
            "📄 2881.txt\n",
            "📄 3116.txt\n",
            "📄 2329.txt\n",
            "📄 3771.txt\n",
            "📄 456.txt\n",
            "📄 898.txt\n",
            "📄 2819.txt\n",
            "📄 124.txt\n",
            "📄 7005.txt\n",
            "📄 3748.txt\n",
            "📄 6439.txt\n",
            "📄 6825.txt\n",
            "📄 6423.txt\n",
            "📄 3201.txt\n",
            "📄 3212.txt\n",
            "📄 5192.txt\n",
            "📄 215.txt\n",
            "📄 4373.txt\n",
            "📄 6740.txt\n",
            "📄 6695.txt\n",
            "📄 4518.txt\n",
            "📄 6038.txt\n",
            "📄 6927.txt\n",
            "📄 6420.txt\n",
            "📄 4759.txt\n",
            "📄 1410.txt\n",
            "📄 1095.txt\n",
            "📄 5330.txt\n",
            "📄 7004.txt\n",
            "📄 3496.txt\n",
            "📄 5565.txt\n",
            "📄 2368.txt\n",
            "📄 317.txt\n",
            "📄 3710.txt\n",
            "📄 2791.txt\n",
            "📄 859.txt\n",
            "📄 6210.txt\n",
            "📄 3338.txt\n",
            "📄 6821.txt\n",
            "📄 6717.txt\n",
            "📄 908.txt\n",
            "📄 3836.txt\n",
            "📄 4824.txt\n",
            "📄 6458.txt\n",
            "📄 3149.txt\n",
            "📄 5164.txt\n",
            "📄 602.txt\n",
            "📄 572.txt\n",
            "📄 4851.txt\n",
            "📄 5674.txt\n",
            "📄 9.txt\n",
            "📄 4979.txt\n",
            "📄 6624.txt\n",
            "📄 92.txt\n",
            "📄 470.txt\n",
            "📄 5867.txt\n",
            "📄 6434.txt\n",
            "📄 5904.txt\n",
            "📄 1993.txt\n",
            "📄 3976.txt\n",
            "📄 3866.txt\n",
            "📄 860.txt\n",
            "📄 6980.txt\n",
            "📄 6269.txt\n",
            "📄 5170.txt\n",
            "📄 1872.txt\n",
            "📄 1437.txt\n",
            "📄 6840.txt\n",
            "📄 4866.txt\n",
            "📄 1333.txt\n",
            "📄 1835.txt\n",
            "📄 5784.txt\n",
            "📄 1725.txt\n",
            "📄 5753.txt\n",
            "📄 2220.txt\n",
            "📄 3954.txt\n",
            "📄 5102.txt\n",
            "📄 5616.txt\n",
            "📄 2327.txt\n",
            "📄 5621.txt\n",
            "📄 1266.txt\n",
            "📄 1398.txt\n",
            "📄 2921.txt\n",
            "📄 4914.txt\n",
            "📄 2365.txt\n",
            "📄 212.txt\n",
            "📄 6225.txt\n",
            "📄 1180.txt\n",
            "📄 659.txt\n",
            "📄 4829.txt\n",
            "📄 2757.txt\n",
            "📄 3199.txt\n",
            "📄 6066.txt\n",
            "📄 2856.txt\n",
            "📄 3895.txt\n",
            "📄 6374.txt\n",
            "📄 769.txt\n",
            "📄 4126.txt\n",
            "📄 5234.txt\n",
            "📄 5963.txt\n",
            "📄 3946.txt\n",
            "📄 3179.txt\n",
            "📄 4204.txt\n",
            "📄 5126.txt\n",
            "📄 3951.txt\n",
            "📄 4888.txt\n",
            "📄 397.txt\n",
            "📄 2581.txt\n",
            "📄 5246.txt\n",
            "📄 2723.txt\n",
            "📄 1046.txt\n",
            "📄 4357.txt\n",
            "📄 4903.txt\n",
            "📄 6033.txt\n",
            "📄 6833.txt\n",
            "📄 7099.txt\n",
            "📄 2743.txt\n",
            "📄 5589.txt\n",
            "📄 5447.txt\n",
            "📄 3278.txt\n",
            "📄 6911.txt\n",
            "📄 388.txt\n",
            "📄 1106.txt\n",
            "📄 3588.txt\n",
            "📄 5225.txt\n",
            "📄 2701.txt\n",
            "📄 1743.txt\n",
            "📄 2103.txt\n",
            "📄 3681.txt\n",
            "📄 3165.txt\n",
            "📄 994.txt\n",
            "📄 6789.txt\n",
            "📄 5260.txt\n",
            "📄 3260.txt\n",
            "📄 3213.txt\n",
            "📄 488.txt\n",
            "📄 4422.txt\n",
            "📄 1680.txt\n",
            "📄 230.txt\n",
            "📄 5580.txt\n",
            "📄 151.txt\n",
            "📄 1685.txt\n",
            "📄 6056.txt\n",
            "📄 6021.txt\n",
            "📄 4572.txt\n",
            "📄 5586.txt\n",
            "📄 4751.txt\n",
            "📄 6166.txt\n",
            "📄 6528.txt\n",
            "📄 4573.txt\n",
            "📄 3757.txt\n",
            "📄 3918.txt\n",
            "📄 6797.txt\n",
            "📄 3689.txt\n",
            "📄 4872.txt\n",
            "📄 2535.txt\n",
            "📄 519.txt\n",
            "📄 504.txt\n",
            "📄 6259.txt\n",
            "📄 4135.txt\n",
            "📄 1274.txt\n",
            "📄 4014.txt\n",
            "📄 1897.txt\n",
            "📄 6354.txt\n",
            "📄 5116.txt\n",
            "📄 1635.txt\n",
            "📄 3205.txt\n",
            "📄 4754.txt\n",
            "📄 665.txt\n",
            "📄 1662.txt\n",
            "📄 5803.txt\n",
            "📄 991.txt\n",
            "📄 4041.txt\n",
            "📄 4890.txt\n",
            "📄 172.txt\n",
            "📄 6914.txt\n",
            "📄 4378.txt\n",
            "📄 965.txt\n",
            "📄 3074.txt\n",
            "📄 826.txt\n",
            "📄 843.txt\n",
            "📄 2935.txt\n",
            "📄 6343.txt\n",
            "📄 5193.txt\n",
            "📄 3105.txt\n",
            "📄 525.txt\n",
            "📄 4714.txt\n",
            "📄 335.txt\n",
            "📄 2599.txt\n",
            "📄 3636.txt\n",
            "📄 1335.txt\n",
            "📄 5227.txt\n",
            "📄 2244.txt\n",
            "📄 6558.txt\n",
            "📄 5974.txt\n",
            "📄 2660.txt\n",
            "📄 1145.txt\n",
            "📄 4051.txt\n",
            "📄 5827.txt\n",
            "📄 2139.txt\n",
            "📄 917.txt\n",
            "📄 1032.txt\n",
            "📄 84.txt\n",
            "📄 4312.txt\n",
            "📄 1012.txt\n",
            "📄 696.txt\n",
            "📄 268.txt\n",
            "📄 5712.txt\n",
            "📄 5347.txt\n",
            "📄 6297.txt\n",
            "📄 4152.txt\n",
            "📄 2147.txt\n",
            "📄 6848.txt\n",
            "📄 6804.txt\n",
            "📄 3852.txt\n",
            "📄 6253.txt\n",
            "📄 2637.txt\n",
            "📄 7069.txt\n",
            "📄 3088.txt\n",
            "📄 1920.txt\n",
            "📄 159.txt\n",
            "📄 463.txt\n",
            "📄 3458.txt\n",
            "📄 1425.txt\n",
            "📄 4541.txt\n",
            "📄 5854.txt\n",
            "📄 6034.txt\n",
            "📄 2634.txt\n",
            "📄 3729.txt\n",
            "📄 827.txt\n",
            "📄 4527.txt\n",
            "📄 4325.txt\n",
            "📄 6998.txt\n",
            "📄 4285.txt\n",
            "📄 1344.txt\n",
            "📄 1715.txt\n",
            "📄 2137.txt\n",
            "📄 1565.txt\n",
            "📄 1856.txt\n",
            "📄 4352.txt\n",
            "📄 4115.txt\n",
            "📄 3271.txt\n",
            "📄 6996.txt\n",
            "📄 510.txt\n",
            "📄 1222.txt\n",
            "📄 5553.txt\n",
            "📄 3904.txt\n",
            "📄 7040.txt\n",
            "📄 4854.txt\n",
            "📄 48.txt\n",
            "📄 3909.txt\n",
            "📄 1981.txt\n",
            "📄 5084.txt\n",
            "📄 2277.txt\n",
            "📄 5024.txt\n",
            "📄 4385.txt\n",
            "📄 5814.txt\n",
            "📄 4478.txt\n",
            "📄 985.txt\n",
            "📄 5196.txt\n",
            "📄 6566.txt\n",
            "📄 7095.txt\n",
            "📄 3392.txt\n",
            "📄 1122.txt\n",
            "📄 3365.txt\n",
            "📄 4974.txt\n",
            "📄 80.txt\n",
            "📄 2746.txt\n",
            "📄 5570.txt\n",
            "📄 4605.txt\n",
            "📄 6962.txt\n",
            "📄 3298.txt\n",
            "📄 4363.txt\n",
            "📄 5572.txt\n",
            "📄 5590.txt\n",
            "📄 3115.txt\n",
            "📄 5767.txt\n",
            "📄 4983.txt\n",
            "📄 1374.txt\n",
            "📄 6811.txt\n",
            "📄 680.txt\n",
            "📄 947.txt\n",
            "📄 1441.txt\n",
            "📄 2061.txt\n",
            "📄 6501.txt\n",
            "📄 4259.txt\n",
            "📄 169.txt\n",
            "📄 4927.txt\n",
            "📄 5315.txt\n",
            "📄 5306.txt\n",
            "📄 4554.txt\n",
            "📄 6619.txt\n",
            "📄 4413.txt\n",
            "📄 4143.txt\n",
            "📄 3174.txt\n",
            "📄 52.txt\n",
            "📄 1461.txt\n",
            "📄 97.txt\n",
            "📄 983.txt\n",
            "📄 2983.txt\n",
            "📄 4007.txt\n",
            "📄 3360.txt\n",
            "📄 6308.txt\n",
            "📄 4603.txt\n",
            "📄 3484.txt\n",
            "📄 6592.txt\n",
            "📄 6266.txt\n",
            "📄 2719.txt\n",
            "📄 3187.txt\n",
            "📄 1599.txt\n",
            "📄 4982.txt\n",
            "📄 5038.txt\n",
            "📄 6382.txt\n",
            "📄 6113.txt\n",
            "📄 1771.txt\n",
            "📄 2736.txt\n",
            "📄 5185.txt\n",
            "📄 3358.txt\n",
            "📄 6661.txt\n",
            "📄 1140.txt\n",
            "📄 6186.txt\n",
            "📄 6383.txt\n",
            "📄 1557.txt\n",
            "📄 1337.txt\n",
            "📄 6280.txt\n",
            "📄 5525.txt\n",
            "📄 2998.txt\n",
            "📄 2352.txt\n",
            "📄 1401.txt\n",
            "📄 7009.txt\n",
            "📄 72.txt\n",
            "📄 1940.txt\n",
            "📄 881.txt\n",
            "📄 5285.txt\n",
            "📄 954.txt\n",
            "📄 3790.txt\n",
            "📄 5576.txt\n",
            "📄 834.txt\n",
            "📄 4019.txt\n",
            "📄 1700.txt\n",
            "📄 221.txt\n",
            "📄 3685.txt\n",
            "📄 669.txt\n",
            "📄 3500.txt\n",
            "📄 709.txt\n",
            "📄 936.txt\n",
            "📄 1603.txt\n",
            "📄 1404.txt\n",
            "📄 6968.txt\n",
            "📄 5491.txt\n",
            "📄 6474.txt\n",
            "📄 5541.txt\n",
            "📄 4335.txt\n",
            "📄 5062.txt\n",
            "📄 3254.txt\n",
            "📄 1510.txt\n",
            "📄 4647.txt\n",
            "📄 3044.txt\n",
            "📄 3551.txt\n",
            "📄 6945.txt\n",
            "📄 4817.txt\n",
            "📄 4677.txt\n",
            "📄 1958.txt\n",
            "📄 4048.txt\n",
            "📄 741.txt\n",
            "📄 5940.txt\n",
            "📄 171.txt\n",
            "📄 1506.txt\n",
            "📄 6024.txt\n",
            "📄 4043.txt\n",
            "📄 3263.txt\n",
            "📄 2538.txt\n",
            "📄 7060.txt\n",
            "📄 5600.txt\n",
            "📄 3892.txt\n",
            "📄 3875.txt\n",
            "📄 4625.txt\n",
            "📄 4693.txt\n",
            "📄 3169.txt\n",
            "📄 3042.txt\n",
            "📄 5505.txt\n",
            "📄 5758.txt\n",
            "📄 3008.txt\n",
            "📄 1783.txt\n",
            "📄 5581.txt\n",
            "📄 5929.txt\n",
            "📄 4623.txt\n",
            "📄 5040.txt\n",
            "📄 4502.txt\n",
            "📄 5738.txt\n",
            "📄 5995.txt\n",
            "📄 3186.txt\n",
            "📄 873.txt\n",
            "📄 543.txt\n",
            "📄 5451.txt\n",
            "📄 1388.txt\n",
            "📄 6526.txt\n",
            "📄 4283.txt\n",
            "📄 7029.txt\n",
            "📄 6906.txt\n",
            "📄 183.txt\n",
            "📄 2245.txt\n",
            "📄 984.txt\n",
            "📄 4010.txt\n",
            "📄 5853.txt\n",
            "📄 4530.txt\n",
            "📄 2070.txt\n",
            "📄 1070.txt\n",
            "📄 279.txt\n",
            "📄 4696.txt\n",
            "📄 1493.txt\n",
            "📄 5528.txt\n",
            "📄 1311.txt\n",
            "📄 3002.txt\n",
            "📄 5542.txt\n",
            "📄 4618.txt\n",
            "📄 4707.txt\n",
            "📄 3980.txt\n",
            "📄 3270.txt\n",
            "📄 580.txt\n",
            "📄 3376.txt\n",
            "📄 255.txt\n",
            "📄 1273.txt\n",
            "📄 4403.txt\n",
            "📄 5123.txt\n",
            "📄 1327.txt\n",
            "📄 4365.txt\n",
            "📄 2583.txt\n",
            "📄 6192.txt\n",
            "📄 394.txt\n",
            "📄 3807.txt\n",
            "📄 2750.txt\n",
            "📄 1767.txt\n",
            "📄 4322.txt\n",
            "📄 4182.txt\n",
            "📄 3845.txt\n",
            "📄 6199.txt\n",
            "📄 4308.txt\n",
            "📄 371.txt\n",
            "📄 6402.txt\n",
            "📄 5507.txt\n",
            "📄 1411.txt\n",
            "📄 3078.txt\n",
            "📄 5354.txt\n",
            "📄 6793.txt\n",
            "📄 3010.txt\n",
            "📄 6892.txt\n",
            "📄 3871.txt\n",
            "📄 619.txt\n",
            "📄 2504.txt\n",
            "📄 6816.txt\n",
            "📄 4781.txt\n",
            "📄 4613.txt\n",
            "📄 4679.txt\n",
            "📄 4680.txt\n",
            "📄 1992.txt\n",
            "📄 168.txt\n",
            "📄 6870.txt\n",
            "📄 4615.txt\n",
            "📄 5335.txt\n",
            "📄 1831.txt\n",
            "📄 2606.txt\n",
            "📄 1538.txt\n",
            "📄 4321.txt\n",
            "📄 2162.txt\n",
            "📄 3774.txt\n",
            "📄 4937.txt\n",
            "📄 1647.txt\n",
            "📄 1998.txt\n",
            "📄 699.txt\n",
            "📄 6853.txt\n",
            "📄 6986.txt\n",
            "📄 2403.txt\n",
            "📄 6585.txt\n",
            "📄 5573.txt\n",
            "📄 5848.txt\n",
            "📄 986.txt\n",
            "📄 22.txt\n",
            "📄 1137.txt\n",
            "📄 1179.txt\n",
            "📄 2517.txt\n",
            "📄 5280.txt\n",
            "📄 3385.txt\n",
            "📄 5950.txt\n",
            "📄 1568.txt\n",
            "📄 6449.txt\n",
            "📄 4317.txt\n",
            "📄 6574.txt\n",
            "📄 206.txt\n",
            "📄 5159.txt\n",
            "📄 3983.txt\n",
            "📄 7021.txt\n",
            "📄 3903.txt\n",
            "📄 4262.txt\n",
            "📄 379.txt\n",
            "📄 5363.txt\n",
            "📄 3735.txt\n",
            "📄 5436.txt\n",
            "📄 6042.txt\n",
            "📄 2345.txt\n",
            "📄 1322.txt\n",
            "📄 6375.txt\n",
            "📄 2421.txt\n",
            "📄 1893.txt\n",
            "📄 2926.txt\n",
            "📄 2686.txt\n",
            "📄 2181.txt\n",
            "📄 4911.txt\n",
            "📄 3649.txt\n",
            "📄 390.txt\n",
            "📄 4353.txt\n",
            "📄 6576.txt\n",
            "📄 4644.txt\n",
            "📄 3812.txt\n",
            "📄 6103.txt\n",
            "📄 3142.txt\n",
            "📄 550.txt\n",
            "📄 1909.txt\n",
            "📄 4118.txt\n",
            "📄 5634.txt\n",
            "📄 7043.txt\n",
            "📄 5393.txt\n",
            "📄 6219.txt\n",
            "📄 407.txt\n",
            "📄 5226.txt\n",
            "📄 1006.txt\n",
            "📄 2783.txt\n",
            "📄 1415.txt\n",
            "📄 5461.txt\n",
            "📄 770.txt\n",
            "📄 305.txt\n",
            "📄 1076.txt\n",
            "📄 4561.txt\n",
            "📄 1161.txt\n",
            "📄 1533.txt\n",
            "📄 3234.txt\n",
            "📄 2031.txt\n",
            "📄 1486.txt\n",
            "📄 6125.txt\n",
            "📄 2944.txt\n",
            "📄 6990.txt\n",
            "📄 3888.txt\n",
            "📄 6841.txt\n",
            "📄 3874.txt\n",
            "📄 4276.txt\n",
            "📄 6389.txt\n",
            "📄 1212.txt\n",
            "📄 3289.txt\n",
            "📄 2621.txt\n",
            "📄 6888.txt\n",
            "📄 5773.txt\n",
            "📄 4557.txt\n",
            "📄 2724.txt\n",
            "📄 2663.txt\n",
            "📄 2740.txt\n",
            "📄 1545.txt\n",
            "📄 4189.txt\n",
            "📄 4343.txt\n",
            "📄 2503.txt\n",
            "📄 2250.txt\n",
            "📄 5587.txt\n",
            "📄 2356.txt\n",
            "📄 3134.txt\n",
            "📄 3594.txt\n",
            "📄 1736.txt\n",
            "📄 3611.txt\n",
            "📄 403.txt\n",
            "📄 738.txt\n",
            "📄 6863.txt\n",
            "📄 175.txt\n",
            "📄 5289.txt\n",
            "📄 5011.txt\n",
            "📄 6963.txt\n",
            "📄 2090.txt\n",
            "📄 3994.txt\n",
            "📄 1785.txt\n",
            "📄 6767.txt\n",
            "📄 5189.txt\n",
            "📄 1970.txt\n",
            "📄 6011.txt\n",
            "📄 352.txt\n",
            "📄 5209.txt\n",
            "📄 6352.txt\n",
            "📄 912.txt\n",
            "📄 2807.txt\n",
            "📄 547.txt\n",
            "📄 1342.txt\n",
            "📄 6694.txt\n",
            "📄 233.txt\n",
            "📄 6529.txt\n",
            "📄 5309.txt\n",
            "📄 5415.txt\n",
            "📄 2923.txt\n",
            "📄 4394.txt\n",
            "📄 1246.txt\n",
            "📄 2910.txt\n",
            "📄 4736.txt\n",
            "📄 667.txt\n",
            "📄 302.txt\n",
            "📄 4953.txt\n",
            "📄 6287.txt\n",
            "📄 2772.txt\n",
            "📄 6652.txt\n",
            "📄 2771.txt\n",
            "📄 4049.txt\n",
            "📄 4226.txt\n",
            "📄 4955.txt\n",
            "📄 1240.txt\n",
            "📄 5369.txt\n",
            "📄 4334.txt\n",
            "📄 4088.txt\n",
            "📄 1261.txt\n",
            "📄 4808.txt\n",
            "📄 5373.txt\n",
            "📄 3953.txt\n",
            "📄 5979.txt\n",
            "📄 6428.txt\n",
            "📄 3744.txt\n",
            "📄 648.txt\n",
            "📄 5658.txt\n",
            "📄 4813.txt\n",
            "📄 3481.txt\n",
            "📄 166.txt\n",
            "📄 6338.txt\n",
            "📄 331.txt\n",
            "📄 5428.txt\n",
            "📄 2889.txt\n",
            "📄 1328.txt\n",
            "📄 3071.txt\n",
            "📄 3797.txt\n",
            "📄 6146.txt\n",
            "📄 3024.txt\n",
            "📄 4070.txt\n",
            "📄 5909.txt\n",
            "📄 5095.txt\n",
            "📄 1929.txt\n",
            "📄 5548.txt\n",
            "📄 6509.txt\n",
            "📄 6187.txt\n",
            "📄 214.txt\n",
            "📄 4988.txt\n",
            "📄 903.txt\n",
            "📄 887.txt\n",
            "📄 6366.txt\n",
            "📄 1174.txt\n",
            "📄 2337.txt\n",
            "📄 4267.txt\n",
            "📄 299.txt\n",
            "📄 2694.txt\n",
            "📄 533.txt\n",
            "📄 2775.txt\n",
            "📄 3511.txt\n",
            "📄 5989.txt\n",
            "📄 4971.txt\n",
            "📄 6828.txt\n",
            "📄 5701.txt\n",
            "📄 17.txt\n",
            "📄 5325.txt\n",
            "📄 3454.txt\n",
            "📄 755.txt\n",
            "📄 1318.txt\n",
            "📄 1242.txt\n",
            "📄 1530.txt\n",
            "📄 136.txt\n",
            "📄 6606.txt\n",
            "📄 4815.txt\n",
            "📄 3678.txt\n",
            "📄 4926.txt\n",
            "📄 3431.txt\n",
            "📄 1705.txt\n",
            "📄 906.txt\n",
            "📄 460.txt\n",
            "📄 1303.txt\n",
            "📄 949.txt\n",
            "📄 759.txt\n",
            "📄 131.txt\n",
            "📄 4138.txt\n",
            "📄 1320.txt\n",
            "📄 676.txt\n",
            "📄 1319.txt\n",
            "📄 4586.txt\n",
            "📄 2085.txt\n",
            "📄 1005.txt\n",
            "📄 5868.txt\n",
            "📄 1339.txt\n",
            "📄 2106.txt\n",
            "📄 6476.txt\n",
            "📄 6556.txt\n",
            "📄 2841.txt\n",
            "📄 34.txt\n",
            "📄 1980.txt\n",
            "📄 3828.txt\n",
            "📄 3418.txt\n",
            "📄 4508.txt\n",
            "📄 2110.txt\n",
            "📄 1126.txt\n",
            "📄 2774.txt\n",
            "📄 4648.txt\n",
            "📄 6936.txt\n",
            "📄 735.txt\n",
            "📄 5456.txt\n",
            "📄 440.txt\n",
            "📄 3684.txt\n",
            "📄 3178.txt\n",
            "📄 3833.txt\n",
            "📄 1464.txt\n",
            "📄 6921.txt\n",
            "📄 3663.txt\n",
            "📄 5015.txt\n",
            "📄 6615.txt\n",
            "📄 4836.txt\n",
            "📄 4294.txt\n",
            "📄 5479.txt\n",
            "📄 1855.txt\n",
            "📄 6197.txt\n",
            "📄 2056.txt\n",
            "📄 3573.txt\n",
            "📄 3317.txt\n",
            "📄 5176.txt\n",
            "📄 2641.txt\n",
            "📄 4356.txt\n",
            "📄 4894.txt\n",
            "📄 4318.txt\n",
            "📄 1901.txt\n",
            "📄 2182.txt\n",
            "📄 4733.txt\n",
            "📄 6712.txt\n",
            "📄 939.txt\n",
            "📄 2363.txt\n",
            "📄 2551.txt\n",
            "📄 6373.txt\n",
            "📄 5255.txt\n",
            "📄 828.txt\n",
            "📄 5333.txt\n",
            "📄 1985.txt\n",
            "📄 6220.txt\n",
            "📄 1346.txt\n",
            "📄 863.txt\n",
            "📄 6408.txt\n",
            "📄 1332.txt\n",
            "📄 3925.txt\n",
            "📄 3250.txt\n",
            "📄 5166.txt\n",
            "📄 5299.txt\n",
            "📄 1804.txt\n",
            "📄 5549.txt\n",
            "📄 1098.txt\n",
            "📄 2373.txt\n",
            "📄 1187.txt\n",
            "📄 2826.txt\n",
            "📄 6729.txt\n",
            "📄 4102.txt\n",
            "📄 761.txt\n",
            "📄 1932.txt\n",
            "📄 2816.txt\n",
            "📄 5981.txt\n",
            "📄 6124.txt\n",
            "📄 5967.txt\n",
            "📄 5454.txt\n",
            "📄 5822.txt\n",
            "📄 1257.txt\n",
            "📄 2484.txt\n",
            "📄 3843.txt\n",
            "📄 110.txt\n",
            "📄 5761.txt\n",
            "📄 4050.txt\n",
            "📄 310.txt\n",
            "📄 2339.txt\n",
            "📄 1331.txt\n",
            "📄 4443.txt\n",
            "📄 2075.txt\n",
            "📄 5677.txt\n",
            "📄 1708.txt\n",
            "📄 4438.txt\n",
            "📄 6401.txt\n",
            "📄 3284.txt\n",
            "📄 4324.txt\n",
            "📄 2922.txt\n",
            "📄 4673.txt\n",
            "📄 3185.txt\n",
            "📄 2044.txt\n",
            "📄 1315.txt\n",
            "📄 4706.txt\n",
            "📄 5754.txt\n",
            "📄 3641.txt\n",
            "📄 1153.txt\n",
            "📄 5156.txt\n",
            "📄 6796.txt\n",
            "📄 2425.txt\n",
            "📄 5427.txt\n",
            "📄 6433.txt\n",
            "📄 3972.txt\n",
            "📄 6591.txt\n",
            "📄 1014.txt\n",
            "📄 2422.txt\n",
            "📄 5791.txt\n",
            "📄 2130.txt\n",
            "📄 5481.txt\n",
            "📄 2820.txt\n",
            "📄 5916.txt\n",
            "📄 7107.txt\n",
            "📄 5379.txt\n",
            "📄 1763.txt\n",
            "📄 4801.txt\n",
            "📄 1324.txt\n",
            "📄 1951.txt\n",
            "📄 3414.txt\n",
            "📄 3902.txt\n",
            "📄 6159.txt\n",
            "📄 438.txt\n",
            "📄 5365.txt\n",
            "📄 1734.txt\n",
            "📄 5383.txt\n",
            "📄 2952.txt\n",
            "📄 6970.txt\n",
            "📄 194.txt\n",
            "📄 4327.txt\n",
            "📄 4395.txt\n",
            "📄 6358.txt\n",
            "📄 6587.txt\n",
            "📄 5318.txt\n",
            "📄 6459.txt\n",
            "📄 1019.txt\n",
            "📄 6943.txt\n",
            "📄 2914.txt\n",
            "📄 6063.txt\n",
            "📄 2946.txt\n",
            "📄 1674.txt\n",
            "📄 2401.txt\n",
            "📄 3235.txt\n",
            "📄 5087.txt\n",
            "📄 6995.txt\n",
            "📄 2969.txt\n",
            "📄 5603.txt\n",
            "📄 3701.txt\n",
            "📄 647.txt\n",
            "📄 5053.txt\n",
            "📄 2545.txt\n",
            "📄 3164.txt\n",
            "📄 6405.txt\n",
            "📄 1854.txt\n",
            "📄 5668.txt\n",
            "📄 471.txt\n",
            "📄 2956.txt\n",
            "📄 5625.txt\n",
            "📄 1244.txt\n",
            "📄 1251.txt\n",
            "📄 4288.txt\n",
            "📄 522.txt\n",
            "📄 1598.txt\n",
            "📄 2334.txt\n",
            "📄 1369.txt\n",
            "📄 2016.txt\n",
            "📄 1175.txt\n",
            "📄 3928.txt\n",
            "📄 3870.txt\n",
            "📄 586.txt\n",
            "📄 5643.txt\n",
            "📄 4470.txt\n",
            "📄 2773.txt\n",
            "📄 444.txt\n",
            "📄 2157.txt\n",
            "📄 2928.txt\n",
            "📄 725.txt\n",
            "📄 1977.txt\n",
            "📄 1620.txt\n",
            "📄 5722.txt\n",
            "📄 1643.txt\n",
            "📄 19.txt\n",
            "📄 7084.txt\n",
            "📄 5419.txt\n",
            "📄 1348.txt\n",
            "📄 1065.txt\n",
            "📄 2734.txt\n",
            "📄 3708.txt\n",
            "📄 1752.txt\n",
            "📄 2266.txt\n",
            "📄 4999.txt\n",
            "📄 2530.txt\n",
            "📄 3897.txt\n",
            "📄 3130.txt\n",
            "📄 490.txt\n",
            "📄 6014.txt\n",
            "📄 6759.txt\n",
            "📄 5455.txt\n",
            "📄 6237.txt\n",
            "📄 1395.txt\n",
            "📄 2289.txt\n",
            "📄 3472.txt\n",
            "📄 176.txt\n",
            "📄 1991.txt\n",
            "📄 1304.txt\n",
            "📄 3607.txt\n",
            "📄 5944.txt\n",
            "📄 3502.txt\n",
            "📄 835.txt\n",
            "📄 4042.txt\n",
            "📄 6882.txt\n",
            "📄 5604.txt\n",
            "📄 2572.txt\n",
            "📄 1236.txt\n",
            "📄 3285.txt\n",
            "📄 1264.txt\n",
            "📄 1953.txt\n",
            "📄 2218.txt\n",
            "📄 6217.txt\n",
            "📄 5047.txt\n",
            "📄 5646.txt\n",
            "📄 2700.txt\n",
            "📄 87.txt\n",
            "📄 5882.txt\n",
            "📄 688.txt\n",
            "📄 5518.txt\n",
            "📄 3949.txt\n",
            "📄 4432.txt\n",
            "📄 5147.txt\n",
            "📄 4869.txt\n",
            "📄 3509.txt\n",
            "📄 2823.txt\n",
            "📄 1528.txt\n",
            "📄 1541.txt\n",
            "📄 5490.txt\n",
            "📄 2687.txt\n",
            "📄 4797.txt\n",
            "📄 962.txt\n",
            "📄 3959.txt\n",
            "📄 3750.txt\n",
            "📄 2630.txt\n",
            "📄 967.txt\n",
            "📄 4957.txt\n",
            "📄 6823.txt\n",
            "📄 6655.txt\n",
            "📄 2419.txt\n",
            "📄 4614.txt\n",
            "📄 6890.txt\n",
            "📄 5706.txt\n",
            "📄 511.txt\n",
            "📄 2295.txt\n",
            "📄 6106.txt\n",
            "📄 5558.txt\n",
            "📄 1021.txt\n",
            "📄 1824.txt\n",
            "📄 2413.txt\n",
            "📄 574.txt\n",
            "📄 6130.txt\n",
            "📄 4749.txt\n",
            "📄 4511.txt\n",
            "📄 767.txt\n",
            "📄 128.txt\n",
            "📄 1480.txt\n",
            "📄 5385.txt\n",
            "📄 6553.txt\n",
            "📄 1902.txt\n",
            "📄 4190.txt\n",
            "📄 868.txt\n",
            "📄 1694.txt\n",
            "📄 5061.txt\n",
            "📄 749.txt\n",
            "📄 4602.txt\n",
            "📄 904.txt\n",
            "📄 4862.txt\n",
            "📄 7070.txt\n",
            "📄 3960.txt\n",
            "📄 609.txt\n",
            "📄 2101.txt\n",
            "📄 4079.txt\n",
            "📄 258.txt\n",
            "📄 1689.txt\n",
            "📄 2410.txt\n",
            "📄 1004.txt\n",
            "📄 5480.txt\n",
            "📄 3719.txt\n",
            "📄 1214.txt\n",
            "📄 891.txt\n",
            "📄 4620.txt\n",
            "📄 44.txt\n",
            "📄 6648.txt\n",
            "📄 2119.txt\n",
            "📄 1814.txt\n",
            "📄 5323.txt\n",
            "📄 5494.txt\n",
            "📄 836.txt\n",
            "📄 5136.txt\n",
            "📄 2529.txt\n",
            "📄 919.txt\n",
            "📄 3425.txt\n",
            "📄 5432.txt\n",
            "📄 5943.txt\n",
            "📄 1832.txt\n",
            "📄 3153.txt\n",
            "📄 6325.txt\n",
            "📄 4967.txt\n",
            "📄 5078.txt\n",
            "📄 6541.txt\n",
            "📄 6436.txt\n",
            "📄 6071.txt\n",
            "📄 4150.txt\n",
            "📄 5105.txt\n",
            "📄 6696.txt\n",
            "📄 3694.txt\n",
            "📄 6206.txt\n",
            "📄 1605.txt\n",
            "📄 205.txt\n",
            "📄 3208.txt\n",
            "📄 4044.txt\n",
            "📄 5727.txt\n",
            "📄 3777.txt\n",
            "📄 6254.txt\n",
            "📄 6766.txt\n",
            "📄 4227.txt\n",
            "📄 6303.txt\n",
            "📄 436.txt\n",
            "📄 1570.txt\n",
            "📄 5172.txt\n",
            "📄 1714.txt\n",
            "📄 1034.txt\n",
            "📄 459.txt\n",
            "📄 7022.txt\n",
            "📄 1051.txt\n",
            "📄 6956.txt\n",
            "📄 804.txt\n",
            "📄 1373.txt\n",
            "📄 2709.txt\n",
            "📄 3286.txt\n",
            "📄 60.txt\n",
            "📄 38.txt\n",
            "📄 3160.txt\n",
            "📄 4028.txt\n",
            "📄 2638.txt\n",
            "📄 6701.txt\n",
            "📄 7019.txt\n",
            "📄 1614.txt\n",
            "📄 3043.txt\n",
            "📄 6871.txt\n",
            "📄 4778.txt\n",
            "📄 1773.txt\n",
            "📄 6672.txt\n",
            "📄 2768.txt\n",
            "📄 6557.txt\n",
            "📄 3877.txt\n",
            "📄 3627.txt\n",
            "📄 993.txt\n",
            "📄 1828.txt\n",
            "📄 3449.txt\n",
            "📄 5091.txt\n",
            "📄 3978.txt\n",
            "📄 3306.txt\n",
            "📄 2307.txt\n",
            "📄 6779.txt\n",
            "📄 4499.txt\n",
            "📄 4168.txt\n",
            "📄 5215.txt\n",
            "📄 2801.txt\n",
            "📄 2544.txt\n",
            "📄 2938.txt\n",
            "📄 1056.txt\n",
            "📄 731.txt\n",
            "📄 5245.txt\n",
            "📄 4225.txt\n",
            "📄 3858.txt\n",
            "📄 4630.txt\n",
            "📄 3547.txt\n",
            "📄 6109.txt\n",
            "📄 6640.txt\n",
            "📄 4669.txt\n",
            "📄 3809.txt\n",
            "📄 5117.txt\n",
            "📄 1096.txt\n",
            "📄 4822.txt\n",
            "📄 2145.txt\n",
            "📄 6238.txt\n",
            "📄 4159.txt\n",
            "📄 5267.txt\n",
            "📄 1905.txt\n",
            "📄 5191.txt\n",
            "📄 5832.txt\n",
            "📄 640.txt\n",
            "📄 6669.txt\n",
            "📄 2107.txt\n",
            "📄 6926.txt\n",
            "📄 645.txt\n",
            "📄 3810.txt\n",
            "📄 6226.txt\n",
            "📄 82.txt\n",
            "📄 1048.txt\n",
            "📄 518.txt\n",
            "📄 4180.txt\n",
            "📄 5438.txt\n",
            "📄 5018.txt\n",
            "📄 178.txt\n",
            "📄 5873.txt\n",
            "📄 1521.txt\n",
            "📄 6091.txt\n",
            "📄 6322.txt\n",
            "📄 6997.txt\n",
            "📄 5297.txt\n",
            "📄 4718.txt\n",
            "📄 3003.txt\n",
            "📄 5877.txt\n",
            "📄 347.txt\n",
            "📄 3308.txt\n",
            "📄 6139.txt\n",
            "📄 6504.txt\n",
            "📄 3921.txt\n",
            "📄 6267.txt\n",
            "📄 6777.txt\n",
            "📄 4307.txt\n",
            "📄 7098.txt\n",
            "📄 2007.txt\n",
            "📄 629.txt\n",
            "📄 6516.txt\n",
            "📄 6155.txt\n",
            "📄 1475.txt\n",
            "📄 5847.txt\n",
            "📄 7090.txt\n",
            "📄 857.txt\n",
            "📄 4671.txt\n",
            "📄 2311.txt\n",
            "📄 6463.txt\n",
            "📄 481.txt\n",
            "📄 3268.txt\n",
            "📄 4056.txt\n",
            "📄 2651.txt\n",
            "📄 3118.txt\n",
            "📄 1113.txt\n",
            "📄 1865.txt\n",
            "📄 2505.txt\n",
            "📄 1937.txt\n",
            "📄 2943.txt\n",
            "📄 4232.txt\n",
            "📄 3688.txt\n",
            "📄 1283.txt\n",
            "📄 4850.txt\n",
            "📄 5895.txt\n",
            "📄 2891.txt\n",
            "📄 4111.txt\n",
            "📄 6386.txt\n",
            "📄 2707.txt\n",
            "📄 549.txt\n",
            "📄 3112.txt\n",
            "📄 5917.txt\n",
            "📄 766.txt\n",
            "📄 5.txt\n",
            "📄 1936.txt\n",
            "📄 816.txt\n",
            "📄 1817.txt\n",
            "📄 1213.txt\n",
            "📄 5987.txt\n",
            "📄 1112.txt\n",
            "📄 552.txt\n",
            "📄 4420.txt\n",
            "📄 2251.txt\n",
            "📄 2194.txt\n",
            "📄 2199.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Abs/test-data\n",
            "📁 summary\n",
            "📁 judgement\n",
            "📄 stats-IN-test.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Abs/test-data/summary\n",
            "📄 1762.txt\n",
            "📄 6778.txt\n",
            "📄 652.txt\n",
            "📄 2796.txt\n",
            "📄 3019.txt\n",
            "📄 2248.txt\n",
            "📄 5994.txt\n",
            "📄 4071.txt\n",
            "📄 3292.txt\n",
            "📄 2124.txt\n",
            "📄 3356.txt\n",
            "📄 7130.txt\n",
            "📄 4451.txt\n",
            "📄 2649.txt\n",
            "📄 362.txt\n",
            "📄 4938.txt\n",
            "📄 5364.txt\n",
            "📄 3210.txt\n",
            "📄 6118.txt\n",
            "📄 784.txt\n",
            "📄 5707.txt\n",
            "📄 3602.txt\n",
            "📄 6413.txt\n",
            "📄 2593.txt\n",
            "📄 2440.txt\n",
            "📄 78.txt\n",
            "📄 4316.txt\n",
            "📄 6003.txt\n",
            "📄 1522.txt\n",
            "📄 1697.txt\n",
            "📄 2627.txt\n",
            "📄 1789.txt\n",
            "📄 4807.txt\n",
            "📄 5861.txt\n",
            "📄 6728.txt\n",
            "📄 2657.txt\n",
            "📄 6647.txt\n",
            "📄 2256.txt\n",
            "📄 4963.txt\n",
            "📄 266.txt\n",
            "📄 415.txt\n",
            "📄 1531.txt\n",
            "📄 5141.txt\n",
            "📄 715.txt\n",
            "📄 2052.txt\n",
            "📄 6245.txt\n",
            "📄 3531.txt\n",
            "📄 3893.txt\n",
            "📄 1181.txt\n",
            "📄 4782.txt\n",
            "📄 3442.txt\n",
            "📄 232.txt\n",
            "📄 6270.txt\n",
            "📄 5248.txt\n",
            "📄 6881.txt\n",
            "📄 690.txt\n",
            "📄 6521.txt\n",
            "📄 3542.txt\n",
            "📄 5397.txt\n",
            "📄 4860.txt\n",
            "📄 1778.txt\n",
            "📄 1195.txt\n",
            "📄 6276.txt\n",
            "📄 2913.txt\n",
            "📄 2609.txt\n",
            "📄 5597.txt\n",
            "📄 2727.txt\n",
            "📄 5142.txt\n",
            "📄 660.txt\n",
            "📄 2065.txt\n",
            "📄 6157.txt\n",
            "📄 6852.txt\n",
            "📄 5888.txt\n",
            "📄 5471.txt\n",
            "📄 4480.txt\n",
            "📄 4568.txt\n",
            "📄 4917.txt\n",
            "📄 1378.txt\n",
            "📄 380.txt\n",
            "📄 6622.txt\n",
            "📄 2035.txt\n",
            "📄 5538.txt\n",
            "📄 2392.txt\n",
            "📄 496.txt\n",
            "📄 3168.txt\n",
            "📄 314.txt\n",
            "📄 2207.txt\n",
            "📄 5266.txt\n",
            "📄 2304.txt\n",
            "📄 3436.txt\n",
            "📄 1329.txt\n",
            "📄 5937.txt\n",
            "📄 7109.txt\n",
            "📄 2122.txt\n",
            "📄 1406.txt\n",
            "📄 4641.txt\n",
            "📄 1974.txt\n",
            "📄 3924.txt\n",
            "📄 3844.txt\n",
            "📄 6668.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Abs/test-data/judgement\n",
            "📄 1762.txt\n",
            "📄 6778.txt\n",
            "📄 652.txt\n",
            "📄 2796.txt\n",
            "📄 3019.txt\n",
            "📄 2248.txt\n",
            "📄 5994.txt\n",
            "📄 4071.txt\n",
            "📄 3292.txt\n",
            "📄 2124.txt\n",
            "📄 3356.txt\n",
            "📄 7130.txt\n",
            "📄 4451.txt\n",
            "📄 2649.txt\n",
            "📄 362.txt\n",
            "📄 4938.txt\n",
            "📄 5364.txt\n",
            "📄 3210.txt\n",
            "📄 6118.txt\n",
            "📄 784.txt\n",
            "📄 5707.txt\n",
            "📄 3602.txt\n",
            "📄 6413.txt\n",
            "📄 2593.txt\n",
            "📄 2440.txt\n",
            "📄 78.txt\n",
            "📄 4316.txt\n",
            "📄 6003.txt\n",
            "📄 1522.txt\n",
            "📄 1697.txt\n",
            "📄 2627.txt\n",
            "📄 1789.txt\n",
            "📄 4807.txt\n",
            "📄 5861.txt\n",
            "📄 6728.txt\n",
            "📄 2657.txt\n",
            "📄 6647.txt\n",
            "📄 2256.txt\n",
            "📄 4963.txt\n",
            "📄 266.txt\n",
            "📄 415.txt\n",
            "📄 1531.txt\n",
            "📄 5141.txt\n",
            "📄 715.txt\n",
            "📄 2052.txt\n",
            "📄 6245.txt\n",
            "📄 3531.txt\n",
            "📄 3893.txt\n",
            "📄 1181.txt\n",
            "📄 4782.txt\n",
            "📄 3442.txt\n",
            "📄 232.txt\n",
            "📄 6270.txt\n",
            "📄 5248.txt\n",
            "📄 6881.txt\n",
            "📄 690.txt\n",
            "📄 6521.txt\n",
            "📄 3542.txt\n",
            "📄 5397.txt\n",
            "📄 4860.txt\n",
            "📄 1778.txt\n",
            "📄 1195.txt\n",
            "📄 6276.txt\n",
            "📄 2913.txt\n",
            "📄 2609.txt\n",
            "📄 5597.txt\n",
            "📄 2727.txt\n",
            "📄 5142.txt\n",
            "📄 660.txt\n",
            "📄 2065.txt\n",
            "📄 6157.txt\n",
            "📄 6852.txt\n",
            "📄 5888.txt\n",
            "📄 5471.txt\n",
            "📄 4480.txt\n",
            "📄 4568.txt\n",
            "📄 4917.txt\n",
            "📄 1378.txt\n",
            "📄 380.txt\n",
            "📄 6622.txt\n",
            "📄 2035.txt\n",
            "📄 5538.txt\n",
            "📄 2392.txt\n",
            "📄 496.txt\n",
            "📄 3168.txt\n",
            "📄 314.txt\n",
            "📄 2207.txt\n",
            "📄 5266.txt\n",
            "📄 2304.txt\n",
            "📄 3436.txt\n",
            "📄 1329.txt\n",
            "📄 5937.txt\n",
            "📄 7109.txt\n",
            "📄 2122.txt\n",
            "📄 1406.txt\n",
            "📄 4641.txt\n",
            "📄 1974.txt\n",
            "📄 3924.txt\n",
            "📄 3844.txt\n",
            "📄 6668.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext\n",
            "📁 summary\n",
            "📁 judgement\n",
            "📄 .DS_Store\n",
            "📄 IN-EXT-length.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary\n",
            "📁 full\n",
            "📁 segment-wise\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/full\n",
            "📁 A1\n",
            "📁 A2\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/full/A1\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/full/A2\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise\n",
            "📁 A1\n",
            "📁 A2\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1\n",
            "📁 analysis\n",
            "📁 facts\n",
            "📁 argument\n",
            "📁 statute\n",
            "📁 judgement\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/analysis\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/facts\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/argument\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/statute\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/judgement\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2\n",
            "📁 analysis\n",
            "📁 facts\n",
            "📁 argument\n",
            "📁 statute\n",
            "📁 judgement\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/analysis\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/facts\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/argument\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/statute\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/judgement\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n",
            "📂 /content/dataset_extracted/dataset/IN-Ext/judgement\n",
            "📄 1954_M_25.txt\n",
            "📄 2004_C_129.txt\n",
            "📄 1987_M_123.txt\n",
            "📄 2007_S_632.txt\n",
            "📄 2006_A_136.txt\n",
            "📄 2006_A_36.txt\n",
            "📄 2001_S_1131.txt\n",
            "📄 2011_I_16.txt\n",
            "📄 1995_S_317.txt\n",
            "📄 1953_L_1.txt\n",
            "📄 2004_I_24.txt\n",
            "📄 1994_M_69.txt\n",
            "📄 2011_S_308.txt\n",
            "📄 2015_J_10.txt\n",
            "📄 1996_B_72.txt\n",
            "📄 2007_S_608.txt\n",
            "📄 1971_S_1.txt\n",
            "📄 1987_S_26.txt\n",
            "📄 1976_T_9.txt\n",
            "📄 2008_I_54.txt\n",
            "📄 2015_S_368.txt\n",
            "📄 1987_C_108.txt\n",
            "📄 2008_A_260.txt\n",
            "📄 2010_J_55.txt\n",
            "📄 1953_S_23.txt\n",
            "📄 2008_P_8.txt\n",
            "📄 2007_C_121.txt\n",
            "📄 2010_S_431.txt\n",
            "📄 1978_M_13.txt\n",
            "📄 2005_S_388.txt\n",
            "📄 2001_A_234.txt\n",
            "📄 1996_T_169.txt\n",
            "📄 1980_W_3.txt\n",
            "📄 2008_S_1411.txt\n",
            "📄 1989_A_55.txt\n",
            "📄 2008_S_549.txt\n",
            "📄 2014_R_41.txt\n",
            "📄 1994_S_246.txt\n",
            "📄 2012_S_270.txt\n",
            "📄 2008_C_166.txt\n",
            "📄 1977_P_19.txt\n",
            "📄 2009_S_146.txt\n",
            "📄 2000_C_151.txt\n",
            "📄 2000_V_80.txt\n",
            "📄 2014_J_33.txt\n",
            "📄 1973_S_68.txt\n",
            "📄 2007_U_18.txt\n",
            "📄 1963_S_59.txt\n",
            "📄 2007_B_76.txt\n",
            "📄 2009_B_16.txt\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##extracting the files in json format from the structure"
      ],
      "metadata": {
        "id": "Mujm_0M1MGUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to dataset\n",
        "dataset_path = \"/content/dataset_extracted/dataset\"\n",
        "\n",
        "# Folders to load\n",
        "folders = [\"IN-Abs\", \"IN-Ext\", \"UK-Abs\"]\n",
        "\n",
        "# Data structure\n",
        "legal_data = []\n",
        "\n",
        "# Function to load text files from a given folder\n",
        "def load_text_files(folder_path):\n",
        "    data = {}\n",
        "    for category in [\"judgement\", \"summary\"]:\n",
        "        cat_path = os.path.join(folder_path, category)\n",
        "        if os.path.exists(cat_path):\n",
        "            data[category] = {}\n",
        "            for file_name in sorted(os.listdir(cat_path)):  # Sorting ensures pairing is correct\n",
        "                file_path = os.path.join(cat_path, file_name)\n",
        "                if file_name.endswith(\".txt\"):\n",
        "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        data[category][file_name] = f.read()\n",
        "    return data\n",
        "\n",
        "# Loop through each main folder (IN-Abs, IN-Ext, UK-Abs)\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(dataset_path, folder)\n",
        "\n",
        "    # Check for train/test data or normal structure\n",
        "    if folder == \"IN-Abs\":\n",
        "        # Process train & test data\n",
        "        for split in [\"train-data\", \"test-data\"]:\n",
        "            split_path = os.path.join(folder_path, split)\n",
        "            if os.path.exists(split_path):\n",
        "                split_data = load_text_files(split_path)\n",
        "                for file_name in split_data.get(\"judgement\", {}):\n",
        "                    legal_data.append({\n",
        "                        \"category\": folder,\n",
        "                        \"split\": split,\n",
        "                        \"file\": file_name,\n",
        "                        \"judgement\": split_data[\"judgement\"].get(file_name, \"\"),\n",
        "                        \"summary\": split_data[\"summary\"].get(file_name, \"\"),\n",
        "                    })\n",
        "    else:\n",
        "        # Process normal structure (judgement/summary)\n",
        "        split_data = load_text_files(folder_path)\n",
        "        for file_name in split_data.get(\"judgement\", {}):\n",
        "            legal_data.append({\n",
        "                \"category\": folder,\n",
        "                \"split\": \"full\",\n",
        "                \"file\": file_name,\n",
        "                \"judgement\": split_data[\"judgement\"].get(file_name, \"\"),\n",
        "                \"summary\": split_data[\"summary\"].get(file_name, \"\"),\n",
        "            })\n",
        "\n",
        "print(f\"✅ Loaded {len(legal_data)} documents from {folders}!\")\n",
        "\n",
        "# Save structured dataset to JSON\n",
        "json_path = \"/content/legal_dataset.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(legal_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Dataset saved as {json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck5rkjx1JCGv",
        "outputId": "74293fca-6010-4152-8e89-80db841c2fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 7180 documents from ['IN-Abs', 'IN-Ext', 'UK-Abs']!\n",
            "✅ Dataset saved as /content/legal_dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pre-processing the legal text for trnsformer model"
      ],
      "metadata": {
        "id": "pDHKxySYMMV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# loading dataset\n",
        "json_path = \"/content/legal_dataset.json\"\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    legal_data = json.load(f)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\n+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s.]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\d{4}\\s\\w+\", \"\", text)\n",
        "\n",
        "    #sentence tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "#preprocessing the whole text\n",
        "for entry in legal_data:\n",
        "    entry[\"judgement\"] = preprocess_text(entry[\"judgement\"])\n",
        "    entry[\"summary\"] = preprocess_text(entry[\"summary\"])\n",
        "\n",
        "# here is saving of cleaned dataset\n",
        "cleaned_json_path = \"/content/legal_dataset_cleaned.json\"\n",
        "with open(cleaned_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(legal_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Preprocessed and saved dataset as {cleaned_json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SazyGMG-JCJF",
        "outputId": "9a7bbe2c-2a9c-4c4b-eea3-8cf2c57986c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed and saved dataset as /content/legal_dataset_cleaned.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_freq = Counter()\n",
        "for entry in legal_data:\n",
        "    words = entry[\"judgement\"].split()\n",
        "    word_freq.update(words)\n",
        "\n",
        "\n",
        "top_words = word_freq.most_common(50)\n",
        "\n",
        "print(\"top 50 legal terms in dataset:\")\n",
        "for word, freq in top_words:\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mT5p0JCJCLZ",
        "outputId": "eabfefe4-a274-4961-9651-37f3ea68a0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 50 legal terms in dataset:\n",
            "the: 3164949\n",
            "of: 1610415\n",
            "to: 892698\n",
            "in: 772592\n",
            "and: 681106\n",
            "that: 526192\n",
            "a: 485868\n",
            "is: 387512\n",
            "by: 354922\n",
            "be: 330916\n",
            "was: 329714\n",
            "for: 291948\n",
            "it: 282230\n",
            "on: 270502\n",
            "as: 252330\n",
            "not: 243726\n",
            "or: 234859\n",
            "court: 180352\n",
            "section: 179603\n",
            "this: 175244\n",
            "which: 174130\n",
            "under: 153865\n",
            "with: 142966\n",
            "any: 124968\n",
            "an: 124225\n",
            "act: 118897\n",
            "from: 117541\n",
            "has: 106348\n",
            "been: 103149\n",
            "are: 101365\n",
            "have: 99601\n",
            "at: 91050\n",
            "were: 90556\n",
            "case: 90181\n",
            "he: 89766\n",
            "had: 89357\n",
            "such: 89300\n",
            "his: 89258\n",
            "high: 87817\n",
            "order: 85963\n",
            "no: 85435\n",
            "there: 76583\n",
            "may: 74504\n",
            "if: 73748\n",
            "state: 73661\n",
            "we: 73500\n",
            "would: 71309\n",
            "but: 70965\n",
            "made: 70157\n",
            "1: 67496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import collections\n",
        "\n",
        "def count_tokens(directory):\n",
        "    \"\"\"\n",
        "    Reads all text files in a directory and counts unique tokens.\n",
        "    \"\"\"\n",
        "    token_counts = collections.Counter()\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):  # Read only text files\n",
        "                try:\n",
        "                    # Try reading with utf-8 first\n",
        "                    with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                        words = f.read().split()  # Basic tokenization (space-split)\n",
        "                        token_counts.update(words)\n",
        "                except UnicodeDecodeError:\n",
        "                    # If utf-8 fails, try 'latin-1' or 'ISO-8859-1'\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), \"r\", encoding=\"latin-1\") as f:\n",
        "                            words = f.read().split()\n",
        "                            token_counts.update(words)\n",
        "                    except UnicodeDecodeError:\n",
        "                        print(f\"Skipping file {file} due to encoding issues.\")\n",
        "                        # You can add more encodings to try here, or log the skipped files\n",
        "\n",
        "    return token_counts\n",
        "\n",
        "# Path to dataset (modify based on your extracted location)\n",
        "dataset_path = \"/content/dataset_extracted/dataset\"\n",
        "\n",
        "# Count unique tokens\n",
        "token_counts = count_tokens(dataset_path)\n",
        "\n",
        "# Display vocabulary statistics\n",
        "unique_tokens = len(token_counts)\n",
        "print(f\"Total Unique Tokens in Dataset: {unique_tokens}\")\n",
        "\n",
        "# Get most common words\n",
        "most_common_words = token_counts.most_common(50)\n",
        "print(\"\\nTop 50 Most Common Words:\", most_common_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCvPla92SAzr",
        "outputId": "dbfa94f4-2be2-45c9-bd7a-326eac9e9511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Unique Tokens in Dataset: 497500\n",
            "\n",
            "Top 50 Most Common Words: [('the', 4495616), ('of', 2492011), ('to', 1474747), ('in', 1109367), ('and', 1051316), ('that', 821596), ('a', 783320), ('is', 603250), ('by', 519754), ('was', 519257), ('be', 514050), ('for', 450322), ('not', 390381), ('on', 386924), ('The', 383755), ('as', 380017), ('or', 357895), ('it', 319829), ('which', 292660), ('with', 228649), ('under', 215139), ('section', 210081), ('an', 208000), ('this', 203741), ('Court', 195804), ('from', 174159), ('any', 174114), ('have', 165222), ('been', 159595), ('has', 154594), ('had', 153926), ('are', 151436), ('at', 146633), ('were', 144767), ('his', 137814), ('would', 131711), ('It', 124190), ('such', 123743), ('no', 123167), ('In', 121836), ('he', 121031), ('Act', 115848), ('case', 115837), ('High', 104555), ('order', 101280), ('made', 98814), ('there', 98370), ('may', 93902), ('other', 93270), ('State', 92029)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#since our vocab is too large we need to use the hugging face tokes\n",
        "\n",
        "Initializes a Byte Pair Encoding (BPE) tokenizer → Efficient for legal text.\n",
        "\n",
        "Uses pre_tokenizers.Whitespace() → Splits words before BPE merges subwords.\n",
        "\n",
        "Trains tokenizer on dataset with:\n",
        "vocab_size=50000\n",
        "\n",
        "min_frequency=2 (removes rare words)"
      ],
      "metadata": {
        "id": "e0Ot-VWSSvwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First we will convert out encoded files to utf-8 since some files are encoded in latin-1"
      ],
      "metadata": {
        "id": "Qvno7nLVTqQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def convert_to_utf8(file_path):\n",
        "    \"\"\"Convert a file to UTF-8 encoding and overwrite it.\"\"\"\n",
        "    try:\n",
        "        # Try opening with UTF-8\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "        encoding_used = \"utf-8\"  # No conversion needed\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            # Try Latin-1 if UTF-8 fails\n",
        "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "                content = f.read()\n",
        "            encoding_used = \"latin-1\"\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"❌ Skipping file {file_path} due to unknown encoding\")\n",
        "            return False\n",
        "\n",
        "    # If conversion was needed, overwrite file with UTF-8\n",
        "    if encoding_used == \"latin-1\":\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "        print(f\"✅ Converted {file_path} from {encoding_used} to UTF-8.\")\n",
        "\n",
        "    return True  # File is now UTF-8\n",
        "\n",
        "# Convert all text files in dataset\n",
        "dataset_path = \"/content/dataset_extracted/dataset\"  # Update if needed\n",
        "converted_files = []\n",
        "\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            if convert_to_utf8(file_path):\n",
        "                converted_files.append(file_path)\n",
        "\n",
        "print(f\"✅ Converted {len(converted_files)} files to UTF-8.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJJ_lTpoToU8",
        "outputId": "dec170f3-873f-49f1-8d20-ba330b123160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/analysis/2006_A_36.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/analysis/2001_S_1131.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A1/analysis/1963_S_59.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/analysis/2001_S_1131.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/analysis/1963_S_59.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/analysis/2007_B_76.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/argument/2004_I_24.txt from latin-1 to UTF-8.\n",
            "✅ Converted /content/dataset_extracted/dataset/IN-Ext/summary/segment-wise/A2/statute/2007_B_76.txt from latin-1 to UTF-8.\n",
            "✅ Converted 16775 files to UTF-8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "\n",
        "# Initialize Byte Pair Encoding (BPE) tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Pre-tokenization (basic splitting before applying BPE)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Training setup\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=50000,  # Target vocabulary size\n",
        "    min_frequency=2,   # Remove very rare words\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]  # Required for Transformer\n",
        ")\n",
        "\n",
        "# Collect all text files from dataset\n",
        "dataset_files = []\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            dataset_files.append(os.path.join(root, file))\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer.train(files=dataset_files, trainer=trainer)\n",
        "\n",
        "# Save the trained tokenizer\n",
        "tokenizer.save(\"legal_tokenizer.json\")\n",
        "\n",
        "print(\"✅ Tokenizer trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3042S4Supr",
        "outputId": "63567ea3-3110-42fe-c064-489cd5c5cbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenizer trained and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading and tokenizing the data with our tokenizer"
      ],
      "metadata": {
        "id": "GETNURzbUaGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "import os\n",
        "\n",
        "# Load our trained tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"legal_tokenizer.json\")\n",
        "\n",
        "# Paths to judgement and summary files\n",
        "dataset_path = \"/content/dataset_extracted/dataset\"\n",
        "\n",
        "judgement_dirs = [\n",
        "    os.path.join(dataset_path, \"IN-Abs/train-data/judgement\"),\n",
        "    os.path.join(dataset_path, \"IN-Ext/judgement\"),  # ✅ Corrected path\n",
        "    os.path.join(dataset_path, \"UK-Abs/train-data/judgement\"),\n",
        "]\n",
        "\n",
        "summary_dirs = [\n",
        "    os.path.join(dataset_path, \"IN-Abs/train-data/summary\"),\n",
        "    os.path.join(dataset_path, \"IN-Ext/summary\"),  # ✅ Corrected path\n",
        "    os.path.join(dataset_path, \"UK-Abs/train-data/summary\"),\n",
        "]\n",
        "\n",
        "# Define max sequence length for tokenization\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Function to read text from files and truncate\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "        return text[:MAX_SEQ_LENGTH]  # ✅ Truncate before tokenization\n",
        "\n",
        "# Tokenize dataset\n",
        "input_texts, target_texts = [], []\n",
        "\n",
        "for judgement_dir, summary_dir in zip(judgement_dirs, summary_dirs):\n",
        "    if not os.path.exists(judgement_dir) or not os.path.exists(summary_dir):\n",
        "        print(f\"⚠️ Skipping: {judgement_dir} or {summary_dir} does not exist.\")\n",
        "        continue  # Skip directories that do not exist\n",
        "\n",
        "    for file in os.listdir(judgement_dir):\n",
        "        if file.endswith(\".txt\"):\n",
        "            judgement_path = os.path.join(judgement_dir, file)\n",
        "            summary_path = os.path.join(summary_dir, file)  # Corresponding summary\n",
        "\n",
        "            if os.path.exists(summary_path):  # Ensure both exist\n",
        "                input_text = read_text(judgement_path)\n",
        "                target_text = read_text(summary_path)\n",
        "\n",
        "                input_texts.append(input_text)\n",
        "                target_texts.append(target_text)\n",
        "\n",
        "print(f\"✅ Loaded {len(input_texts)} judgement-summary pairs!\")\n",
        "\n",
        "# **Tokenization WITHOUT truncation (handled manually before)**\n",
        "input_encodings = tokenizer.encode_batch(input_texts)\n",
        "target_encodings = tokenizer.encode_batch(target_texts)\n",
        "\n",
        "# Convert tokenized output into tensors\n",
        "input_ids = [torch.tensor(encoding.ids, dtype=torch.long) for encoding in input_encodings]\n",
        "target_ids = [torch.tensor(encoding.ids, dtype=torch.long) for encoding in target_encodings]\n",
        "\n",
        "# **Ensure fixed-length padding**\n",
        "input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "    input_ids, batch_first=True, padding_value=0\n",
        ")\n",
        "target_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "    target_ids, batch_first=True, padding_value=0\n",
        ")\n",
        "\n",
        "print(f\"✅ Tokenized, truncated manually, and padded dataset! Max length: {MAX_SEQ_LENGTH}\")\n",
        "\n",
        "# Save preprocessed data\n",
        "torch.save((input_ids, target_ids), \"preprocessed_legal_data.pt\")\n",
        "print(\"✅ Preprocessed dataset saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTHwej_yUe1-",
        "outputId": "64a206f7-9fbf-4303-992f-2ef99e79d762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 7723 judgement-summary pairs!\n",
            "✅ Tokenized, truncated manually, and padded dataset! Max length: 512\n",
            "✅ Preprocessed dataset saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer Architecture\n"
      ],
      "metadata": {
        "id": "ho_f-SMjReP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# positional embeddings"
      ],
      "metadata": {
        "id": "j6qP6RenRjXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1024):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Position indices (0, 1, 2, ... max_len)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute sine and cosine terms\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to input\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "g0TUfV4vRIh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#multihead attention for to understand the context of each query and word for all keys(text) so that it understands the whole aspects of a single token.\n"
      ],
      "metadata": {
        "id": "pPY14000XxCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0  # Ensure divisibility\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads  # Each head gets a portion\n",
        "\n",
        "        # Linear layers for Query, Key, Value\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final output projection\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # 1️⃣ Transform inputs: (batch, seq_len, d_model) → (batch, num_heads, seq_len, head_dim)\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 2️⃣ Compute attention scores (scaled dot-product)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # 3️⃣ Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)  # Softmax over last dimension\n",
        "\n",
        "        # 4️⃣ Compute weighted sum of values\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # 5️⃣ Concatenate heads: (batch, num_heads, seq_len, head_dim) → (batch, seq_len, d_model)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.w_o(attn_output)  # Final output projection\n"
      ],
      "metadata": {
        "id": "TDqVihLfXWYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##feed forward network to find non-linear patterns and make model understands different importance of words at different places"
      ],
      "metadata": {
        "id": "xZWNn0WZYVQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))  # Apply ReLU activation\n"
      ],
      "metadata": {
        "id": "FMDoqnRHXWap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Each Encoder Layer consists of:\n",
        "\n",
        "Multi-Head Attention\n",
        "\n",
        "Feedforward Layer\n",
        "\n",
        "Layer Normalization\n",
        "\n",
        "Residual Connections"
      ],
      "metadata": {
        "id": "Cy91ECxOYcUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Apply Multi-Head Attention with Residual Connection or skip connection\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # Apply Feedforward Layer with Residual Connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))  # Add & Norm\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zlIC73ASXWdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Decoder Layer\n",
        "\n",
        "Each Decoder Layer has:\n",
        "\n",
        "Masked Multi-Head Attention (prevents looking ahead)\n",
        "\n",
        "Multi-Head Attention on Encoder Output\n",
        "\n",
        "Feedforward Layer\n",
        "\n",
        "Layer Normalization\n",
        "\n",
        "Residual Connections"
      ],
      "metadata": {
        "id": "CYK9hP0uYouY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # 1️⃣ Masked Self-Attention (prevents looking ahead)\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # 2️⃣ Cross Attention with Encoder Output\n",
        "        attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # 3️⃣ Feedforward Layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))  # Add & Norm\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Z61TvgvMXWft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Complete Transformer Model\n",
        "\n",
        "Now, we stack multiple encoder & decoder layers into a full Transformer."
      ],
      "metadata": {
        "id": "V0N_9f_BYxrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, hidden_dim, max_length, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        x = self.embedding(src)  # ✅ Correct embedding first\n",
        "        x = x + self.pos_encoding(x)  # ✅ Add positional encoding\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        x = self.embedding(tgt)  # ✅ Correct embedding first\n",
        "        x = x + self.pos_encoding(x)  # ✅ Add positional encoding\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "D-JJIYaNXWiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now that we have built our custom Transformer architecture, we need to train it using supervised learning.\n",
        "\n",
        "We will:\n",
        "\n",
        "Define Loss Function: Cross-entropy loss.\n",
        "\n",
        "Set Optimizer: AdamW with weight decay.\n",
        "\n",
        "Implement Training Loop: Mini-batch training.\n",
        "\n",
        "Handle Teacher Forcing: Improve decoder learning."
      ],
      "metadata": {
        "id": "GTCcuSCFZBAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Configuration"
      ],
      "metadata": {
        "id": "J-7o5DqSZNpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = 50000  # Based on our tokenizer\n",
        "D_MODEL = 512       # Embedding size\n",
        "NUM_HEADS = 8       # Number of attention heads\n",
        "NUM_LAYERS = 6      # Number of transformer layers\n",
        "HIDDEN_DIM = 2048   # Feedforward hidden layer size\n",
        "MAX_LENGTH = 1024   # Maximum token length per sequence\n",
        "BATCH_SIZE = 64   # Training batch size\n",
        "EPOCHS = 50         # Number of training epochs\n",
        "LR = 5e-5           # Learning rate\n",
        "PATIENCE = 5\n",
        "\n",
        "# Load preprocessed dataset\n",
        "input_ids, target_ids = torch.load(\"preprocessed_legal_data.pt\")\n",
        "\n",
        "# Create dataset & dataloader\n",
        "dataset = TensorDataset(input_ids, target_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, MAX_LENGTH)\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k4L4Md7RImJ",
        "outputId": "179ea411-2166-46c4-d453-594673061419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-ad2cf6381bf6>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_ids, target_ids = torch.load(\"preprocessed_legal_data.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loss function & Optimizer\n",
        "\n",
        "We use CrossEntropyLoss to train the model."
      ],
      "metadata": {
        "id": "t8nkj8CmZSGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function (ignore padding index)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)  # Padding token ID is 0\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n"
      ],
      "metadata": {
        "id": "Ao1py_K6JCbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop with Teacher Forcing\n",
        "\n",
        "We will:\n",
        "\n",
        "Use teacher forcing to train the decoder efficiently.\n",
        "\n",
        "Iterate over mini-batches for training."
      ],
      "metadata": {
        "id": "-UIT_GaWZdwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Learning Rate Scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Early Stopping Variables\n",
        "best_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "# Training function with Early Stopping & LR Scheduling\n",
        "def train_model(model, dataloader, optimizer, loss_fn, scheduler, epochs, patience):\n",
        "    global best_loss, patience_counter\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(dataloader):\n",
        "            src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare decoder input (shifted right target sequence)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Get model predictions\n",
        "            predictions = model(src, tgt_input)\n",
        "\n",
        "            # Compute loss (reshape predictions and targets)\n",
        "            loss = loss_fn(predictions.reshape(-1, VOCAB_SIZE), tgt_output.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}] | Step [{batch_idx}/{len(dataloader)}] | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"epoch {epoch+1} completed! Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Update Learning Rate\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        # Early Stopping Check\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model, \"best_legal_transformer.pth\")  # Save best model\n",
        "            print(f\" Model improved! Saved new best model with loss {avg_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\" No improvement for {patience_counter}/{patience} epochs.\")\n",
        "\n",
        "        # Stop training if patience exceeded\n",
        "        if patience_counter >= patience:\n",
        "            print(\" early Stopping triggered! Stopping training.\")\n",
        "            break\n",
        "\n",
        "# Train the model\n",
        "train_model(model, dataloader, optimizer, loss_fn, scheduler, EPOCHS, PATIENCE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_xwmT_kO5q",
        "outputId": "24664299-0021-4778-d93c-e2704bfac2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] | Step [0/121] | Loss: 10.9625\n",
            "Epoch [1/50] | Step [100/121] | Loss: 6.9178\n",
            "epoch 1 completed! Average Loss: 7.9407\n",
            " Model improved! Saved new best model with loss 7.9407\n",
            "Epoch [2/50] | Step [0/121] | Loss: 6.5768\n",
            "Epoch [2/50] | Step [100/121] | Loss: 6.0029\n",
            "epoch 2 completed! Average Loss: 6.1536\n",
            " Model improved! Saved new best model with loss 6.1536\n",
            "Epoch [3/50] | Step [0/121] | Loss: 5.7597\n",
            "Epoch [3/50] | Step [100/121] | Loss: 5.5977\n",
            "epoch 3 completed! Average Loss: 5.7072\n",
            " Model improved! Saved new best model with loss 5.7072\n",
            "Epoch [4/50] | Step [0/121] | Loss: 5.5940\n",
            "Epoch [4/50] | Step [100/121] | Loss: 5.4000\n",
            "epoch 4 completed! Average Loss: 5.4552\n",
            " Model improved! Saved new best model with loss 5.4552\n",
            "Epoch [5/50] | Step [0/121] | Loss: 5.4005\n",
            "Epoch [5/50] | Step [100/121] | Loss: 5.2393\n",
            "epoch 5 completed! Average Loss: 5.2766\n",
            " Model improved! Saved new best model with loss 5.2766\n",
            "Epoch [6/50] | Step [0/121] | Loss: 4.9760\n",
            "Epoch [6/50] | Step [100/121] | Loss: 5.0854\n",
            "epoch 6 completed! Average Loss: 5.1261\n",
            " Model improved! Saved new best model with loss 5.1261\n",
            "Epoch [7/50] | Step [0/121] | Loss: 4.9602\n",
            "Epoch [7/50] | Step [100/121] | Loss: 4.9932\n",
            "epoch 7 completed! Average Loss: 4.9875\n",
            " Model improved! Saved new best model with loss 4.9875\n",
            "Epoch [8/50] | Step [0/121] | Loss: 4.8811\n",
            "Epoch [8/50] | Step [100/121] | Loss: 4.6874\n",
            "epoch 8 completed! Average Loss: 4.8386\n",
            " Model improved! Saved new best model with loss 4.8386\n",
            "Epoch [9/50] | Step [0/121] | Loss: 4.7052\n",
            "Epoch [9/50] | Step [100/121] | Loss: 4.6172\n",
            "epoch 9 completed! Average Loss: 4.6818\n",
            " Model improved! Saved new best model with loss 4.6818\n",
            "Epoch [10/50] | Step [0/121] | Loss: 4.5429\n",
            "Epoch [10/50] | Step [100/121] | Loss: 4.3192\n",
            "epoch 10 completed! Average Loss: 4.5173\n",
            " Model improved! Saved new best model with loss 4.5173\n",
            "Epoch [11/50] | Step [0/121] | Loss: 4.5878\n",
            "Epoch [11/50] | Step [100/121] | Loss: 4.2588\n",
            "epoch 11 completed! Average Loss: 4.3267\n",
            " Model improved! Saved new best model with loss 4.3267\n",
            "Epoch [12/50] | Step [0/121] | Loss: 4.2511\n",
            "Epoch [12/50] | Step [100/121] | Loss: 4.1001\n",
            "epoch 12 completed! Average Loss: 4.0885\n",
            " Model improved! Saved new best model with loss 4.0885\n",
            "Epoch [13/50] | Step [0/121] | Loss: 4.0325\n",
            "Epoch [13/50] | Step [100/121] | Loss: 3.8130\n",
            "epoch 13 completed! Average Loss: 3.8410\n",
            " Model improved! Saved new best model with loss 3.8410\n",
            "Epoch [14/50] | Step [0/121] | Loss: 3.6471\n",
            "Epoch [14/50] | Step [100/121] | Loss: 3.5129\n",
            "epoch 14 completed! Average Loss: 3.6100\n",
            " Model improved! Saved new best model with loss 3.6100\n",
            "Epoch [15/50] | Step [0/121] | Loss: 3.4807\n",
            "Epoch [15/50] | Step [100/121] | Loss: 3.2986\n",
            "epoch 15 completed! Average Loss: 3.3893\n",
            " Model improved! Saved new best model with loss 3.3893\n",
            "Epoch [16/50] | Step [0/121] | Loss: 3.2802\n",
            "Epoch [16/50] | Step [100/121] | Loss: 3.1304\n",
            "epoch 16 completed! Average Loss: 3.1767\n",
            " Model improved! Saved new best model with loss 3.1767\n",
            "Epoch [17/50] | Step [0/121] | Loss: 3.1089\n",
            "Epoch [17/50] | Step [100/121] | Loss: 2.8645\n",
            "epoch 17 completed! Average Loss: 2.9645\n",
            " Model improved! Saved new best model with loss 2.9645\n",
            "Epoch [18/50] | Step [0/121] | Loss: 2.8889\n",
            "Epoch [18/50] | Step [100/121] | Loss: 2.7190\n",
            "epoch 18 completed! Average Loss: 2.7523\n",
            " Model improved! Saved new best model with loss 2.7523\n",
            "Epoch [19/50] | Step [0/121] | Loss: 2.5989\n",
            "Epoch [19/50] | Step [100/121] | Loss: 2.6021\n",
            "epoch 19 completed! Average Loss: 2.5423\n",
            " Model improved! Saved new best model with loss 2.5423\n",
            "Epoch [20/50] | Step [0/121] | Loss: 2.3752\n",
            "Epoch [20/50] | Step [100/121] | Loss: 2.1961\n",
            "epoch 20 completed! Average Loss: 2.3301\n",
            " Model improved! Saved new best model with loss 2.3301\n",
            "Epoch [21/50] | Step [0/121] | Loss: 2.1763\n",
            "Epoch [21/50] | Step [100/121] | Loss: 2.0858\n",
            "epoch 21 completed! Average Loss: 2.1105\n",
            " Model improved! Saved new best model with loss 2.1105\n",
            "Epoch [22/50] | Step [0/121] | Loss: 1.9766\n",
            "Epoch [22/50] | Step [100/121] | Loss: 1.9229\n",
            "epoch 22 completed! Average Loss: 1.9115\n",
            " Model improved! Saved new best model with loss 1.9115\n",
            "Epoch [23/50] | Step [0/121] | Loss: 1.7635\n",
            "Epoch [23/50] | Step [100/121] | Loss: 1.7097\n",
            "epoch 23 completed! Average Loss: 1.7295\n",
            " Model improved! Saved new best model with loss 1.7295\n",
            "Epoch [24/50] | Step [0/121] | Loss: 1.6651\n",
            "Epoch [24/50] | Step [100/121] | Loss: 1.4943\n",
            "epoch 24 completed! Average Loss: 1.5629\n",
            " Model improved! Saved new best model with loss 1.5629\n",
            "Epoch [25/50] | Step [0/121] | Loss: 1.4782\n",
            "Epoch [25/50] | Step [100/121] | Loss: 1.3776\n",
            "epoch 25 completed! Average Loss: 1.4177\n",
            " Model improved! Saved new best model with loss 1.4177\n",
            "Epoch [26/50] | Step [0/121] | Loss: 1.4427\n",
            "Epoch [26/50] | Step [100/121] | Loss: 1.3253\n",
            "epoch 26 completed! Average Loss: 1.2839\n",
            " Model improved! Saved new best model with loss 1.2839\n",
            "Epoch [27/50] | Step [0/121] | Loss: 1.0923\n",
            "Epoch [27/50] | Step [100/121] | Loss: 1.2022\n",
            "epoch 27 completed! Average Loss: 1.1631\n",
            " Model improved! Saved new best model with loss 1.1631\n",
            "Epoch [28/50] | Step [0/121] | Loss: 0.9408\n",
            "Epoch [28/50] | Step [100/121] | Loss: 1.1255\n",
            "epoch 28 completed! Average Loss: 1.0523\n",
            " Model improved! Saved new best model with loss 1.0523\n",
            "Epoch [29/50] | Step [0/121] | Loss: 1.0476\n",
            "Epoch [29/50] | Step [100/121] | Loss: 0.9139\n",
            "epoch 29 completed! Average Loss: 0.9592\n",
            " Model improved! Saved new best model with loss 0.9592\n",
            "Epoch [30/50] | Step [0/121] | Loss: 0.8850\n",
            "Epoch [30/50] | Step [100/121] | Loss: 0.9341\n",
            "epoch 30 completed! Average Loss: 0.8688\n",
            " Model improved! Saved new best model with loss 0.8688\n",
            "Epoch [31/50] | Step [0/121] | Loss: 0.8002\n",
            "Epoch [31/50] | Step [100/121] | Loss: 0.7746\n",
            "epoch 31 completed! Average Loss: 0.7905\n",
            " Model improved! Saved new best model with loss 0.7905\n",
            "Epoch [32/50] | Step [0/121] | Loss: 0.7631\n",
            "Epoch [32/50] | Step [100/121] | Loss: 0.6561\n",
            "epoch 32 completed! Average Loss: 0.7209\n",
            " Model improved! Saved new best model with loss 0.7209\n",
            "Epoch [33/50] | Step [0/121] | Loss: 0.7459\n",
            "Epoch [33/50] | Step [100/121] | Loss: 0.5799\n",
            "epoch 33 completed! Average Loss: 0.6577\n",
            " Model improved! Saved new best model with loss 0.6577\n",
            "Epoch [34/50] | Step [0/121] | Loss: 0.6417\n",
            "Epoch [34/50] | Step [100/121] | Loss: 0.6559\n",
            "epoch 34 completed! Average Loss: 0.6012\n",
            " Model improved! Saved new best model with loss 0.6012\n",
            "Epoch [35/50] | Step [0/121] | Loss: 0.6125\n",
            "Epoch [35/50] | Step [100/121] | Loss: 0.5873\n",
            "epoch 35 completed! Average Loss: 0.5478\n",
            " Model improved! Saved new best model with loss 0.5478\n",
            "Epoch [36/50] | Step [0/121] | Loss: 0.4884\n",
            "Epoch [36/50] | Step [100/121] | Loss: 0.5241\n",
            "epoch 36 completed! Average Loss: 0.5012\n",
            " Model improved! Saved new best model with loss 0.5012\n",
            "Epoch [37/50] | Step [0/121] | Loss: 0.4159\n",
            "Epoch [37/50] | Step [100/121] | Loss: 0.4667\n",
            "epoch 37 completed! Average Loss: 0.4570\n",
            " Model improved! Saved new best model with loss 0.4570\n",
            "Epoch [38/50] | Step [0/121] | Loss: 0.4527\n",
            "Epoch [38/50] | Step [100/121] | Loss: 0.4230\n",
            "epoch 38 completed! Average Loss: 0.4183\n",
            " Model improved! Saved new best model with loss 0.4183\n",
            "Epoch [39/50] | Step [0/121] | Loss: 0.4245\n",
            "Epoch [39/50] | Step [100/121] | Loss: 0.3495\n",
            "epoch 39 completed! Average Loss: 0.3832\n",
            " Model improved! Saved new best model with loss 0.3832\n",
            "Epoch [40/50] | Step [0/121] | Loss: 0.2980\n",
            "Epoch [40/50] | Step [100/121] | Loss: 0.3703\n",
            "epoch 40 completed! Average Loss: 0.3501\n",
            " Model improved! Saved new best model with loss 0.3501\n",
            "Epoch [41/50] | Step [0/121] | Loss: 0.2882\n",
            "Epoch [41/50] | Step [100/121] | Loss: 0.3080\n",
            "epoch 41 completed! Average Loss: 0.3196\n",
            " Model improved! Saved new best model with loss 0.3196\n",
            "Epoch [42/50] | Step [0/121] | Loss: 0.3147\n",
            "Epoch [42/50] | Step [100/121] | Loss: 0.3140\n",
            "epoch 42 completed! Average Loss: 0.2920\n",
            " Model improved! Saved new best model with loss 0.2920\n",
            "Epoch [43/50] | Step [0/121] | Loss: 0.2306\n",
            "Epoch [43/50] | Step [100/121] | Loss: 0.3111\n",
            "epoch 43 completed! Average Loss: 0.2647\n",
            " Model improved! Saved new best model with loss 0.2647\n",
            "Epoch [44/50] | Step [0/121] | Loss: 0.2472\n",
            "Epoch [44/50] | Step [100/121] | Loss: 0.2445\n",
            "epoch 44 completed! Average Loss: 0.2418\n",
            " Model improved! Saved new best model with loss 0.2418\n",
            "Epoch [45/50] | Step [0/121] | Loss: 0.1927\n",
            "Epoch [45/50] | Step [100/121] | Loss: 0.2418\n",
            "epoch 45 completed! Average Loss: 0.2196\n",
            " Model improved! Saved new best model with loss 0.2196\n",
            "Epoch [46/50] | Step [0/121] | Loss: 0.1867\n",
            "Epoch [46/50] | Step [100/121] | Loss: 0.2099\n",
            "epoch 46 completed! Average Loss: 0.1992\n",
            " Model improved! Saved new best model with loss 0.1992\n",
            "Epoch [47/50] | Step [0/121] | Loss: 0.1771\n",
            "Epoch [47/50] | Step [100/121] | Loss: 0.1465\n",
            "epoch 47 completed! Average Loss: 0.1799\n",
            " Model improved! Saved new best model with loss 0.1799\n",
            "Epoch [48/50] | Step [0/121] | Loss: 0.1632\n",
            "Epoch [48/50] | Step [100/121] | Loss: 0.1658\n",
            "epoch 48 completed! Average Loss: 0.1635\n",
            " Model improved! Saved new best model with loss 0.1635\n",
            "Epoch [49/50] | Step [0/121] | Loss: 0.1722\n",
            "Epoch [49/50] | Step [100/121] | Loss: 0.1356\n",
            "epoch 49 completed! Average Loss: 0.1473\n",
            " Model improved! Saved new best model with loss 0.1473\n",
            "Epoch [50/50] | Step [0/121] | Loss: 0.1272\n",
            "Epoch [50/50] | Step [100/121] | Loss: 0.1298\n",
            "epoch 50 completed! Average Loss: 0.1341\n",
            " Model improved! Saved new best model with loss 0.1341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load preprocessed dataset\n",
        "input_ids, target_ids = torch.load(\"preprocessed_legal_data.pt\")\n",
        "\n",
        "# Split into training & validation (90% train, 10% validation)\n",
        "split_ratio = 0.9\n",
        "split_idx = int(len(input_ids) * split_ratio)\n",
        "\n",
        "train_input_ids, val_input_ids = input_ids[:split_idx], input_ids[split_idx:]\n",
        "train_target_ids, val_target_ids = target_ids[:split_idx], target_ids[split_idx:]\n",
        "\n",
        "# Create validation dataset & dataloader\n",
        "val_dataset = TensorDataset(val_input_ids, val_target_ids)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"✅ Validation Data Loaded: {len(val_input_ids)} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4Vs5ZpUZYbh",
        "outputId": "004aeb05-45d3-4968-9b1e-8a832dce7ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validation Data Loaded: 773 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-a1582b63560e>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_ids, target_ids = torch.load(\"preprocessed_legal_data.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize the model\n",
        "model = Transformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, MAX_LENGTH)\n",
        "\n",
        "# Load the trained weights from the best model\n",
        "model.load_state_dict(torch.load(\"best_legal_transformer.pth\"))\n",
        "model = model.to(\"cuda\")  # Move to GPU\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Best trained model loaded for validation!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ_orTDq-YZ-",
        "outputId": "9f8a2af5-d637-4e8c-df1d-e49d4dbf8718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-06486b6236d6>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_legal_transformer.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best trained model loaded for validation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def validate_model(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(\"cuda\"), tgt.to(\"cuda\")\n",
        "\n",
        "            # Shifted input for decoder\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Get model predictions\n",
        "            predictions = model(src, tgt_input)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(predictions.reshape(-1, VOCAB_SIZE), tgt_output.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert token predictions to text\n",
        "            predicted_tokens = predictions.argmax(dim=-1).cpu().numpy()\n",
        "            target_tokens = tgt_output.cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(predicted_tokens)\n",
        "            all_targets.extend(target_tokens)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"✅ Validation Loss: {avg_loss:.4f}\")\n",
        "    return all_predictions, all_targets, avg_loss\n"
      ],
      "metadata": {
        "id": "CAwHCwLv-YgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function (same as training)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)  # Padding token ID = 0\n",
        "\n",
        "# Run validation\n",
        "predictions, targets, val_loss = validate_model(model, val_dataloader, loss_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipvnYPL2-Yim",
        "outputId": "b963b1ba-b7b0-4351-eb89-c804f6c89da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validation Loss: 0.1353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ctdFyx-YlM",
        "outputId": "c7a61bf7-6501-4d2e-a81c-3e8b3a1f3f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d7c9a08a08a162473578f164249179b0ee8d96981a4559ed48f99a6211f4fbbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Function to compute ROUGE score\n",
        "def compute_rouge(predictions, targets, tokenizer):\n",
        "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = []\n",
        "\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "        target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
        "\n",
        "        score = rouge.score(target_text, pred_text)\n",
        "        scores.append(score[\"rougeL\"].fmeasure)  # Use ROUGE-L F1-score\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Compute ROUGE\n",
        "rouge_score = compute_rouge(predictions, targets, tokenizer)\n",
        "print(f\"✅ ROUGE Score: {rouge_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PxDYocL-zP0",
        "outputId": "45e0b754-b744-4c84-9edf-b00a52237d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ROUGE Score: 0.7261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu(predictions, targets, tokenizer):\n",
        "    bleu_scores = []\n",
        "\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "        target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
        "\n",
        "        # Tokenize sentences\n",
        "        pred_tokens = pred_text.split()\n",
        "        target_tokens = [target_text.split()]\n",
        "\n",
        "        # Compute BLEU\n",
        "        score = sentence_bleu(target_tokens, pred_tokens)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# Compute BLEU\n",
        "bleu_score = compute_bleu(predictions, targets, tokenizer)\n",
        "print(f\"✅ BLEU Score: {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjRoU8WFZjBD",
        "outputId": "2d19f3be-642e-4d48-dc9c-8bca0f767f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BLEU Score: 0.5600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r6JZWCa_ERh",
        "outputId": "1ec51a4e-ade4-4563-f399-96f21373669a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable-baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reward Function: Higher BLEU & ROUGE = Higher Reward\n",
        "def compute_reward(pred_text, target_text, tokenizer):\n",
        "    pred_tokens = pred_text.split()\n",
        "    target_tokens = [target_text.split()]\n",
        "\n",
        "    # Compute BLEU Score (for fluency)\n",
        "    bleu = sentence_bleu(target_tokens, pred_tokens)\n",
        "\n",
        "    # Compute ROUGE Score (for relevance)\n",
        "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_score = rouge.score(target_text, pred_text)\n",
        "\n",
        "    rougeL = rouge_score[\"rougeL\"].fmeasure  # ROUGE-L F1 score\n",
        "\n",
        "    # Combine into final reward (weighted sum)\n",
        "    reward = (0.7 * rougeL) + (0.3 * bleu)\n",
        "\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "tHHnfj8G_EXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXH2aLqG_tYp",
        "outputId": "cfc18003-9b68-47e5-89c7-15e1253979ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U shimmy gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WprvN0_QK1Y0",
        "outputId": "74ca9ca5-933c-4a61-af1d-e47874daaf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra] gymnasium transformers rouge-score sacrebleu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK0KJEEALfB9",
        "outputId": "5e79942b-de62-49b9-d893-9d6c6e00b9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, ale-py, sacrebleu\n",
            "Successfully installed ale-py-0.10.1 colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym  # 🔼 Updated import\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from gymnasium import spaces  # 🔼 Updated import\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from torch.utils.data import DataLoader\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# 1. Load Tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"legal_tokenizer.json\")\n",
        "\n",
        "# 2. Load Model with Security Fix\n",
        "model = torch.load(\n",
        "    \"best_legal_transformer.pth\",\n",
        "    map_location=\"cuda\",\n",
        "    weights_only=True  # 🔼 Security fix for untrusted models\n",
        ")\n",
        "\n",
        "# 3. Load Dataset with Security Fix\n",
        "input_ids, target_ids = torch.load(\n",
        "    \"preprocessed_legal_data.pt\",\n",
        "    weights_only=True  # 🔼 Security fix\n",
        ")\n",
        "val_dataset = torch.utils.data.TensorDataset(input_ids, target_ids)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LENGTH = 512\n",
        "VOCAB_SIZE = 50000\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    \"\"\"Ensure fixed sequence length for observation space.\"\"\"\n",
        "    if len(seq) > max_length:\n",
        "        return seq[:max_length]\n",
        "    return seq + [0] * (max_length - len(seq))\n",
        "\n",
        "class LegalTextEnv(gym.Env):\n",
        "    \"\"\"PPO Environment for Legal Text Summarization\"\"\"\n",
        "\n",
        "    def __init__(self, model, dataloader, tokenizer):\n",
        "        super(LegalTextEnv, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.current_batch = iter(self.dataloader)\n",
        "\n",
        "        # Corrected Observation Space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=VOCAB_SIZE,\n",
        "            shape=(MAX_LENGTH,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Action Space (token selection)\n",
        "        self.action_space = spaces.Discrete(VOCAB_SIZE)\n",
        "\n",
        "        # Tracking\n",
        "        self.current_step = 0\n",
        "        self.current_input_ids = []\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset environment for new episode\"\"\"\n",
        "        super().reset(seed=seed)  # 🔼 Required for Gymnasium\n",
        "\n",
        "        try:\n",
        "            self.input_texts, self.target_texts = next(self.current_batch)\n",
        "        except StopIteration:\n",
        "            self.current_batch = iter(self.dataloader)\n",
        "            self.input_texts, self.target_texts = next(self.current_batch)\n",
        "\n",
        "        # Process input text\n",
        "        input_text = self.tokenizer.decode(\n",
        "            self.input_texts[0].tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        self.current_input_ids = pad_or_truncate(\n",
        "            self.tokenizer.encode(input_text).ids,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        self.current_step = 0\n",
        "        return np.array(self.current_input_ids, dtype=np.int32), {}  # 🔼 Gymnasium requires info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one step in the environment\"\"\"\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Decode generated token\n",
        "        pred_text = self.tokenizer.decode([action], skip_special_tokens=True)\n",
        "        target_text = self.tokenizer.decode(\n",
        "            self.target_texts[0].tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # Calculate enhanced reward\n",
        "        reward = self._compute_reward(pred_text, target_text)\n",
        "\n",
        "        # Termination conditions\n",
        "        terminated = bool(self.current_step >= MAX_LENGTH or action == 0)  # 🔼 Ensure boolean\n",
        "        truncated = False  # 🔼 Gymnasium requires truncated flag\n",
        "\n",
        "        return (\n",
        "            np.array(self.current_input_ids, dtype=np.int32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            {}  # 🔼 Gymnasium requires info dict\n",
        "        )\n",
        "\n",
        "    def _compute_reward(self, pred_text, target_text):\n",
        "        \"\"\"Enhanced reward function with length penalty\"\"\"\n",
        "        # Original scores\n",
        "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "        rouge = scorer.score(target_text, pred_text)['rougeL'].fmeasure\n",
        "        bleu = sacrebleu.corpus_bleu([pred_text], [[target_text]]).score / 100\n",
        "\n",
        "        # Length penalty\n",
        "        pred_len = len(pred_text.split())\n",
        "        target_len = len(target_text.split())\n",
        "        length_penalty = np.exp(-abs(pred_len/target_len - 1))\n",
        "\n",
        "        # Weighted reward\n",
        "        return (0.5 * rouge) + (0.3 * bleu) + (0.2 * length_penalty)\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "# 4. Environment Validation\n",
        "env = LegalTextEnv(model, val_dataloader, tokenizer)\n",
        "check_env(env)  # 🔼 Validate environment compatibility\n",
        "\n",
        "# 5. Create Vectorized Environment\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# 6. Optimized PPO Configuration\n",
        "ppo_model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    device=\"cuda\",\n",
        "    verbose=1,\n",
        "    learning_rate=1e-5,  # 🔼 **Reduce learning rate (stabilize training)**\n",
        "    batch_size=256,  # ✅ Keep batch size moderate\n",
        "    n_steps=2048,  # ✅ Keep steps per update as is\n",
        "    gamma=0.98,  # ✅ Slightly higher discount factor\n",
        "    ent_coef=0.05,  # ✅ Reduce entropy coefficient (improve policy stability)\n",
        "    clip_range=0.1,  # 🔽 **Reduce clip range (prevent over-clipping)**\n",
        "    max_grad_norm=0.5,  # ✅ Keep max gradient clipping\n",
        "    vf_coef=2.0,  # 🔼 **Increase value function weight (fix variance issue)**\n",
        "    n_epochs=5,  # ✅ Reduce epochs (prevent overfitting)\n",
        "    policy_kwargs={\n",
        "        \"net_arch\": [512, 256],  # ✅ Keep architecture same\n",
        "        \"activation_fn\": torch.nn.ReLU,\n",
        "        \"ortho_init\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "# 7. Add Evaluation Callback\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=\"./best_ppo/\",\n",
        "    log_path=\"./logs/\",\n",
        "    eval_freq=5000,\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "# 8. Train with Progress Bar\n",
        "try:\n",
        "    ppo_model.learn(\n",
        "        total_timesteps=150000,\n",
        "        callback=eval_callback,\n",
        "        progress_bar=True\n",
        "    )\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted. Saving final model...\")\n",
        "finally:\n",
        "    ppo_model.save(\"ppo_finetuned_legal_transformer\")\n",
        "    print(\"✅ Training complete. Model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1d8a2310cfd24026a3218ce1ba578ec5",
            "f35911f2b07d496fa4188916e6fab3ba"
          ]
        },
        "id": "WJaAe_2X_EZe",
        "outputId": "f9441295-886f-43ba-c290-1ca646dcbb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d8a2310cfd24026a3218ce1ba578ec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 255  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 8    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 242        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 16         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09208967 |\n",
            "|    clip_fraction        | 0.435      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.5       |\n",
            "|    explained_variance   | 0.543      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.57e+04   |\n",
            "|    n_updates            | 5          |\n",
            "|    policy_gradient_loss | -0.0233    |\n",
            "|    value_loss           | 7.06e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=5000, episode_reward=38.06 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=5000, episode_reward=38.06 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012400968 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.82       |\n",
            "|    explained_variance   | 0.734       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.63e+05    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0288     |\n",
            "|    value_loss           | 3.23e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 177  |\n",
            "|    iterations      | 3    |\n",
            "|    time_elapsed    | 34   |\n",
            "|    total_timesteps | 6144 |\n",
            "-----------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 187        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 43         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.35497323 |\n",
            "|    clip_fraction        | 0.425      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.11      |\n",
            "|    explained_variance   | 0.732      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.68e+05   |\n",
            "|    n_updates            | 15         |\n",
            "|    policy_gradient_loss | -0.0106    |\n",
            "|    value_loss           | 3.88e+05   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=10000, episode_reward=38.02 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=38.02 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 10000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04078826 |\n",
            "|    clip_fraction        | 0.46       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.23      |\n",
            "|    explained_variance   | 0.789      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 3.47e+04   |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0508    |\n",
            "|    value_loss           | 7.35e+04   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 167   |\n",
            "|    iterations      | 5     |\n",
            "|    time_elapsed    | 61    |\n",
            "|    total_timesteps | 10240 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 175         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 70          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022160482 |\n",
            "|    clip_fraction        | 0.338       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.16       |\n",
            "|    explained_variance   | 0.698       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.11e+04    |\n",
            "|    n_updates            | 25          |\n",
            "|    policy_gradient_loss | -0.0395     |\n",
            "|    value_loss           | 1.23e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 181        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 79         |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07202038 |\n",
            "|    clip_fraction        | 0.374      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.09      |\n",
            "|    explained_variance   | 0.784      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 7.26e+04   |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.0433    |\n",
            "|    value_loss           | 6.32e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=15000, episode_reward=38.06 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=15000, episode_reward=38.06 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 15000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.059352633 |\n",
            "|    clip_fraction        | 0.552       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.16       |\n",
            "|    explained_variance   | 0.827       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.37e+04    |\n",
            "|    n_updates            | 35          |\n",
            "|    policy_gradient_loss | -0.0568     |\n",
            "|    value_loss           | 2.35e+04    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 167   |\n",
            "|    iterations      | 8     |\n",
            "|    time_elapsed    | 97    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 171        |\n",
            "|    iterations           | 9          |\n",
            "|    time_elapsed         | 107        |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07165943 |\n",
            "|    clip_fraction        | 0.528      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.26      |\n",
            "|    explained_variance   | 0.808      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 5.49e+04   |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0267    |\n",
            "|    value_loss           | 9.21e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=20000, episode_reward=38.04 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=38.04 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 512       |\n",
            "|    mean_reward          | 38        |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 20000     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.2641656 |\n",
            "|    clip_fraction        | 0.412     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -7.55     |\n",
            "|    explained_variance   | 0.813     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.27e+04  |\n",
            "|    n_updates            | 45        |\n",
            "|    policy_gradient_loss | -0.0165   |\n",
            "|    value_loss           | 6.77e+04  |\n",
            "---------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 161   |\n",
            "|    iterations      | 10    |\n",
            "|    time_elapsed    | 126   |\n",
            "|    total_timesteps | 20480 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 165        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 136        |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17689596 |\n",
            "|    clip_fraction        | 0.538      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -7.33      |\n",
            "|    explained_variance   | 0.744      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.26e+05   |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | 0.00345    |\n",
            "|    value_loss           | 1.43e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 168        |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 145        |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04629939 |\n",
            "|    clip_fraction        | 0.397      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.45      |\n",
            "|    explained_variance   | 0.814      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 7.09e+03   |\n",
            "|    n_updates            | 55         |\n",
            "|    policy_gradient_loss | -0.0507    |\n",
            "|    value_loss           | 3.2e+04    |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=25000, episode_reward=38.06 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=25000, episode_reward=38.06 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 25000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07404547 |\n",
            "|    clip_fraction        | 0.301      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.9       |\n",
            "|    explained_variance   | 0.841      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 3.67e+04   |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.0313    |\n",
            "|    value_loss           | 7.94e+04   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 161   |\n",
            "|    iterations      | 13    |\n",
            "|    time_elapsed    | 164   |\n",
            "|    total_timesteps | 26624 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 164        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 174        |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14729142 |\n",
            "|    clip_fraction        | 0.554      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.82      |\n",
            "|    explained_variance   | 0.709      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 7.17e+04   |\n",
            "|    n_updates            | 65         |\n",
            "|    policy_gradient_loss | -0.0433    |\n",
            "|    value_loss           | 7.36e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=30000, episode_reward=38.05 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=38.05 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 30000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.045477673 |\n",
            "|    clip_fraction        | 0.544       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.21       |\n",
            "|    explained_variance   | 0.783       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.18e+04    |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0589     |\n",
            "|    value_loss           | 2.79e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 159   |\n",
            "|    iterations      | 15    |\n",
            "|    time_elapsed    | 192   |\n",
            "|    total_timesteps | 30720 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 201         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020870294 |\n",
            "|    clip_fraction        | 0.33        |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.79       |\n",
            "|    explained_variance   | 0.741       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 9.07e+04    |\n",
            "|    n_updates            | 75          |\n",
            "|    policy_gradient_loss | -0.0411     |\n",
            "|    value_loss           | 2.29e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 211         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.068391286 |\n",
            "|    clip_fraction        | 0.599       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.2        |\n",
            "|    explained_variance   | 0.787       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 9.85e+03    |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0601     |\n",
            "|    value_loss           | 1.53e+04    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=35000, episode_reward=38.07 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=35000, episode_reward=38.07 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 35000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13575223 |\n",
            "|    clip_fraction        | 0.66       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.4       |\n",
            "|    explained_variance   | 0.797      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 4.91e+03   |\n",
            "|    n_updates            | 85         |\n",
            "|    policy_gradient_loss | -0.0658    |\n",
            "|    value_loss           | 5.91e+03   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 159   |\n",
            "|    iterations      | 18    |\n",
            "|    time_elapsed    | 231   |\n",
            "|    total_timesteps | 36864 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 241         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055055328 |\n",
            "|    clip_fraction        | 0.27        |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.29       |\n",
            "|    explained_variance   | 0.742       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 4.64e+05    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00313    |\n",
            "|    value_loss           | 4.27e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=40000, episode_reward=38.06 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=38.06 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 40000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02135352 |\n",
            "|    clip_fraction        | 0.269      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.23      |\n",
            "|    explained_variance   | 0.773      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 3.5e+05    |\n",
            "|    n_updates            | 95         |\n",
            "|    policy_gradient_loss | -0.0368    |\n",
            "|    value_loss           | 3.51e+05   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 157   |\n",
            "|    iterations      | 20    |\n",
            "|    time_elapsed    | 260   |\n",
            "|    total_timesteps | 40960 |\n",
            "------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 159      |\n",
            "|    iterations           | 21       |\n",
            "|    time_elapsed         | 270      |\n",
            "|    total_timesteps      | 43008    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.390516 |\n",
            "|    clip_fraction        | 0.4      |\n",
            "|    clip_range           | 0.1      |\n",
            "|    entropy_loss         | -8.86    |\n",
            "|    explained_variance   | 0.811    |\n",
            "|    learning_rate        | 1e-05    |\n",
            "|    loss                 | 2.18e+04 |\n",
            "|    n_updates            | 100      |\n",
            "|    policy_gradient_loss | -0.0465  |\n",
            "|    value_loss           | 3.8e+04  |\n",
            "--------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=45000, episode_reward=38.04 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=45000, episode_reward=38.04 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 45000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041270133 |\n",
            "|    clip_fraction        | 0.491       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.35       |\n",
            "|    explained_variance   | 0.656       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.29e+04    |\n",
            "|    n_updates            | 105         |\n",
            "|    policy_gradient_loss | -0.0453     |\n",
            "|    value_loss           | 2.21e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 156   |\n",
            "|    iterations      | 22    |\n",
            "|    time_elapsed    | 288   |\n",
            "|    total_timesteps | 45056 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 157        |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 298        |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09094155 |\n",
            "|    clip_fraction        | 0.547      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.07      |\n",
            "|    explained_variance   | 0.786      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 2.55e+04   |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.046     |\n",
            "|    value_loss           | 3.81e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 159        |\n",
            "|    iterations           | 24         |\n",
            "|    time_elapsed         | 308        |\n",
            "|    total_timesteps      | 49152      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13023214 |\n",
            "|    clip_fraction        | 0.53       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.53      |\n",
            "|    explained_variance   | 0.787      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 6.61e+03   |\n",
            "|    n_updates            | 115        |\n",
            "|    policy_gradient_loss | -0.0365    |\n",
            "|    value_loss           | 4.96e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=50000, episode_reward=38.04 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=50000, episode_reward=38.04 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 50000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07716779 |\n",
            "|    clip_fraction        | 0.447      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -7.22      |\n",
            "|    explained_variance   | 0.785      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 6.88e+04   |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.0214    |\n",
            "|    value_loss           | 1.74e+05   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 156   |\n",
            "|    iterations      | 25    |\n",
            "|    time_elapsed    | 327   |\n",
            "|    total_timesteps | 51200 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 337         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030921584 |\n",
            "|    clip_fraction        | 0.302       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.09       |\n",
            "|    explained_variance   | 0.785       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.75e+05    |\n",
            "|    n_updates            | 125         |\n",
            "|    policy_gradient_loss | -0.0419     |\n",
            "|    value_loss           | 2.21e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=55000, episode_reward=38.06 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=55000, episode_reward=38.06 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 55000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10621764 |\n",
            "|    clip_fraction        | 0.485      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.5       |\n",
            "|    explained_variance   | 0.811      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.22e+04   |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0439    |\n",
            "|    value_loss           | 3.79e+04   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 155   |\n",
            "|    iterations      | 27    |\n",
            "|    time_elapsed    | 356   |\n",
            "|    total_timesteps | 55296 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 156        |\n",
            "|    iterations           | 28         |\n",
            "|    time_elapsed         | 366        |\n",
            "|    total_timesteps      | 57344      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05633644 |\n",
            "|    clip_fraction        | 0.413      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.96      |\n",
            "|    explained_variance   | 0.766      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.5e+05    |\n",
            "|    n_updates            | 135        |\n",
            "|    policy_gradient_loss | -0.0397    |\n",
            "|    value_loss           | 1.4e+05    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 158        |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 375        |\n",
            "|    total_timesteps      | 59392      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10084043 |\n",
            "|    clip_fraction        | 0.531      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.02      |\n",
            "|    explained_variance   | 0.803      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.93e+04   |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.055     |\n",
            "|    value_loss           | 4.95e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=60000, episode_reward=38.01 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=60000, episode_reward=38.01 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 60000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.23538932 |\n",
            "|    clip_fraction        | 0.624      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.1       |\n",
            "|    explained_variance   | 0.768      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.34e+04   |\n",
            "|    n_updates            | 145        |\n",
            "|    policy_gradient_loss | -0.0582    |\n",
            "|    value_loss           | 2.32e+04   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 155   |\n",
            "|    iterations      | 30    |\n",
            "|    time_elapsed    | 394   |\n",
            "|    total_timesteps | 61440 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 404         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060314104 |\n",
            "|    clip_fraction        | 0.391       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9          |\n",
            "|    explained_variance   | 0.828       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 3.28e+04    |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0433     |\n",
            "|    value_loss           | 1.03e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=65000, episode_reward=38.05 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=65000, episode_reward=38.05 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037791423 |\n",
            "|    clip_fraction        | 0.449       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.11       |\n",
            "|    explained_variance   | 0.774       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.48e+05    |\n",
            "|    n_updates            | 155         |\n",
            "|    policy_gradient_loss | -0.0355     |\n",
            "|    value_loss           | 2.14e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 154   |\n",
            "|    iterations      | 32    |\n",
            "|    time_elapsed    | 423   |\n",
            "|    total_timesteps | 65536 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 433        |\n",
            "|    total_timesteps      | 67584      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.20643204 |\n",
            "|    clip_fraction        | 0.648      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.03      |\n",
            "|    explained_variance   | 0.772      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 2.58e+04   |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.0479    |\n",
            "|    value_loss           | 3.03e+04   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 156       |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 443       |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0365054 |\n",
            "|    clip_fraction        | 0.331     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -9.12     |\n",
            "|    explained_variance   | 0.775     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 5.82e+04  |\n",
            "|    n_updates            | 165       |\n",
            "|    policy_gradient_loss | -0.0403   |\n",
            "|    value_loss           | 1.35e+05  |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=70000, episode_reward=38.05 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=70000, episode_reward=38.05 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 70000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028703913 |\n",
            "|    clip_fraction        | 0.339       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.19       |\n",
            "|    explained_variance   | 0.628       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 6.81e+04    |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0362     |\n",
            "|    value_loss           | 7.78e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 155   |\n",
            "|    iterations      | 35    |\n",
            "|    time_elapsed    | 462   |\n",
            "|    total_timesteps | 71680 |\n",
            "------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 156       |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 471       |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0441724 |\n",
            "|    clip_fraction        | 0.425     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -9.13     |\n",
            "|    explained_variance   | 0.803     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.6e+04   |\n",
            "|    n_updates            | 175       |\n",
            "|    policy_gradient_loss | -0.0497   |\n",
            "|    value_loss           | 8.31e+04  |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=75000, episode_reward=38.05 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=75000, episode_reward=38.05 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 75000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026123952 |\n",
            "|    clip_fraction        | 0.473       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.02       |\n",
            "|    explained_variance   | 0.779       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 4.8e+04     |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0485     |\n",
            "|    value_loss           | 6.07e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 154   |\n",
            "|    iterations      | 37    |\n",
            "|    time_elapsed    | 490   |\n",
            "|    total_timesteps | 75776 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 500        |\n",
            "|    total_timesteps      | 77824      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02745517 |\n",
            "|    clip_fraction        | 0.35       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.26      |\n",
            "|    explained_variance   | 0.739      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.69e+05   |\n",
            "|    n_updates            | 185        |\n",
            "|    policy_gradient_loss | -0.0445    |\n",
            "|    value_loss           | 1.16e+05   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 156       |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 510       |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0295607 |\n",
            "|    clip_fraction        | 0.322     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -8.86     |\n",
            "|    explained_variance   | 0.781     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 1.25e+05  |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -0.0406   |\n",
            "|    value_loss           | 2.3e+05   |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=80000, episode_reward=38.06 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=38.06 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.095290825 |\n",
            "|    clip_fraction        | 0.558       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.28       |\n",
            "|    explained_variance   | 0.761       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 6.18e+04    |\n",
            "|    n_updates            | 195         |\n",
            "|    policy_gradient_loss | -0.0563     |\n",
            "|    value_loss           | 5.56e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 154   |\n",
            "|    iterations      | 40    |\n",
            "|    time_elapsed    | 529   |\n",
            "|    total_timesteps | 81920 |\n",
            "------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 155       |\n",
            "|    iterations           | 41        |\n",
            "|    time_elapsed         | 539       |\n",
            "|    total_timesteps      | 83968     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.0150276 |\n",
            "|    clip_fraction        | 0.311     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -8.65     |\n",
            "|    explained_variance   | 0.78      |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 8.22e+03  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -0.00981  |\n",
            "|    value_loss           | 8.87e+04  |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=85000, episode_reward=38.04 +/- 0.04\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=85000, episode_reward=38.04 +/- 0.04\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 85000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.19791989 |\n",
            "|    clip_fraction        | 0.517      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.52      |\n",
            "|    explained_variance   | 0.796      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.04e+04   |\n",
            "|    n_updates            | 205        |\n",
            "|    policy_gradient_loss | -0.0408    |\n",
            "|    value_loss           | 2.18e+04   |\n",
            "----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 154   |\n",
            "|    iterations      | 42    |\n",
            "|    time_elapsed    | 558   |\n",
            "|    total_timesteps | 86016 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 567        |\n",
            "|    total_timesteps      | 88064      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08441355 |\n",
            "|    clip_fraction        | 0.448      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.39      |\n",
            "|    explained_variance   | 0.802      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 3.51e+04   |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0479    |\n",
            "|    value_loss           | 3.7e+04    |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=90000, episode_reward=38.06 +/- 0.04\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=90000, episode_reward=38.06 +/- 0.04\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 90000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026418338 |\n",
            "|    clip_fraction        | 0.317       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.57       |\n",
            "|    explained_variance   | 0.821       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.53e+04    |\n",
            "|    n_updates            | 215         |\n",
            "|    policy_gradient_loss | -0.0382     |\n",
            "|    value_loss           | 2.08e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 153   |\n",
            "|    iterations      | 44    |\n",
            "|    time_elapsed    | 586   |\n",
            "|    total_timesteps | 90112 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 154         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 596         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.059942625 |\n",
            "|    clip_fraction        | 0.544       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.46       |\n",
            "|    explained_variance   | 0.754       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 6.49e+04    |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0471     |\n",
            "|    value_loss           | 6.37e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 155         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 606         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054913938 |\n",
            "|    clip_fraction        | 0.501       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -7.76       |\n",
            "|    explained_variance   | 0.775       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.25e+05    |\n",
            "|    n_updates            | 225         |\n",
            "|    policy_gradient_loss | -0.0282     |\n",
            "|    value_loss           | 1.52e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=95000, episode_reward=38.06 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=95000, episode_reward=38.06 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 95000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.065223254 |\n",
            "|    clip_fraction        | 0.53        |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.23       |\n",
            "|    explained_variance   | 0.788       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.43e+04    |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0557     |\n",
            "|    value_loss           | 2.67e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 154   |\n",
            "|    iterations      | 47    |\n",
            "|    time_elapsed    | 624   |\n",
            "|    total_timesteps | 96256 |\n",
            "------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 634        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13623586 |\n",
            "|    clip_fraction        | 0.472      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.11      |\n",
            "|    explained_variance   | 0.795      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 6.39e+03   |\n",
            "|    n_updates            | 235        |\n",
            "|    policy_gradient_loss | -0.0368    |\n",
            "|    value_loss           | 7.6e+03    |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=100000, episode_reward=38.06 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=100000, episode_reward=38.06 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 100000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10284864 |\n",
            "|    clip_fraction        | 0.483      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.41      |\n",
            "|    explained_variance   | 0.802      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.38e+04   |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.0273    |\n",
            "|    value_loss           | 5.05e+04   |\n",
            "----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 153    |\n",
            "|    iterations      | 49     |\n",
            "|    time_elapsed    | 652    |\n",
            "|    total_timesteps | 100352 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 154         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 661         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.058920264 |\n",
            "|    clip_fraction        | 0.373       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.11       |\n",
            "|    explained_variance   | 0.806       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.45e+05    |\n",
            "|    n_updates            | 245         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 1.78e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 155         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 670         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.122386485 |\n",
            "|    clip_fraction        | 0.391       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8          |\n",
            "|    explained_variance   | 0.741       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 8.63e+04    |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | 0.00405     |\n",
            "|    value_loss           | 1.01e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=105000, episode_reward=38.05 +/- 0.04\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=105000, episode_reward=38.05 +/- 0.04\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 105000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13798398 |\n",
            "|    clip_fraction        | 0.36       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.39      |\n",
            "|    explained_variance   | 0.787      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 4.89e+04   |\n",
            "|    n_updates            | 255        |\n",
            "|    policy_gradient_loss | -0.0233    |\n",
            "|    value_loss           | 7.67e+04   |\n",
            "----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 154    |\n",
            "|    iterations      | 52     |\n",
            "|    time_elapsed    | 687    |\n",
            "|    total_timesteps | 106496 |\n",
            "-------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 697        |\n",
            "|    total_timesteps      | 108544     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09108953 |\n",
            "|    clip_fraction        | 0.702      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.08      |\n",
            "|    explained_variance   | 0.809      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 5.39e+03   |\n",
            "|    n_updates            | 260        |\n",
            "|    policy_gradient_loss | -0.0692    |\n",
            "|    value_loss           | 5.05e+03   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=110000, episode_reward=38.05 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=110000, episode_reward=38.05 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 110000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15076423 |\n",
            "|    clip_fraction        | 0.464      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -7.99      |\n",
            "|    explained_variance   | 0.785      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.52e+04   |\n",
            "|    n_updates            | 265        |\n",
            "|    policy_gradient_loss | -0.0284    |\n",
            "|    value_loss           | 1.82e+04   |\n",
            "----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 154    |\n",
            "|    iterations      | 54     |\n",
            "|    time_elapsed    | 714    |\n",
            "|    total_timesteps | 110592 |\n",
            "-------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 723        |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13487922 |\n",
            "|    clip_fraction        | 0.552      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.77      |\n",
            "|    explained_variance   | 0.799      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 9.77e+04   |\n",
            "|    n_updates            | 270        |\n",
            "|    policy_gradient_loss | -0.0534    |\n",
            "|    value_loss           | 8.6e+04    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 732         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.047973383 |\n",
            "|    clip_fraction        | 0.328       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.75       |\n",
            "|    explained_variance   | 0.775       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.71e+05    |\n",
            "|    n_updates            | 275         |\n",
            "|    policy_gradient_loss | -0.0395     |\n",
            "|    value_loss           | 1.69e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=115000, episode_reward=38.06 +/- 0.01\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=115000, episode_reward=38.06 +/- 0.01\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 512       |\n",
            "|    mean_reward          | 38.1      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 115000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.2311371 |\n",
            "|    clip_fraction        | 0.518     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -9.22     |\n",
            "|    explained_variance   | 0.765     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 1.04e+04  |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -0.0504   |\n",
            "|    value_loss           | 9.83e+03  |\n",
            "---------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 155    |\n",
            "|    iterations      | 57     |\n",
            "|    time_elapsed    | 750    |\n",
            "|    total_timesteps | 116736 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 759         |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.081606835 |\n",
            "|    clip_fraction        | 0.456       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.83       |\n",
            "|    explained_variance   | 0.822       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.23e+04    |\n",
            "|    n_updates            | 285         |\n",
            "|    policy_gradient_loss | -0.0373     |\n",
            "|    value_loss           | 4.22e+04    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=120000, episode_reward=38.05 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=120000, episode_reward=38.05 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 120000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038845077 |\n",
            "|    clip_fraction        | 0.465       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.3        |\n",
            "|    explained_variance   | 0.748       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.02e+04    |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.049      |\n",
            "|    value_loss           | 4.55e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 155    |\n",
            "|    iterations      | 59     |\n",
            "|    time_elapsed    | 776    |\n",
            "|    total_timesteps | 120832 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 785         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049506657 |\n",
            "|    clip_fraction        | 0.335       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.86       |\n",
            "|    explained_variance   | 0.776       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 4.37e+04    |\n",
            "|    n_updates            | 295         |\n",
            "|    policy_gradient_loss | -0.0351     |\n",
            "|    value_loss           | 1.24e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 795         |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060041014 |\n",
            "|    clip_fraction        | 0.403       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.83       |\n",
            "|    explained_variance   | 0.836       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 5.79e+03    |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0441     |\n",
            "|    value_loss           | 2.45e+04    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=125000, episode_reward=38.08 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=125000, episode_reward=38.08 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 125000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08393167 |\n",
            "|    clip_fraction        | 0.47       |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.69      |\n",
            "|    explained_variance   | 0.778      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 5.81e+04   |\n",
            "|    n_updates            | 305        |\n",
            "|    policy_gradient_loss | -0.0528    |\n",
            "|    value_loss           | 8.19e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 156    |\n",
            "|    iterations      | 62     |\n",
            "|    time_elapsed    | 812    |\n",
            "|    total_timesteps | 126976 |\n",
            "-------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 156       |\n",
            "|    iterations           | 63        |\n",
            "|    time_elapsed         | 822       |\n",
            "|    total_timesteps      | 129024    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0988369 |\n",
            "|    clip_fraction        | 0.587     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -9.4      |\n",
            "|    explained_variance   | 0.79      |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.87e+04  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -0.0569   |\n",
            "|    value_loss           | 2.7e+04   |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=130000, episode_reward=38.05 +/- 0.03\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=130000, episode_reward=38.05 +/- 0.03\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 512         |\n",
            "|    mean_reward          | 38          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 130000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028283918 |\n",
            "|    clip_fraction        | 0.433       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.74       |\n",
            "|    explained_variance   | 0.838       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.48e+04    |\n",
            "|    n_updates            | 315         |\n",
            "|    policy_gradient_loss | -0.0494     |\n",
            "|    value_loss           | 1.09e+05    |\n",
            "-----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 156    |\n",
            "|    iterations      | 64     |\n",
            "|    time_elapsed    | 839    |\n",
            "|    total_timesteps | 131072 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 848         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019708317 |\n",
            "|    clip_fraction        | 0.381       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.51       |\n",
            "|    explained_variance   | 0.732       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 8.86e+04    |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0485     |\n",
            "|    value_loss           | 1.23e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=135000, episode_reward=38.09 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=135000, episode_reward=38.09 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 135000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17666523 |\n",
            "|    clip_fraction        | 0.6        |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.18      |\n",
            "|    explained_variance   | 0.808      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 3.86e+03   |\n",
            "|    n_updates            | 325        |\n",
            "|    policy_gradient_loss | -0.0533    |\n",
            "|    value_loss           | 9e+03      |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 155    |\n",
            "|    iterations      | 66     |\n",
            "|    time_elapsed    | 866    |\n",
            "|    total_timesteps | 135168 |\n",
            "-------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 156       |\n",
            "|    iterations           | 67        |\n",
            "|    time_elapsed         | 875       |\n",
            "|    total_timesteps      | 137216    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.1805835 |\n",
            "|    clip_fraction        | 0.638     |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -7.22     |\n",
            "|    explained_variance   | 0.79      |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.67e+04  |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | 0.0554    |\n",
            "|    value_loss           | 1.08e+05  |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 884         |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028217709 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.44       |\n",
            "|    explained_variance   | 0.785       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.07e+05    |\n",
            "|    n_updates            | 335         |\n",
            "|    policy_gradient_loss | -0.0311     |\n",
            "|    value_loss           | 4.32e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=140000, episode_reward=38.07 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=140000, episode_reward=38.07 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38.1       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 140000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.43477604 |\n",
            "|    clip_fraction        | 0.324      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -8.97      |\n",
            "|    explained_variance   | 0.782      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 5.15e+04   |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | -0.00471   |\n",
            "|    value_loss           | 5.12e+04   |\n",
            "----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 156    |\n",
            "|    iterations      | 69     |\n",
            "|    time_elapsed    | 902    |\n",
            "|    total_timesteps | 141312 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 911         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.046972424 |\n",
            "|    clip_fraction        | 0.395       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -8.97       |\n",
            "|    explained_variance   | 0.758       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 3.29e+04    |\n",
            "|    n_updates            | 345         |\n",
            "|    policy_gradient_loss | -0.0417     |\n",
            "|    value_loss           | 1.07e+05    |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=145000, episode_reward=38.04 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=145000, episode_reward=38.04 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 512       |\n",
            "|    mean_reward          | 38        |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 145000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.2769775 |\n",
            "|    clip_fraction        | 0.31      |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -8.42     |\n",
            "|    explained_variance   | 0.766     |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 4.07e+04  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | 0.0284    |\n",
            "|    value_loss           | 1.42e+05  |\n",
            "---------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 156    |\n",
            "|    iterations      | 71     |\n",
            "|    time_elapsed    | 929    |\n",
            "|    total_timesteps | 145408 |\n",
            "-------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 938         |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.058777228 |\n",
            "|    clip_fraction        | 0.532       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -9.59       |\n",
            "|    explained_variance   | 0.759       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.71e+04    |\n",
            "|    n_updates            | 355         |\n",
            "|    policy_gradient_loss | -0.058      |\n",
            "|    value_loss           | 2.98e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 157        |\n",
            "|    iterations           | 73         |\n",
            "|    time_elapsed         | 947        |\n",
            "|    total_timesteps      | 149504     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05859364 |\n",
            "|    clip_fraction        | 0.441      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.21      |\n",
            "|    explained_variance   | 0.822      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 1.38e+04   |\n",
            "|    n_updates            | 360        |\n",
            "|    policy_gradient_loss | -0.0521    |\n",
            "|    value_loss           | 2.43e+04   |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=150000, episode_reward=38.04 +/- 0.02\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=150000, episode_reward=38.04 +/- 0.02\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 512.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 512.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 512        |\n",
            "|    mean_reward          | 38         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 150000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05780679 |\n",
            "|    clip_fraction        | 0.406      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -9.24      |\n",
            "|    explained_variance   | 0.816      |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 2.53e+04   |\n",
            "|    n_updates            | 365        |\n",
            "|    policy_gradient_loss | -0.0504    |\n",
            "|    value_loss           | 1.19e+05   |\n",
            "----------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 157    |\n",
            "|    iterations      | 74     |\n",
            "|    time_elapsed    | 965    |\n",
            "|    total_timesteps | 151552 |\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training complete. Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load trained PPO model\n",
        "ppo_model = PPO.load(\"ppo_finetuned_legal_transformer\")\n",
        "print(\"✅ PPO Model Loaded Successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_OBBfF6_Ebr",
        "outputId": "0487658b-1e51-4b99-fb06-a34e51364d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PPO Model Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tokenizers import Tokenizer\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# ✅ Load tokenizer properly\n",
        "tokenizer = Tokenizer.from_file(\"legal_tokenizer.json\")  # JSON tokenizers should be loaded with `from_file()`\n",
        "\n",
        "# ✅ Load PPO-trained Transformer model\n",
        "ppo_model = PPO.load(\"ppo_finetuned_legal_transformer\")  # Load fine-tuned PPO model\n",
        "\n",
        "MAX_LENGTH = 512  # Maximum sequence length\n",
        "\n",
        "def pad_or_truncate(sequence, max_length):\n",
        "    \"\"\"Ensure input sequence has fixed length.\"\"\"\n",
        "    if len(sequence) > max_length:\n",
        "        return sequence[:max_length]  # Truncate if too long\n",
        "    return sequence + [tokenizer.token_to_id(\"[PAD]\")] * (max_length - len(sequence))  # Pad if too short\n",
        "\n",
        "def generate_summary(input_text, max_length=MAX_LENGTH):\n",
        "    \"\"\"Generate a summary using the fine-tuned PPO model with proper token selection.\"\"\"\n",
        "    # Tokenize input text & pad/truncate\n",
        "    input_ids = tokenizer.encode(input_text).ids\n",
        "    input_ids_padded = pad_or_truncate(input_ids, max_length)\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor(input_ids_padded, dtype=torch.long).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "    # Ensure tensor shape matches PPO expectations\n",
        "    input_array = input_tensor.cpu().numpy()\n",
        "\n",
        "    # Generate summary using PPO model\n",
        "    generated_tokens = []\n",
        "    obs = input_array  # Initial observation\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        action, _ = ppo_model.predict(obs, deterministic=True)\n",
        "\n",
        "        # ✅ Ensure valid token selection\n",
        "        action = np.clip(action[0], 0, tokenizer.get_vocab_size() - 1)\n",
        "        generated_tokens.append(action)\n",
        "\n",
        "        if action == tokenizer.token_to_id(\"[SEP]\"):  # Stop if SEP token is reached\n",
        "            break\n",
        "\n",
        "        # ✅ Update observation with new token\n",
        "        obs = np.array([pad_or_truncate(generated_tokens, max_length)])\n",
        "\n",
        "    # Decode generated tokens to text\n",
        "    summary_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return summary_text\n",
        "\n",
        "# ✅ Sample Test\n",
        "test_text = \"The court held that the contract was not enforceable due to a lack of mutual agreement...\"\n",
        "generated_summary = generate_summary(test_text)\n",
        "\n",
        "print(\"🔹 **Original Legal Text:**\")\n",
        "print(test_text)\n",
        "print(\"\\n✅ **Generated Summary (After Fixes):**\")\n",
        "print(generated_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_jY5kGg_Edx",
        "outputId": "9d9605a7-8ae8-4f2d-9c6a-da17d9356c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Original Legal Text:**\n",
            "The court held that the contract was not enforceable due to a lack of mutual agreement...\n",
            "\n",
            "✅ **Generated Summary (After Fixes):**\n",
            "Commercial undergone converse Nattamaigars MAD doctrinally await await GDP expanse indopril expanse expanse dece dece dece indefiniteness dissected Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan Pan 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 Pan Pan Pan Pan Pan Pan Pan Pan Pan indefiniteness indefiniteness rael vitiating vitiating fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc hoc fix fix fix fix fix fix hoc fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix fix eradicated eradicated vitiating eradicated eradicated vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating eradicated vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating eradicated eradicated eradicated eradicated vitiating vitiating eradicated eradicated eradicated eradicated eradicated vitiating vitiating eradicated eradicated eradicated eradicated eradicated eradicated eradicated eradicated eradicated eradicated eradicated eradicated vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating eradicated vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating eradicated eradicated eradicated vitiating vitiating vitiating vitiating vitiating vitiating eradicated vitiating eradicated vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating vitiating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9o7fIKtPxZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWXfm8mUPxcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}