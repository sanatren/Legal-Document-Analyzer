# -*- coding: utf-8 -*-
"""finetuned-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1svxsKCB5SZS5PXbwDpbaN46hj323RCRu
"""

!pip install datasets
!pip install wandb

import os
import zipfile
import json

outer_zip_path = "/content/7152317.zip"  # Main zip file
extract_outer_path = "/content/dataset"
final_extract_path = "/content/dataset_extracted"

# 1. Extract Outer Zip
with zipfile.ZipFile(outer_zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_outer_path)
print(f"âœ… Outer extraction done: {extract_outer_path}")

# 2. Extract Inner dataset.zip
inner_zip_path = os.path.join(extract_outer_path, "dataset.zip")
if os.path.exists(inner_zip_path):
    with zipfile.ZipFile(inner_zip_path, 'r') as zip_ref:
        zip_ref.extractall(final_extract_path)
    print(f"âœ… Inner extraction done: {final_extract_path}")
else:
    print("âš ï¸ Inner dataset.zip not found!")

# 3. Folder structure visualization (Optional)
for root, dirs, files in os.walk(final_extract_path):
    print(f"ðŸ“‚ {root}")
    for d in dirs:
        print(f"ðŸ“ {d}")
    for f in files:
        print(f"ðŸ“„ {f}")
    print("-"*40)

# 4. Load Data and Format for HuggingFace
dataset_path = os.path.join(final_extract_path, "dataset")
folders = ["IN-Abs", "IN-Ext", "UK-Abs"]

hf_dataset = []

def load_text_files(folder_path):
    data = {}
    for category in ["judgement", "summary"]:
        cat_path = os.path.join(folder_path, category)
        if os.path.exists(cat_path):
            data[category] = {}
            for file in sorted(os.listdir(cat_path)):
                file_path = os.path.join(cat_path, file)
                if file.endswith(".txt"):
                    with open(file_path, "r", encoding="utf-8") as f:
                        data[category][file] = f.read().strip()
    return data

for folder in folders:
    folder_path = os.path.join(dataset_path, folder)

    if folder == "IN-Abs":
        for split in ["train-data", "test-data"]:
            split_path = os.path.join(folder_path, split)
            if os.path.exists(split_path):
                split_data = load_text_files(split_path)
                for file in split_data.get("judgement", {}):
                    judgement = split_data["judgement"].get(file, "")
                    summary = split_data["summary"].get(file, "")
                    if judgement and summary:
                        hf_dataset.append({"text": judgement, "summary": summary})
    else:
        split_data = load_text_files(folder_path)
        for file in split_data.get("judgement", {}):
            judgement = split_data["judgement"].get(file, "")
            summary = split_data["summary"].get(file, "")
            if judgement and summary:
                hf_dataset.append({"text": judgement, "summary": summary})

print(f"âœ… Total usable Judgement-Summary pairs: {len(hf_dataset)}")

# 5. Save Final Dataset for Hugging Face Trainer
output_path = "/content/legal_dataset_hf.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(hf_dataset, f, indent=4, ensure_ascii=False)

print(f"ðŸŽ¯ Final dataset ready at: {output_path}")

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

dataset = load_dataset("json", data_files="/content/legal_dataset_hf.json")
dataset = dataset["train"].train_test_split(test_size=0.1)

print(dataset)

model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""## Tokenizer & Preprocessing"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    model_inputs = tokenizer(examples["text"], max_length=512, truncation=True, padding="max_length")

    # Labels
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Map preprocessing to entire dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)

tokenized_dataset

"""## Loading model

"""

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""## Training Parameters

## Training set up
"""

training_args = Seq2SeqTrainingArguments(
    output_dir="./results_legal",
    evaluation_strategy="steps",
    save_steps=500,
    eval_steps=500,
    logging_steps=100,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=4,
    warmup_steps=500,
    predict_with_generate=True,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    generation_max_length=128,
    generation_num_beams=5,
    lr_scheduler_type="linear"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

"""## Training"""

import wandb

wandb.init(project="legal-doc-summarizer", name="bart-large-cnn-finetune")

trainer.train()

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_path = "/content/results_legal/checkpoint-800"

tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)

model.save_pretrained("legal-summarizer-final")
tokenizer.save_pretrained("legal-summarizer-final")

print("Model & Tokenizer Saved Successfully âœ…")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Load from your saved final model
model_path = "/content/legal-summarizer-final"

tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True).to("cuda")

# Your Legal Text for testing
legal_text = """
The appellant had entered into a long-term lease agreement way back in early 2018 specifically on January 1st, which was supposed to last for a duration of five years. But after expiry, the respondent shockingly did not leave the property. Even though several written notices were sent repeatedly by the appellant to the respondent asking to vacate the property, there was non-compliance. The appellant was then forced to approach the court demanding eviction, unpaid rent, and damages.


"""

# Tokenize the input text
inputs = tokenizer(
    legal_text,
    return_tensors="pt",
    max_length=512,
    truncation=True,
    padding="max_length"
)

# Move tensors to correct device
inputs = {k: v.to(model.device) for k, v in inputs.items() if torch.is_tensor(v)}

# Generate the summary
summary_ids = model.generate(
    inputs["input_ids"],
    max_length=512,           # or increase if you want longer summaries
    num_beams=5,
    length_penalty=1.0,
    repetition_penalty=2.0,
    early_stopping=True,
    no_repeat_ngram_size=3

)

# Decode the summary
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("Generated Summary:")
print(summary)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Load Summarizer Model
summarizer_path = "/content/legal-summarizer-final"
sum_tokenizer = AutoTokenizer.from_pretrained(summarizer_path, local_files_only=True)
sum_model = AutoModelForSeq2SeqLM.from_pretrained(summarizer_path, local_files_only=True).to("cuda")

# Load Paraphraser Model
paraphraser_path = "Vamsi/T5_Paraphrase_Paws"  # you can also use 'flax-community/t5-paraphraser-large'
para_tokenizer = AutoTokenizer.from_pretrained(paraphraser_path)
para_model = AutoModelForSeq2SeqLM.from_pretrained(paraphraser_path).to("cuda")


def generate_summary(text: str) -> str:
    inputs = sum_tokenizer(text, return_tensors="pt", max_length=1024, truncation=True, padding="max_length")
    inputs = {k: v.to(sum_model.device) for k, v in inputs.items() if torch.is_tensor(v)}

    summary_ids = sum_model.generate(inputs["input_ids"], max_length=512, num_beams=5, early_stopping=True)
    return sum_tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def paraphrase_text(text: str) -> str:
    input_text = f"paraphrase: {text} </s>"

    inputs = para_tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True, padding="max_length")
    inputs = {k: v.to(para_model.device) for k, v in inputs.items() if torch.is_tensor(v)}

    output_ids = para_model.generate(inputs["input_ids"], max_length=256, num_beams=5, early_stopping=True)
    return para_tokenizer.decode(output_ids[0], skip_special_tokens=True)


def legal_summarizer_pipeline(legal_text: str) -> str:
    print("Step 1: Summarizing Legal Text...")
    summary = generate_summary(legal_text)

    print("\nStep 2: Paraphrasing into Easy Language...")
    final_easy_summary = paraphrase_text(summary)

    return final_easy_summary


# Example Usage
legal_text = """The appellant had entered into a long-term lease agreement way back in early 2018 specifically on January 1st, which was supposed to last for a duration of five years. But after expiry, the respondent shockingly did not leave the property. Even though several written notices were sent repeatedly by the appellant to the respondent asking to vacate the property, there was non-compliance. The appellant was then forced to approach the court demanding eviction, unpaid rent, and damages.
"""

output = legal_summarizer_pipeline(legal_text)

print("\n==== FINAL EASY LEGAL SUMMARY ====\n")
print(output)

!tar -cvf legal_project.tar legal-summarizer-final results_legal legal_dataset_hf.json

from huggingface_hub import notebook_login
notebook_login()

para_tokenizer.save_pretrained("paraphraser-t5")
para_model.save_pretrained("paraphraser-t5")

from huggingface_hub import HfApi

api = HfApi()

# Summarizer Repo
api.create_repo("legal-summarizer-bart", private=True)

# Paraphraser Repo (Optional if needed)
api.create_repo("t5-legal-paraphraser", private=True)

# Upload summarizer
api.upload_folder(
    folder_path="legal-summarizer-final",
    repo_id="sanatann/legal-summarizer-bart"
)

# Upload paraphraser
api.upload_folder(
    folder_path="paraphraser-t5",
    repo_id="sanatann/t5-legal-paraphraser"
)

































from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import textwrap

# Load Model
model_path = "/content/legal-summarizer-final"
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True).to("cuda")


def chunk_text(text: str, tokenizer, max_tokens: int = 900, overlap: int = 100) -> list[str]:
    """
    Auto chunk any long text for transformer input limits.
    """
    tokens = tokenizer(text, return_tensors="pt", truncation=False)["input_ids"][0]
    total_tokens = len(tokens)

    chunks = []
    start = 0

    while start < total_tokens:
        end = min(start + max_tokens, total_tokens)
        chunk_ids = tokens[start:end]
        chunk = tokenizer.decode(chunk_ids, skip_special_tokens=True)
        chunks.append(chunk)

        start = end - overlap  # smart overlap for context

    return chunks


def summarize_chunk(chunk: str) -> str:
    inputs = tokenizer(chunk, return_tensors="pt", truncation=True, padding="max_length", max_length=1024)
    inputs = {k: v.to(model.device) for k, v in inputs.items() if torch.is_tensor(v)}

    summary_ids = model.generate(
        inputs["input_ids"],
        max_length=400,
        num_beams=5,
        length_penalty=1.0,
        repetition_penalty=2.0,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def summarize_long_legal_text(legal_text: str) -> str:
    chunks = chunk_text(legal_text, tokenizer)
    print(f"Total Chunks Created: {len(chunks)}")

    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"Summarizing Chunk {i + 1}/{len(chunks)}...")
        summary = summarize_chunk(chunk)
        summaries.append(summary)

    final_summary = "\n\n".join(summaries)
    return final_summary


# Usage
legal_text = """PASTE YOUR FULL LEGAL TEXT HERE"""

summary = summarize_long_legal_text(legal_text)

print("\n==== FINAL LEGAL SUMMARY ====\n")
print(summary)